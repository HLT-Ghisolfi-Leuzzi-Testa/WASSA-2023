{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\giuli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\giuli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_2108\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_2108\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_2108\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_2108\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_2108\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_2108\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_2108\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n"
     ]
    }
   ],
   "source": [
    "categories = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'hope']\n",
    "categories_dfs = {}\n",
    "for category in categories:\n",
    "\tcategories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>hope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11384</th>\n",
       "      <td>aback</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11830</th>\n",
       "      <td>abacus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281</th>\n",
       "      <td>zone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>zoo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7907</th>\n",
       "      <td>zoological</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13967</th>\n",
       "      <td>zoology</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5509</th>\n",
       "      <td>zoom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14153 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  anger  disgust  fear  joy  sadness  surprise  hope\n",
       "11384        aback      0        0     0    0        0         0     0\n",
       "11830       abacus      0        0     0    0        0         0     0\n",
       "1324       abandon      0        0     1    0        1         0     0\n",
       "663      abandoned      1        0     1    0        1         0     0\n",
       "609    abandonment      1        0     1    0        1         1     0\n",
       "...            ...    ...      ...   ...  ...      ...       ...   ...\n",
       "5281          zone      0        0     0    0        0         0     0\n",
       "5458           zoo      0        0     0    0        0         0     0\n",
       "7907    zoological      0        0     0    0        0         0     0\n",
       "13967      zoology      0        0     0    0        0         0     0\n",
       "5509          zoom      0        0     0    0        0         0     0\n",
       "\n",
       "[14153 rows x 8 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = pd.DataFrame(columns=['word'])\n",
    "for category in categories:\n",
    "    lexicon = pd.merge(lexicon, categories_dfs[category], on='word', how='outer')\n",
    "lexicon.dropna(inplace=True) # row with empty string\n",
    "lexicon.sort_values(by='word', inplace=True)\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN_DATA_PATH = \"./datasets/WASSA23_essay_level_with_labels_train.tsv\"\n",
    "TRAIN_DATA_PATH = \"./datasets/WASSA23_essay_level_dev_preproc.tsv\"\n",
    "#TRAIN_DATA_PATH = \"datasets/WASSA23_essay_level_test_preproc.tsv\"\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>hope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abduct</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aberr</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abhor</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abhorr</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zani</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeal</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealou</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zest</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2984 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        anger disgust fear joy sadness surprise hope\n",
       "abandon     1       0    1   0       1        1    0\n",
       "abduct      0       0    1   0       1        1    0\n",
       "aberr       0       1    0   0       0        0    0\n",
       "abhor       1       1    1   0       0        0    0\n",
       "abhorr      1       1    1   0       0        0    0\n",
       "...       ...     ...  ...  ..     ...      ...  ...\n",
       "youth       1       0    1   1       0        1    0\n",
       "zani        0       0    0   0       0        1    0\n",
       "zeal        0       0    0   1       0        1    1\n",
       "zealou      0       0    0   1       0        0    0\n",
       "zest        0       0    0   1       0        0    1\n",
       "\n",
       "[2984 rows x 7 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "preproc_lexicon = {}\n",
    "lexicon['preproc_word'] = ''\n",
    "for index, row in lexicon.iterrows():\n",
    "    lemma = lemmatizer.lemmatize(row['word'])\n",
    "    stemma = stemmer.stem(lemma)\n",
    "    lexicon.loc[index, 'preproc_word'] = stemma\n",
    "    if stemma in preproc_lexicon:\n",
    "        preproc_lexicon[stemma] += lexicon.loc[index, categories] # |=\n",
    "    else:\n",
    "        preproc_lexicon[stemma] = lexicon.loc[index, categories]\n",
    "preproc_lexicon = pd.DataFrame(preproc_lexicon).T\n",
    "preproc_lexicon[preproc_lexicon > 0] = 1\n",
    "preproc_lexicon = preproc_lexicon.loc[(preproc_lexicon!=0).any(axis=1)]\n",
    "preproc_lexicon.to_csv('./lexicon/preproc_lexicon.csv')\n",
    "lexicon.to_csv('./lexicon/lexicon.csv', index=False)\n",
    "preproc_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # python library to recoginze punctuation and digit\n",
    "import contractions # python library to recoginze contractions\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_digit(text):\n",
    "    return re.sub('\\d+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>speaker_number</th>\n",
       "      <th>...</th>\n",
       "      <th>iri_personal_distress</th>\n",
       "      <th>iri_fantasy</th>\n",
       "      <th>iri_empathatic_concern</th>\n",
       "      <th>anger_count</th>\n",
       "      <th>disgust_count</th>\n",
       "      <th>fear_count</th>\n",
       "      <th>joy_count</th>\n",
       "      <th>sadness_count</th>\n",
       "      <th>surprise_count</th>\n",
       "      <th>hope_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>How sad is it that this kind of pain and suffe...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.286</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>The article is kind of tragic and hits close t...</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>64000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.429</td>\n",
       "      <td>1.429</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>213</td>\n",
       "      <td>I think that these kinds of stories, are sad, ...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.286</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>213</td>\n",
       "      <td>It's crazy that random accidents like this hap...</td>\n",
       "      <td>84</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>55000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8570</td>\n",
       "      <td>3.571</td>\n",
       "      <td>3.143</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>78</td>\n",
       "      <td>This story makes me so so sad.... As someone w...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>471</td>\n",
       "      <td>297</td>\n",
       "      <td>After reading the article, you can't help but ...</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>85000</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0715</td>\n",
       "      <td>4.143</td>\n",
       "      <td>4.643</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>478</td>\n",
       "      <td>53</td>\n",
       "      <td>I am definiltly guilty of wasting food. I buy ...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.286</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>484</td>\n",
       "      <td>292</td>\n",
       "      <td>I wish this article had given us more info. We...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.286</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>494</td>\n",
       "      <td>218</td>\n",
       "      <td>This story is really horrifying to me. Knowing...</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>25000</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2860</td>\n",
       "      <td>3.571</td>\n",
       "      <td>3.714</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>497</td>\n",
       "      <td>103</td>\n",
       "      <td>An option for this very well may be that the m...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.286</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     conversation_id  article_id  \\\n",
       "0                  1          35   \n",
       "1                  4          35   \n",
       "2                  7         213   \n",
       "3                  9         213   \n",
       "4                 12          78   \n",
       "..               ...         ...   \n",
       "203              471         297   \n",
       "204              478          53   \n",
       "205              484         292   \n",
       "206              494         218   \n",
       "207              497         103   \n",
       "\n",
       "                                                 essay  speaker_id  gender  \\\n",
       "0    How sad is it that this kind of pain and suffe...          68       2   \n",
       "1    The article is kind of tragic and hits close t...          79       1   \n",
       "2    I think that these kinds of stories, are sad, ...          68       2   \n",
       "3    It's crazy that random accidents like this hap...          84       2   \n",
       "4    This story makes me so so sad.... As someone w...          68       2   \n",
       "..                                                 ...         ...     ...   \n",
       "203  After reading the article, you can't help but ...          70       1   \n",
       "204  I am definiltly guilty of wasting food. I buy ...          68       2   \n",
       "205  I wish this article had given us more info. We...          68       2   \n",
       "206  This story is really horrifying to me. Knowing...          96       2   \n",
       "207  An option for this very well may be that the m...          68       2   \n",
       "\n",
       "     education  race  age  income  speaker_number  ... iri_personal_distress  \\\n",
       "0            2     1   21   20000               1  ...                3.0000   \n",
       "1            6     3   33   64000               1  ...                1.0000   \n",
       "2            2     1   21   20000               1  ...                3.0000   \n",
       "3            4     1   25   55000               1  ...                2.8570   \n",
       "4            2     1   21   20000               1  ...                3.0000   \n",
       "..         ...   ...  ...     ...             ...  ...                   ...   \n",
       "203          6     1   29   85000               2  ...                2.0715   \n",
       "204          2     1   21   20000               2  ...                3.0000   \n",
       "205          2     1   21   20000               2  ...                3.0000   \n",
       "206          3     1   27   25000               2  ...                2.2860   \n",
       "207          2     1   21   20000               2  ...                3.0000   \n",
       "\n",
       "     iri_fantasy  iri_empathatic_concern  anger_count disgust_count  \\\n",
       "0          3.143                   3.286            3             2   \n",
       "1          2.429                   1.429            4             1   \n",
       "2          3.143                   3.286            1             1   \n",
       "3          3.571                   3.143            2             1   \n",
       "4          3.143                   3.286            0             0   \n",
       "..           ...                     ...          ...           ...   \n",
       "203        4.143                   4.643            4             2   \n",
       "204        3.143                   3.286            4             2   \n",
       "205        3.143                   3.286            5             2   \n",
       "206        3.571                   3.714            2             3   \n",
       "207        3.143                   3.286            4             2   \n",
       "\n",
       "     fear_count  joy_count  sadness_count  surprise_count  hope_count  \n",
       "0             4          3              6               1           0  \n",
       "1             5          3              3               1           0  \n",
       "2             1          6              5               5           4  \n",
       "3             4          5              6               2           1  \n",
       "4             1          2              3               1           1  \n",
       "..          ...        ...            ...             ...         ...  \n",
       "203           5          3              8               4           1  \n",
       "204           2          4              4               1           0  \n",
       "205           6          2              4               1           2  \n",
       "206           4          2              3               0           1  \n",
       "207           5          0              5               2           0  \n",
       "\n",
       "[208 rows x 31 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for category in categories:\n",
    "\ttrain_df[f'{category}_count'] = [0 for _ in range(len(train_df))]\n",
    "\n",
    "for index, row in train_df.iterrows(): #TODO: controllare ( on no?)\n",
    "\tfor word in row['essay']:\n",
    "\t\tif word in lexicon['word']:\n",
    "\t\t\tfor category in categories:\n",
    "\t\t\t\ttrain_df.loc[index, f'{category}_count'] += lexicon.loc[word][category]\n",
    "\n",
    "\tessay = remove_punctuations(row['essay'])\n",
    "\tessay = remove_digit(essay)\n",
    "\tcontracs = [contractions.fix(word) for word in essay.split()]\n",
    "\tlemmas = [lemmatizer.lemmatize(contraction) for contraction in contracs]\n",
    "\tstemmas = [stemmer.stem(lemma) for lemma in lemmas]\n",
    "\n",
    "\tfor stemma in stemmas:\n",
    "\t\tif stemma in preproc_lexicon.index:\n",
    "\t\t\tfor category in categories:\n",
    "\t\t\t\ttrain_df.loc[index, f'{category}_count'] += preproc_lexicon.loc[stemma][category]\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>speaker_number</th>\n",
       "      <th>...</th>\n",
       "      <th>iri_personal_distress</th>\n",
       "      <th>iri_fantasy</th>\n",
       "      <th>iri_empathatic_concern</th>\n",
       "      <th>anger_count</th>\n",
       "      <th>disgust_count</th>\n",
       "      <th>fear_count</th>\n",
       "      <th>joy_count</th>\n",
       "      <th>sadness_count</th>\n",
       "      <th>surprise_count</th>\n",
       "      <th>hope_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>How sad is it that this kind of pain and suffe...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.286</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>3.174603</td>\n",
       "      <td>6.349206</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>9.523810</td>\n",
       "      <td>1.587302</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>The article is kind of tragic and hits close t...</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>64000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>2.429</td>\n",
       "      <td>1.429</td>\n",
       "      <td>6.349206</td>\n",
       "      <td>1.587302</td>\n",
       "      <td>7.936508</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>4.761905</td>\n",
       "      <td>1.587302</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>213</td>\n",
       "      <td>I think that these kinds of stories, are sad, ...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.286</td>\n",
       "      <td>1.754386</td>\n",
       "      <td>1.754386</td>\n",
       "      <td>1.754386</td>\n",
       "      <td>10.526316</td>\n",
       "      <td>8.771930</td>\n",
       "      <td>8.771930</td>\n",
       "      <td>7.017544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>213</td>\n",
       "      <td>It's crazy that random accidents like this hap...</td>\n",
       "      <td>84</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>55000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.8570</td>\n",
       "      <td>3.571</td>\n",
       "      <td>3.143</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>1.388889</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>6.944444</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>1.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>78</td>\n",
       "      <td>This story makes me so so sad.... As someone w...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>2.631579</td>\n",
       "      <td>3.947368</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>1.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>471</td>\n",
       "      <td>297</td>\n",
       "      <td>After reading the article, you can't help but ...</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>85000</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0715</td>\n",
       "      <td>4.143</td>\n",
       "      <td>4.643</td>\n",
       "      <td>5.970149</td>\n",
       "      <td>2.985075</td>\n",
       "      <td>7.462687</td>\n",
       "      <td>4.477612</td>\n",
       "      <td>11.940299</td>\n",
       "      <td>5.970149</td>\n",
       "      <td>1.492537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>478</td>\n",
       "      <td>53</td>\n",
       "      <td>I am definiltly guilty of wasting food. I buy ...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.286</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>2.777778</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>1.388889</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>484</td>\n",
       "      <td>292</td>\n",
       "      <td>I wish this article had given us more info. We...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.286</td>\n",
       "      <td>8.620690</td>\n",
       "      <td>3.448276</td>\n",
       "      <td>10.344828</td>\n",
       "      <td>3.448276</td>\n",
       "      <td>6.896552</td>\n",
       "      <td>1.724138</td>\n",
       "      <td>3.448276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>494</td>\n",
       "      <td>218</td>\n",
       "      <td>This story is really horrifying to me. Knowing...</td>\n",
       "      <td>96</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>25000</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2.2860</td>\n",
       "      <td>3.571</td>\n",
       "      <td>3.714</td>\n",
       "      <td>2.325581</td>\n",
       "      <td>3.488372</td>\n",
       "      <td>4.651163</td>\n",
       "      <td>2.325581</td>\n",
       "      <td>3.488372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.162791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>497</td>\n",
       "      <td>103</td>\n",
       "      <td>An option for this very well may be that the m...</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>3.143</td>\n",
       "      <td>3.286</td>\n",
       "      <td>5.970149</td>\n",
       "      <td>2.985075</td>\n",
       "      <td>7.462687</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.462687</td>\n",
       "      <td>2.985075</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     conversation_id  article_id  \\\n",
       "0                  1          35   \n",
       "1                  4          35   \n",
       "2                  7         213   \n",
       "3                  9         213   \n",
       "4                 12          78   \n",
       "..               ...         ...   \n",
       "203              471         297   \n",
       "204              478          53   \n",
       "205              484         292   \n",
       "206              494         218   \n",
       "207              497         103   \n",
       "\n",
       "                                                 essay  speaker_id  gender  \\\n",
       "0    How sad is it that this kind of pain and suffe...          68       2   \n",
       "1    The article is kind of tragic and hits close t...          79       1   \n",
       "2    I think that these kinds of stories, are sad, ...          68       2   \n",
       "3    It's crazy that random accidents like this hap...          84       2   \n",
       "4    This story makes me so so sad.... As someone w...          68       2   \n",
       "..                                                 ...         ...     ...   \n",
       "203  After reading the article, you can't help but ...          70       1   \n",
       "204  I am definiltly guilty of wasting food. I buy ...          68       2   \n",
       "205  I wish this article had given us more info. We...          68       2   \n",
       "206  This story is really horrifying to me. Knowing...          96       2   \n",
       "207  An option for this very well may be that the m...          68       2   \n",
       "\n",
       "     education  race  age  income  speaker_number  ... iri_personal_distress  \\\n",
       "0            2     1   21   20000               1  ...                3.0000   \n",
       "1            6     3   33   64000               1  ...                1.0000   \n",
       "2            2     1   21   20000               1  ...                3.0000   \n",
       "3            4     1   25   55000               1  ...                2.8570   \n",
       "4            2     1   21   20000               1  ...                3.0000   \n",
       "..         ...   ...  ...     ...             ...  ...                   ...   \n",
       "203          6     1   29   85000               2  ...                2.0715   \n",
       "204          2     1   21   20000               2  ...                3.0000   \n",
       "205          2     1   21   20000               2  ...                3.0000   \n",
       "206          3     1   27   25000               2  ...                2.2860   \n",
       "207          2     1   21   20000               2  ...                3.0000   \n",
       "\n",
       "     iri_fantasy  iri_empathatic_concern  anger_count disgust_count  \\\n",
       "0          3.143                   3.286     4.761905      3.174603   \n",
       "1          2.429                   1.429     6.349206      1.587302   \n",
       "2          3.143                   3.286     1.754386      1.754386   \n",
       "3          3.571                   3.143     2.777778      1.388889   \n",
       "4          3.143                   3.286     0.000000      0.000000   \n",
       "..           ...                     ...          ...           ...   \n",
       "203        4.143                   4.643     5.970149      2.985075   \n",
       "204        3.143                   3.286     5.555556      2.777778   \n",
       "205        3.143                   3.286     8.620690      3.448276   \n",
       "206        3.571                   3.714     2.325581      3.488372   \n",
       "207        3.143                   3.286     5.970149      2.985075   \n",
       "\n",
       "     fear_count  joy_count  sadness_count  surprise_count  hope_count  \n",
       "0      6.349206   4.761905       9.523810        1.587302    0.000000  \n",
       "1      7.936508   4.761905       4.761905        1.587302    0.000000  \n",
       "2      1.754386  10.526316       8.771930        8.771930    7.017544  \n",
       "3      5.555556   6.944444       8.333333        2.777778    1.388889  \n",
       "4      1.315789   2.631579       3.947368        1.315789    1.315789  \n",
       "..          ...        ...            ...             ...         ...  \n",
       "203    7.462687   4.477612      11.940299        5.970149    1.492537  \n",
       "204    2.777778   5.555556       5.555556        1.388889    0.000000  \n",
       "205   10.344828   3.448276       6.896552        1.724138    3.448276  \n",
       "206    4.651163   2.325581       3.488372        0.000000    1.162791  \n",
       "207    7.462687   0.000000       7.462687        2.985075    0.000000  \n",
       "\n",
       "[208 rows x 31 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in train_df.iterrows():\n",
    "    word_count = len(row['essay'].split())\n",
    "    for category in categories:\n",
    "\t    train_df.loc[index, f'{category}_count'] /= (word_count/100)\n",
    "\n",
    "train_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['conversation_id', 'article_id', 'essay', 'speaker_id', 'gender',\n",
       "       'education', 'race', 'age', 'income', 'speaker_number', 'split',\n",
       "       'essay_id', 'empathy', 'distress', 'emotion',\n",
       "       'personality_conscientiousness', 'personality_openess',\n",
       "       'personality_extraversion', 'personality_agreeableness',\n",
       "       'personality_stability', 'iri_perspective_taking',\n",
       "       'iri_personal_distress', 'iri_fantasy', 'iri_empathatic_concern',\n",
       "       'anger_count', 'disgust_count', 'fear_count', 'joy_count',\n",
       "       'sadness_count', 'surprise_count', 'hope_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv(\"datasets/train_essay_lev_preproc.tsv\", index=False, sep='\\t')\n",
    "train_df.to_csv(\"datasets/dev_essay_lev_preproc.tsv\", index=False, sep='\\t')\n",
    "#train_df.to_csv(\"datasets/test_essay_lev_preproc.tsv\", index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
