{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\giuli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\giuli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_38904\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_38904\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_38904\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_38904\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_38904\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_38904\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "C:\\Users\\giuli\\AppData\\Local\\Temp\\ipykernel_38904\\445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n"
     ]
    }
   ],
   "source": [
    "categories = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'hope']\n",
    "categories_dfs = {}\n",
    "for category in categories:\n",
    "\tcategories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>hope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11384</th>\n",
       "      <td>aback</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11830</th>\n",
       "      <td>abacus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281</th>\n",
       "      <td>zone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>zoo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7907</th>\n",
       "      <td>zoological</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13967</th>\n",
       "      <td>zoology</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5509</th>\n",
       "      <td>zoom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14153 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  anger  disgust  fear  joy  sadness  surprise  hope\n",
       "11384        aback      0        0     0    0        0         0     0\n",
       "11830       abacus      0        0     0    0        0         0     0\n",
       "1324       abandon      0        0     1    0        1         0     0\n",
       "663      abandoned      1        0     1    0        1         0     0\n",
       "609    abandonment      1        0     1    0        1         1     0\n",
       "...            ...    ...      ...   ...  ...      ...       ...   ...\n",
       "5281          zone      0        0     0    0        0         0     0\n",
       "5458           zoo      0        0     0    0        0         0     0\n",
       "7907    zoological      0        0     0    0        0         0     0\n",
       "13967      zoology      0        0     0    0        0         0     0\n",
       "5509          zoom      0        0     0    0        0         0     0\n",
       "\n",
       "[14153 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = pd.DataFrame(columns=['word'])\n",
    "for category in categories:\n",
    "    lexicon = pd.merge(lexicon, categories_dfs[category], on='word', how='outer')\n",
    "lexicon.dropna(inplace=True) # row with empty string\n",
    "lexicon.sort_values(by='word', inplace=True)\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN_DATA_PATH = \"./datasets/WASSA23_essay_level_with_labels_train.tsv\"\n",
    "#TRAIN_DATA_PATH = \"./datasets/WASSA23_essay_level_dev_preproc.tsv\"\n",
    "TRAIN_DATA_PATH = \"datasets/WASSA23_essay_level_test_preproc.tsv\"\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>hope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abduct</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aberr</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abhor</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abhorr</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zani</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeal</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealou</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zest</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2984 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        anger disgust fear joy sadness surprise hope\n",
       "abandon     1       0    1   0       1        1    0\n",
       "abduct      0       0    1   0       1        1    0\n",
       "aberr       0       1    0   0       0        0    0\n",
       "abhor       1       1    1   0       0        0    0\n",
       "abhorr      1       1    1   0       0        0    0\n",
       "...       ...     ...  ...  ..     ...      ...  ...\n",
       "youth       1       0    1   1       0        1    0\n",
       "zani        0       0    0   0       0        1    0\n",
       "zeal        0       0    0   1       0        1    1\n",
       "zealou      0       0    0   1       0        0    0\n",
       "zest        0       0    0   1       0        0    1\n",
       "\n",
       "[2984 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "preproc_lexicon = {}\n",
    "lexicon['preproc_word'] = ''\n",
    "for index, row in lexicon.iterrows():\n",
    "    lemma = lemmatizer.lemmatize(row['word'])\n",
    "    stemma = stemmer.stem(lemma)\n",
    "    lexicon.loc[index, 'preproc_word'] = stemma\n",
    "    if stemma in preproc_lexicon:\n",
    "        preproc_lexicon[stemma] += lexicon.loc[index, categories] # |=\n",
    "    else:\n",
    "        preproc_lexicon[stemma] = lexicon.loc[index, categories]\n",
    "preproc_lexicon = pd.DataFrame(preproc_lexicon).T\n",
    "preproc_lexicon[preproc_lexicon > 0] = 1\n",
    "preproc_lexicon = preproc_lexicon.loc[(preproc_lexicon!=0).any(axis=1)]\n",
    "preproc_lexicon.to_csv('./lexicon/preproc_lexicon.csv')\n",
    "lexicon.to_csv('./lexicon/lexicon.csv', index=False)\n",
    "preproc_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # python library to recoginze punctuation and digit\n",
    "import contractions # python library to recoginze contractions\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_digit(text):\n",
    "    return re.sub('\\d+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>speaker_number</th>\n",
       "      <th>split</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>anger_count</th>\n",
       "      <th>disgust_count</th>\n",
       "      <th>fear_count</th>\n",
       "      <th>joy_count</th>\n",
       "      <th>sadness_count</th>\n",
       "      <th>surprise_count</th>\n",
       "      <th>hope_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1192</td>\n",
       "      <td>6</td>\n",
       "      <td>hi my dear friend, how r you?. Are you used in...</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1192</td>\n",
       "      <td>6</td>\n",
       "      <td>The loss of human life is always a tragic even...</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>30000</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1059</td>\n",
       "      <td>6</td>\n",
       "      <td>Hello mate, it's been a long time since I met ...</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>40000</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1059</td>\n",
       "      <td>6</td>\n",
       "      <td>us America and the security council adopts res...</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1060</td>\n",
       "      <td>6</td>\n",
       "      <td>My thought was NATO and government forces are ...</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>75000</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1183</td>\n",
       "      <td>19</td>\n",
       "      <td>Check out this story. It is an outrage that so...</td>\n",
       "      <td>143</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>95</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1184</td>\n",
       "      <td>19</td>\n",
       "      <td>I just read a story about a person who was bro...</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>55000</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>96</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1184</td>\n",
       "      <td>19</td>\n",
       "      <td>I just read this article and it's really messe...</td>\n",
       "      <td>144</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>36000</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>97</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1190</td>\n",
       "      <td>19</td>\n",
       "      <td>I just read about a man who is being deported ...</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>36000</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>98</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1190</td>\n",
       "      <td>19</td>\n",
       "      <td>I feel very sorry for the individual in this a...</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>275000</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    conversation_id  article_id  \\\n",
       "0              1192           6   \n",
       "1              1192           6   \n",
       "2              1059           6   \n",
       "3              1059           6   \n",
       "4              1060           6   \n",
       "..              ...         ...   \n",
       "95             1183          19   \n",
       "96             1184          19   \n",
       "97             1184          19   \n",
       "98             1190          19   \n",
       "99             1190          19   \n",
       "\n",
       "                                                essay  speaker_id  gender  \\\n",
       "0   hi my dear friend, how r you?. Are you used in...          45       2   \n",
       "1   The loss of human life is always a tragic even...          65       1   \n",
       "2   Hello mate, it's been a long time since I met ...          53       2   \n",
       "3   us America and the security council adopts res...          92       2   \n",
       "4   My thought was NATO and government forces are ...          47       2   \n",
       "..                                                ...         ...     ...   \n",
       "95  Check out this story. It is an outrage that so...         143       2   \n",
       "96  I just read a story about a person who was bro...         145       1   \n",
       "97  I just read this article and it's really messe...         144       2   \n",
       "98  I just read about a man who is being deported ...         130       1   \n",
       "99  I feel very sorry for the individual in this a...         149       1   \n",
       "\n",
       "    education  race  age  income  speaker_number split  essay_id  anger_count  \\\n",
       "0           6     1   25   50000               2  test         0            2   \n",
       "1           6     1   34   30000               1  test         1            9   \n",
       "2           6     1   64   40000               2  test         2            9   \n",
       "3           6     1   35    5000               1  test         3           11   \n",
       "4           7     1   34   75000               1  test         4           10   \n",
       "..        ...   ...  ...     ...             ...   ...       ...          ...   \n",
       "95          6     1   42   50000               1  test        95            6   \n",
       "96          4     1   63   55000               2  test        96            8   \n",
       "97          6     3   31   36000               1  test        97            9   \n",
       "98          2     1   30   36000               2  test        98            5   \n",
       "99          7     1   56  275000               1  test        99            3   \n",
       "\n",
       "    disgust_count  fear_count  joy_count  sadness_count  surprise_count  \\\n",
       "0               1           2          4              1               0   \n",
       "1               3          10          3             11               2   \n",
       "2               0          11          4             11               3   \n",
       "3               3           9          2              4               3   \n",
       "4               3          14          0             10               3   \n",
       "..            ...         ...        ...            ...             ...   \n",
       "95              5           5          8              5               1   \n",
       "96              3           7          3             10               2   \n",
       "97              6           9          2              9               1   \n",
       "98              3           5          3              6               1   \n",
       "99              2           3          5              7               4   \n",
       "\n",
       "    hope_count  \n",
       "0            2  \n",
       "1            2  \n",
       "2            2  \n",
       "3            1  \n",
       "4            2  \n",
       "..         ...  \n",
       "95           0  \n",
       "96           0  \n",
       "97           0  \n",
       "98           1  \n",
       "99           2  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for category in categories:\n",
    "\ttrain_df[f'{category}_count'] = [0 for _ in range(len(train_df))]\n",
    "\n",
    "for index, row in train_df.iterrows(): #TODO: controllare ( on no?)\n",
    "\tfor word in row['essay']:\n",
    "\t\tif word in lexicon['word']:\n",
    "\t\t\tfor category in categories:\n",
    "\t\t\t\ttrain_df.loc[index, f'{category}_count'] += lexicon.loc[word][category]\n",
    "\n",
    "\tessay = remove_punctuations(row['essay'])\n",
    "\tessay = remove_digit(essay)\n",
    "\tcontracs = [contractions.fix(word) for word in essay.split()]\n",
    "\tlemmas = [lemmatizer.lemmatize(contraction) for contraction in contracs]\n",
    "\tstemmas = [stemmer.stem(lemma) for lemma in lemmas]\n",
    "\n",
    "\tfor stemma in stemmas:\n",
    "\t\tif stemma in preproc_lexicon.index:\n",
    "\t\t\tfor category in categories:\n",
    "\t\t\t\ttrain_df.loc[index, f'{category}_count'] += preproc_lexicon.loc[stemma][category]\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>speaker_number</th>\n",
       "      <th>split</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>anger_count</th>\n",
       "      <th>disgust_count</th>\n",
       "      <th>fear_count</th>\n",
       "      <th>joy_count</th>\n",
       "      <th>sadness_count</th>\n",
       "      <th>surprise_count</th>\n",
       "      <th>hope_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1192</td>\n",
       "      <td>6</td>\n",
       "      <td>hi my dear friend, how r you?. Are you used in...</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>3.448276</td>\n",
       "      <td>1.724138</td>\n",
       "      <td>3.448276</td>\n",
       "      <td>6.896552</td>\n",
       "      <td>1.724138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.448276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1192</td>\n",
       "      <td>6</td>\n",
       "      <td>The loss of human life is always a tragic even...</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>30000</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>1</td>\n",
       "      <td>6.338028</td>\n",
       "      <td>2.112676</td>\n",
       "      <td>7.042254</td>\n",
       "      <td>2.112676</td>\n",
       "      <td>7.746479</td>\n",
       "      <td>1.408451</td>\n",
       "      <td>1.408451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1059</td>\n",
       "      <td>6</td>\n",
       "      <td>Hello mate, it's been a long time since I met ...</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>40000</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>2</td>\n",
       "      <td>7.258065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.870968</td>\n",
       "      <td>3.225806</td>\n",
       "      <td>8.870968</td>\n",
       "      <td>2.419355</td>\n",
       "      <td>1.612903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1059</td>\n",
       "      <td>6</td>\n",
       "      <td>us America and the security council adopts res...</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>3</td>\n",
       "      <td>8.593750</td>\n",
       "      <td>2.343750</td>\n",
       "      <td>7.031250</td>\n",
       "      <td>1.562500</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>2.343750</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1060</td>\n",
       "      <td>6</td>\n",
       "      <td>My thought was NATO and government forces are ...</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>75000</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>4</td>\n",
       "      <td>7.633588</td>\n",
       "      <td>2.290076</td>\n",
       "      <td>10.687023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.633588</td>\n",
       "      <td>2.290076</td>\n",
       "      <td>1.526718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1183</td>\n",
       "      <td>19</td>\n",
       "      <td>Check out this story. It is an outrage that so...</td>\n",
       "      <td>143</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>95</td>\n",
       "      <td>4.225352</td>\n",
       "      <td>3.521127</td>\n",
       "      <td>3.521127</td>\n",
       "      <td>5.633803</td>\n",
       "      <td>3.521127</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1184</td>\n",
       "      <td>19</td>\n",
       "      <td>I just read a story about a person who was bro...</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>55000</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>96</td>\n",
       "      <td>5.194805</td>\n",
       "      <td>1.948052</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>1.948052</td>\n",
       "      <td>6.493506</td>\n",
       "      <td>1.298701</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1184</td>\n",
       "      <td>19</td>\n",
       "      <td>I just read this article and it's really messe...</td>\n",
       "      <td>144</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>36000</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>97</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1190</td>\n",
       "      <td>19</td>\n",
       "      <td>I just read about a man who is being deported ...</td>\n",
       "      <td>130</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>36000</td>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>98</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>4.545455</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1190</td>\n",
       "      <td>19</td>\n",
       "      <td>I feel very sorry for the individual in this a...</td>\n",
       "      <td>149</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>275000</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>99</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>2.105263</td>\n",
       "      <td>3.157895</td>\n",
       "      <td>5.263158</td>\n",
       "      <td>7.368421</td>\n",
       "      <td>4.210526</td>\n",
       "      <td>2.105263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    conversation_id  article_id  \\\n",
       "0              1192           6   \n",
       "1              1192           6   \n",
       "2              1059           6   \n",
       "3              1059           6   \n",
       "4              1060           6   \n",
       "..              ...         ...   \n",
       "95             1183          19   \n",
       "96             1184          19   \n",
       "97             1184          19   \n",
       "98             1190          19   \n",
       "99             1190          19   \n",
       "\n",
       "                                                essay  speaker_id  gender  \\\n",
       "0   hi my dear friend, how r you?. Are you used in...          45       2   \n",
       "1   The loss of human life is always a tragic even...          65       1   \n",
       "2   Hello mate, it's been a long time since I met ...          53       2   \n",
       "3   us America and the security council adopts res...          92       2   \n",
       "4   My thought was NATO and government forces are ...          47       2   \n",
       "..                                                ...         ...     ...   \n",
       "95  Check out this story. It is an outrage that so...         143       2   \n",
       "96  I just read a story about a person who was bro...         145       1   \n",
       "97  I just read this article and it's really messe...         144       2   \n",
       "98  I just read about a man who is being deported ...         130       1   \n",
       "99  I feel very sorry for the individual in this a...         149       1   \n",
       "\n",
       "    education  race  age  income  speaker_number split  essay_id  anger_count  \\\n",
       "0           6     1   25   50000               2  test         0     3.448276   \n",
       "1           6     1   34   30000               1  test         1     6.338028   \n",
       "2           6     1   64   40000               2  test         2     7.258065   \n",
       "3           6     1   35    5000               1  test         3     8.593750   \n",
       "4           7     1   34   75000               1  test         4     7.633588   \n",
       "..        ...   ...  ...     ...             ...   ...       ...          ...   \n",
       "95          6     1   42   50000               1  test        95     4.225352   \n",
       "96          4     1   63   55000               2  test        96     5.194805   \n",
       "97          6     3   31   36000               1  test        97     7.500000   \n",
       "98          2     1   30   36000               2  test        98     4.545455   \n",
       "99          7     1   56  275000               1  test        99     3.157895   \n",
       "\n",
       "    disgust_count  fear_count  joy_count  sadness_count  surprise_count  \\\n",
       "0        1.724138    3.448276   6.896552       1.724138        0.000000   \n",
       "1        2.112676    7.042254   2.112676       7.746479        1.408451   \n",
       "2        0.000000    8.870968   3.225806       8.870968        2.419355   \n",
       "3        2.343750    7.031250   1.562500       3.125000        2.343750   \n",
       "4        2.290076   10.687023   0.000000       7.633588        2.290076   \n",
       "..            ...         ...        ...            ...             ...   \n",
       "95       3.521127    3.521127   5.633803       3.521127        0.704225   \n",
       "96       1.948052    4.545455   1.948052       6.493506        1.298701   \n",
       "97       5.000000    7.500000   1.666667       7.500000        0.833333   \n",
       "98       2.727273    4.545455   2.727273       5.454545        0.909091   \n",
       "99       2.105263    3.157895   5.263158       7.368421        4.210526   \n",
       "\n",
       "    hope_count  \n",
       "0     3.448276  \n",
       "1     1.408451  \n",
       "2     1.612903  \n",
       "3     0.781250  \n",
       "4     1.526718  \n",
       "..         ...  \n",
       "95    0.000000  \n",
       "96    0.000000  \n",
       "97    0.000000  \n",
       "98    0.909091  \n",
       "99    2.105263  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in train_df.iterrows():\n",
    "    word_count = len(row['essay'].split())\n",
    "    for category in categories:\n",
    "\t    train_df.loc[index, f'{category}_count'] /= (word_count/100)\n",
    "\n",
    "train_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['conversation_id', 'article_id', 'essay', 'speaker_id', 'gender',\n",
       "       'education', 'race', 'age', 'income', 'speaker_number', 'split',\n",
       "       'essay_id', 'anger_count', 'disgust_count', 'fear_count', 'joy_count',\n",
       "       'sadness_count', 'surprise_count', 'hope_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv(\"datasets/train_essay_level_preproc.tsv\", index=False, sep='\\t')\n",
    "#train_df.to_csv(\"datasets/dev_essay_level_preproc.tsv\", index=False, sep='\\t')\n",
    "train_df.to_csv(\"datasets/test_essay_level_preproc.tsv\", index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
