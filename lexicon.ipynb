{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/irenetesta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/irenetesta/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/24/k023d_r17kq_q1b7bj27y7t80000gn/T/ipykernel_1164/445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "/var/folders/24/k023d_r17kq_q1b7bj27y7t80000gn/T/ipykernel_1164/445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "/var/folders/24/k023d_r17kq_q1b7bj27y7t80000gn/T/ipykernel_1164/445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "/var/folders/24/k023d_r17kq_q1b7bj27y7t80000gn/T/ipykernel_1164/445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "/var/folders/24/k023d_r17kq_q1b7bj27y7t80000gn/T/ipykernel_1164/445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "/var/folders/24/k023d_r17kq_q1b7bj27y7t80000gn/T/ipykernel_1164/445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n",
      "/var/folders/24/k023d_r17kq_q1b7bj27y7t80000gn/T/ipykernel_1164/445186173.py:4: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  categories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)\n"
     ]
    }
   ],
   "source": [
    "categories = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'hope']\n",
    "categories_dfs = {}\n",
    "for category in categories:\n",
    "\tcategories_dfs[category] = pd.read_csv(f\"./lexicon/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>hope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11384</th>\n",
       "      <td>aback</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11830</th>\n",
       "      <td>abacus</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>abandon</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>abandoned</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>abandonment</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5281</th>\n",
       "      <td>zone</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>zoo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7907</th>\n",
       "      <td>zoological</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13967</th>\n",
       "      <td>zoology</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5509</th>\n",
       "      <td>zoom</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14153 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  anger  disgust  fear  joy  sadness  surprise  hope\n",
       "11384        aback      0        0     0    0        0         0     0\n",
       "11830       abacus      0        0     0    0        0         0     0\n",
       "1324       abandon      0        0     1    0        1         0     0\n",
       "663      abandoned      1        0     1    0        1         0     0\n",
       "609    abandonment      1        0     1    0        1         1     0\n",
       "...            ...    ...      ...   ...  ...      ...       ...   ...\n",
       "5281          zone      0        0     0    0        0         0     0\n",
       "5458           zoo      0        0     0    0        0         0     0\n",
       "7907    zoological      0        0     0    0        0         0     0\n",
       "13967      zoology      0        0     0    0        0         0     0\n",
       "5509          zoom      0        0     0    0        0         0     0\n",
       "\n",
       "[14153 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = pd.DataFrame(columns=['word'])\n",
    "for category in categories:\n",
    "    lexicon = pd.merge(lexicon, categories_dfs[category], on='word', how='outer')\n",
    "lexicon.dropna(inplace=True) # row with empty string\n",
    "lexicon.sort_values(by='word', inplace=True)\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"./datasets/WASSA23_essay_level_with_labels_train.tsv\"\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>hope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abandon</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abduct</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aberr</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abhor</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abhorr</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>youth</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zani</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zeal</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zealou</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zest</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3052 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        anger disgust fear joy sadness surprise hope\n",
       "abandon     1       0    1   0       1        1    0\n",
       "abduct      0       0    1   0       1        1    0\n",
       "aberr       0       1    0   0       0        0    0\n",
       "abhor       1       1    1   0       0        0    0\n",
       "abhorr      1       1    1   0       0        0    0\n",
       "...       ...     ...  ...  ..     ...      ...  ...\n",
       "youth       1       0    1   1       0        1    1\n",
       "zani        0       0    0   0       0        1    0\n",
       "zeal        0       0    0   1       0        1    1\n",
       "zealou      0       0    0   1       0        0    0\n",
       "zest        0       0    0   1       0        0    1\n",
       "\n",
       "[3052 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "preproc_lexicon = {}\n",
    "lexicon['preproc_word'] = ''\n",
    "for index, row in lexicon.iterrows():\n",
    "    lemma = lemmatizer.lemmatize(row['word'])\n",
    "    stemma = stemmer.stem(lemma)\n",
    "    lexicon.loc[index, 'preproc_word'] = stemma\n",
    "    if stemma in preproc_lexicon:\n",
    "        preproc_lexicon[stemma] += lexicon.loc[index, categories] # |=\n",
    "    else:\n",
    "        preproc_lexicon[stemma] = lexicon.loc[index, categories]\n",
    "preproc_lexicon = pd.DataFrame(preproc_lexicon).T\n",
    "preproc_lexicon[preproc_lexicon > 0] = 1\n",
    "preproc_lexicon = preproc_lexicon.loc[(preproc_lexicon!=0).any(axis=1)]\n",
    "preproc_lexicon.to_csv('./lexicon/preproc_lexicon.csv')\n",
    "lexicon.to_csv('./lexicon/lexicon.csv', index=False)\n",
    "preproc_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', 'break', 'my', 'heart', 'to', 'see', 'peopl', 'live', 'in', 'those', 'conditions.', 'i', 'hope', 'that', 'all', 'the', 'aid', 'that', 'wa', 'sent', 'to', 'the', 'island', 'make', 'it', 'to', 'the', 'peopl', 'who', 'need', 'it', 'the', 'most.', 'i', 'do', 'not', 'know', 'what', 'i', 'would', 'do', 'it', 'that', 'wa', 'my', 'famili', 'and', 'i.', 'i', 'would', 'hope', 'that', 'i', 'would', 'do', 'my', 'best,', 'but', 'i', 'can', 'see', 'how', 'depress', 'and', 'hopeless', 'you', 'could', 'feel', 'have', 'your', 'whole', 'life', 'chang', 'becaus', 'of', 'a', 'storm', 'and', 'not', 'know', 'where', 'your', 'next', 'meal', 'is', 'come', 'from.']\n",
      "break\n",
      "hope\n",
      "hope\n",
      "depress\n",
      "hopeless\n",
      "feel\n",
      "chang\n",
      "storm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>empathy</th>\n",
       "      <th>distress</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>...</th>\n",
       "      <th>split</th>\n",
       "      <th>essay_id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>anger_count</th>\n",
       "      <th>disgust_count</th>\n",
       "      <th>fear_count</th>\n",
       "      <th>joy_count</th>\n",
       "      <th>sadness_count</th>\n",
       "      <th>surprise_count</th>\n",
       "      <th>hope_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>It breaks my heart to see people living in tho...</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>6.625</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>Hope/Sadness</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>I wonder why there aren't more people trying t...</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>6.000</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>Anger</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>After reading the article, you can't help but ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.375</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>train</td>\n",
       "      <td>4</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>213</td>\n",
       "      <td>It is so sad that someone who had such an amaz...</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>6.625</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>train</td>\n",
       "      <td>5</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>213</td>\n",
       "      <td>From reading the article, it looks like the wo...</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>1.000</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>train</td>\n",
       "      <td>7</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>495</td>\n",
       "      <td>218</td>\n",
       "      <td>I feel that this will become a national proble...</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>6.750</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>train</td>\n",
       "      <td>994</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>496</td>\n",
       "      <td>103</td>\n",
       "      <td>The whole situation is sketchy. The wavering r...</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>6.375</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>train</td>\n",
       "      <td>995</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>498</td>\n",
       "      <td>103</td>\n",
       "      <td>The death of a former aide to Russian Presiden...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>train</td>\n",
       "      <td>997</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>499</td>\n",
       "      <td>103</td>\n",
       "      <td>Everything about Russia really freaks me out. ...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>train</td>\n",
       "      <td>998</td>\n",
       "      <td>Fear</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>500</td>\n",
       "      <td>103</td>\n",
       "      <td>Whenever Russia and Putin are involved  I do n...</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>4.875</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>train</td>\n",
       "      <td>999</td>\n",
       "      <td>Sadness</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>792 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     conversation_id  article_id  \\\n",
       "0                  2          35   \n",
       "1                  3          35   \n",
       "2                  5          35   \n",
       "3                  6         213   \n",
       "4                  8         213   \n",
       "..               ...         ...   \n",
       "787              495         218   \n",
       "788              496         103   \n",
       "789              498         103   \n",
       "790              499         103   \n",
       "791              500         103   \n",
       "\n",
       "                                                 essay   empathy  distress  \\\n",
       "0    It breaks my heart to see people living in tho...  6.833333     6.625   \n",
       "1    I wonder why there aren't more people trying t...  5.833333     6.000   \n",
       "2    After reading the article, you can't help but ...  1.000000     1.375   \n",
       "3    It is so sad that someone who had such an amaz...  6.166667     6.625   \n",
       "4    From reading the article, it looks like the wo...  6.833333     1.000   \n",
       "..                                                 ...       ...       ...   \n",
       "787  I feel that this will become a national proble...  6.500000     6.750   \n",
       "788  The whole situation is sketchy. The wavering r...  3.166667     6.375   \n",
       "789  The death of a former aide to Russian Presiden...  6.000000     2.000   \n",
       "790  Everything about Russia really freaks me out. ...  6.000000     6.000   \n",
       "791  Whenever Russia and Putin are involved  I do n...  1.500000     4.875   \n",
       "\n",
       "     speaker_id gender education race age  ...  split essay_id       emotion  \\\n",
       "0            30      1         6    3  37  ...  train        1  Hope/Sadness   \n",
       "1            19      1         6    2  32  ...  train        2         Anger   \n",
       "2            17      1         6    1  29  ...  train        4       Sadness   \n",
       "3            16      2         5    1  28  ...  train        5       Sadness   \n",
       "4            30      1         6    3  37  ...  train        7       Neutral   \n",
       "..          ...    ...       ...  ...  ..  ...    ...      ...           ...   \n",
       "787          30      1         6    3  37  ...  train      994       Neutral   \n",
       "788          16      2         5    1  28  ...  train      995       Neutral   \n",
       "789          43      2         6    1  33  ...  train      997       Neutral   \n",
       "790          53      2         3    1  27  ...  train      998          Fear   \n",
       "791          30      1         6    3  37  ...  train      999       Sadness   \n",
       "\n",
       "    anger_count disgust_count fear_count joy_count sadness_count  \\\n",
       "0             4             3          4         3             3   \n",
       "1             0             0          0         0             0   \n",
       "2             0             0          0         0             0   \n",
       "3             0             0          0         0             0   \n",
       "4             0             0          0         0             0   \n",
       "..          ...           ...        ...       ...           ...   \n",
       "787           0             0          0         0             0   \n",
       "788           0             0          0         0             0   \n",
       "789           0             0          0         0             0   \n",
       "790           0             0          0         0             0   \n",
       "791           0             0          0         0             0   \n",
       "\n",
       "    surprise_count hope_count  \n",
       "0                4          3  \n",
       "1                0          0  \n",
       "2                0          0  \n",
       "3                0          0  \n",
       "4                0          0  \n",
       "..             ...        ...  \n",
       "787              0          0  \n",
       "788              0          0  \n",
       "789              0          0  \n",
       "790              0          0  \n",
       "791              0          0  \n",
       "\n",
       "[792 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for category in categories:\n",
    "\ttrain_df[f'{category}_count'] = [0 for _ in range(len(train_df))]\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "\tlemmas = [lemmatizer.lemmatize(word) for word in row['essay'].split()]\n",
    "\tstemmas = [stemmer.stem(lemma) for lemma in lemmas]\n",
    "\tprint(stemmas)\n",
    "\tfor stemma in stemmas:\n",
    "\t\tif stemma in preproc_lexicon.index:\n",
    "\t\t\tprint(stemma)\n",
    "\t\t\tfor category in categories:\n",
    "\t\t\t\ttrain_df.loc[index, f'{category}_count'] += preproc_lexicon.loc[stemma][category]\n",
    "\tbreak\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.4357142857142857\n",
      "0.24523809523809526\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "print(TextBlob('think').sentiment.subjectivity)\n",
    "print(TextBlob('I think so').sentiment.subjectivity)\n",
    "print(TextBlob('I think so!').sentiment.subjectivity)\n",
    "print(TextBlob('Textblob is amazingly simple to use. What great fun!').sentiment.subjectivity)\n",
    "count = 0\n",
    "words = 'Textblob is amazingly simple to use. What great fun!'.split()\n",
    "for word in words:\n",
    "    count += TextBlob(word).sentiment.subjectivity\n",
    "count /= len(words)\n",
    "print(count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
