{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "l = []\n",
    "for i in range(len(gold_emotions)):\n",
    "  if gold_emotions[i] != predicted_emotions[i]:\n",
    "    l.append(i)\n",
    "  \n",
    "s = []\n",
    "for i in l:\n",
    "  s.append(outs.predictions[0][i])\n",
    "\n",
    "# open a file, where you ant to store the data\n",
    "file = open('logist_errate.pkl', 'wb')\n",
    "\n",
    "# dump information to that file\n",
    "pickle.dump(s, file)\n",
    "\n",
    "# close the file\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = 'logist_errate.pkl'\n",
    "obj = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-8.064367 , -7.201281 , -6.8728104, -5.240647 , -6.4233584,\n",
       "         0.7787335,  1.9870163, -7.222992 ], dtype=float32),\n",
       " array([-6.944066 ,  1.7112933, -6.1993055, -6.051587 , -6.8772316,\n",
       "        -1.3442721, -3.657677 , -5.2473454], dtype=float32),\n",
       " array([-4.6992183, -4.982564 , -6.7400994, -6.209856 , -6.613549 ,\n",
       "         4.45661  , -6.5877223, -6.761219 ], dtype=float32),\n",
       " array([-4.8130465, -4.5439067, -3.2511902, -6.1554418, -6.4552383,\n",
       "        -7.0971675,  6.6741643, -6.376077 ], dtype=float32),\n",
       " array([-5.1539335, -6.333181 , -5.293275 , -7.0705185, -7.1192274,\n",
       "        -4.202336 ,  5.933602 , -7.339711 ], dtype=float32),\n",
       " array([-3.8355594,  4.414359 , -5.0458574, -5.4942336, -5.8431177,\n",
       "        -6.23308  ,  1.7687093, -5.4080048], dtype=float32),\n",
       " array([-6.409288 , -6.191746 , -5.83625  , -5.684992 , -6.1039276,\n",
       "         4.9795356, -5.9744163, -6.1504884], dtype=float32),\n",
       " array([-6.221209 , -6.0350413, -5.4647164, -6.5888824, -6.9954085,\n",
       "        -6.434654 ,  5.80316  , -6.500257 ], dtype=float32),\n",
       " array([-6.284244 , -6.4838324, -6.634144 , -4.7318416, -7.145549 ,\n",
       "        -5.8867307,  5.9607797, -6.9962487], dtype=float32),\n",
       " array([-5.998175 , -6.3112483, -7.089682 , -7.337433 , -7.558521 ,\n",
       "         2.8974028, -1.7447139, -7.078431 ], dtype=float32)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/irenetesta/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nrclex import NRCLex\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def generate_prompt(essay, gender, education, ethnicity, age, income, empathy, distress):\n",
    "    if gender == 1: gender_str = \"male\"\n",
    "    else: gender_str = \"female\"\n",
    "\n",
    "    if education == 1: education_str = \"with less than a high school diploma\"\n",
    "    elif education == 2: education_str = \"with a high school diploma\"\n",
    "    elif education == 3: education_str = \"went to a technical/vocational school\"\n",
    "    elif education == 4: education_str = \"went to college\"\n",
    "    elif education == 5: education_str = \"with a two year associate degree\"\n",
    "    elif education == 6: education_str = \"with a four year bachelor's degree\"\n",
    "    else: education_str = \"postgradute or with a professional degree\"\n",
    "\n",
    "    if ethnicity == 1: ethnicity_str = \" white\"\n",
    "    elif ethnicity == 2: ethnicity_str = \" hispanic or latino\"\n",
    "    elif ethnicity == 3: ethnicity_str = \" black or african american\"\n",
    "    elif ethnicity == 4: ethnicity_str = \" native american or american indian\"\n",
    "    elif ethnicity == 5: ethnicity_str = \" asian/pacific islander\"\n",
    "    else: ethnicity_str = \"\"\n",
    "\n",
    "    text_prompt_bio = \"An essay written by a {} years old{} {}, {}, with an income of {}$.\".format(\n",
    "        age, ethnicity_str,\n",
    "        gender_str,\n",
    "        education_str,\n",
    "        income\n",
    "        )\n",
    "    \n",
    "    if empathy is not None:\n",
    "        if empathy < 3: empathy_value = \"low\"\n",
    "        elif empathy < 5: empathy_value = \"medium\"\n",
    "        else: empathy_value = \"high\"\n",
    "        if distress < 3: distress_value = \"low\"\n",
    "        elif distress < 5: distress_value = \"medium\"\n",
    "        else: distress_value = \"high\"\n",
    "        text_prompt_emp = \"The essay expresses {} empathy and {} distress levels.\".format(\n",
    "            empathy_value,\n",
    "            distress_value\n",
    "            )\n",
    "\n",
    "    emotions = NRCLex(essay).top_emotions\n",
    "    if (sum(np.array([emo[1] for emo in emotions])))==0:\n",
    "        emotions = {'neutral': 1}\n",
    "    n_emo = len(emotions)\n",
    "    emo_string = \"\"\n",
    "    for i, emo in enumerate(emotions):\n",
    "        emo_string += emo[0]\n",
    "        if i < n_emo-1:\n",
    "            emo_string += \", \"\n",
    "    text_prompt_emo = \" The top emotions expressed in the essay are: {}.\".format(emo_string)\n",
    "    \n",
    "    return text_prompt_bio, text_prompt_emp, text_prompt_emo\n",
    "\n",
    "def add_prompt(dataframe):\n",
    "    dataframe[\"prompt_bio\"] = \"\"\n",
    "    dataframe[\"prompt_emp\"] = \"\"\n",
    "    dataframe[\"prompt_emo\"] = \"\"\n",
    "    for idx, row in dataframe.iterrows():\n",
    "        bio_prompt, emp_prompt, emo_prompt = generate_prompt(\n",
    "            row['essay'],\n",
    "            row['gender'],\n",
    "            row['education'],\n",
    "            row['race'],\n",
    "            row['age'],\n",
    "            row['income'],\n",
    "            row['empathy'] if 'empathy' in row else None,\n",
    "            row['distress'] if 'empathy' in row else None\n",
    "            )\n",
    "        dataframe.at[idx, \"prompt_bio\"] = bio_prompt\n",
    "        dataframe.at[idx, \"prompt_emp\"] = emp_prompt\n",
    "        dataframe.at[idx, \"prompt_emo\"] = emo_prompt\n",
    "    return dataframe\n",
    "\n",
    "year=23\n",
    "\n",
    "TRAIN = f\"datasets/WASSA{year}_essay_level_original_internal_train_preproc.tsv\"\n",
    "VAL = f\"datasets/WASSA{year}_essay_level_original_internal_val_preproc.tsv\"\n",
    "DEV = f\"datasets/WASSA{year}_essay_level_dev_preproc.tsv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN, sep='\\t')\n",
    "val_df = pd.read_csv(VAL, sep='\\t')\n",
    "dev_df = pd.read_csv(DEV, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train = add_prompt(train_df)\n",
    "new_train.to_csv(f\"datasets/WASSA{year}_essay_level_original_internal_train_preproc_prompt.tsv\", index=False, sep='\\t')\n",
    "new_val = add_prompt(val_df)\n",
    "new_val.to_csv(f\"datasets/WASSA{year}_essay_level_original_internal_val_preproc_prompt.tsv\", index=False, sep='\\t')\n",
    "new_dev = add_prompt(dev_df)\n",
    "new_dev.to_csv(f\"datasets/WASSA{year}_essay_level_dev_preproc_prompt.tsv\", index=False, sep='\\t')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
