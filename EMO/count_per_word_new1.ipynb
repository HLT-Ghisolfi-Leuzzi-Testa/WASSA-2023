{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EMO task","metadata":{"id":"9_B-_QwdhWLe"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9sDeMksltGSk","outputId":"ffe7fb51-949b-4004-fd1a-5849a35e50ee"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Mounted at /content/drive\n"}]},{"cell_type":"markdown","source":"## Dependencies","metadata":{"id":"15KoBLrVhbI8"}},{"cell_type":"code","source":"!pip install transformers -q\n!pip install accelerate -U -q\n!pip install datasets -q\n!pip install torch-summary -q\n!pip install graphviz -q\n!pip install torchview -q\n!pip install bertviz -q\n\nrepo_path = \"https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/\"\nbranch = \"main\"","metadata":{"id":"1gnWTVNYDMx2","execution":{"iopub.status.busy":"2023-07-06T18:02:04.441466Z","iopub.execute_input":"2023-07-06T18:02:04.442370Z","iopub.status.idle":"2023-07-06T18:03:37.136890Z","shell.execute_reply.started":"2023-07-06T18:02:04.442325Z","shell.execute_reply":"2023-07-06T18:03:37.135684Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.5.0 requires botocore<1.29.77,>=1.29.76, but you have botocore 1.29.165 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"utils_url = f\"{repo_path}{branch}/utils.py\"\nevaluation_url = f\"{repo_path}{branch}/evaluation.py\"\n\nimport os\nif os.path.exists(\"utils.py\"):\n  !rm \"utils.py\"\nif os.path.exists(\"evaluation.py\"):\n  !rm \"evaluation.py\"\n\n!wget {utils_url}\n!wget {evaluation_url}","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Xbo-Ei3IAYB","outputId":"c24494d1-967a-405c-f837-4653b59aa33b","execution":{"iopub.status.busy":"2023-07-06T18:03:56.623941Z","iopub.execute_input":"2023-07-06T18:03:56.624528Z","iopub.status.idle":"2023-07-06T18:04:00.774553Z","shell.execute_reply.started":"2023-07-06T18:03:56.624496Z","shell.execute_reply":"2023-07-06T18:04:00.773349Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"--2023-07-06 18:03:59--  https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/utils.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 19809 (19K) [text/plain]\nSaving to: ‘utils.py’\n\nutils.py            100%[===================>]  19.34K  --.-KB/s    in 0.001s  \n\n2023-07-06 18:03:59 (28.3 MB/s) - ‘utils.py’ saved [19809/19809]\n\n--2023-07-06 18:04:00--  https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/evaluation.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10675 (10K) [text/plain]\nSaving to: ‘evaluation.py’\n\nevaluation.py       100%[===================>]  10.42K  --.-KB/s    in 0s      \n\n2023-07-06 18:04:00 (43.1 MB/s) - ‘evaluation.py’ saved [10675/10675]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport torch\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\nfrom transformers import TrainingArguments, Trainer, TrainerCallback, EarlyStoppingCallback\nfrom transformers import BertPreTrainedModel, BertModel\nfrom transformers import RobertaModel,RobertaPreTrainedModel\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nimport importlib\nimport sys\nfrom utils import *\nimportlib.reload(sys.modules['utils'])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDmQslVxDhy1","outputId":"2dfb24ae-392b-4750-da85-df6f7be8c1df","execution":{"iopub.status.busy":"2023-07-06T18:04:28.566620Z","iopub.execute_input":"2023-07-06T18:04:28.567642Z","iopub.status.idle":"2023-07-06T18:04:28.578029Z","shell.execute_reply.started":"2023-07-06T18:04:28.567605Z","shell.execute_reply":"2023-07-06T18:04:28.576725Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"<module 'utils' from '/kaggle/working/utils.py'>"},"metadata":{}}]},{"cell_type":"code","source":"# set CUDA if available\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(\"======= CUDA Available =======\")\nelse:\n    device = torch.device('cpu')\n    print(\"======= CUDA NOT Available, run on CPU =======\")\ndevice = torch.device('cpu') # otw goes out of memory","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quom7lWCDiiI","outputId":"7f3e0d1a-dee5-4645-ee5b-7398e4b4563f","execution":{"iopub.status.busy":"2023-07-06T18:04:28.579612Z","iopub.execute_input":"2023-07-06T18:04:28.580282Z","iopub.status.idle":"2023-07-06T18:04:28.657774Z","shell.execute_reply.started":"2023-07-06T18:04:28.580246Z","shell.execute_reply":"2023-07-06T18:04:28.656793Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"======= CUDA Available =======\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Dataset","metadata":{"id":"4gsXzUtCBv-j"}},{"cell_type":"markdown","source":"Dataset paths","metadata":{"id":"VUd7BQjSsVMJ"}},{"cell_type":"code","source":"TRAIN_DATA = f\"{repo_path}{branch}/datasets/WASSA23_essay_level_internal_train_preproc.tsv\" # f\"{repo_path}{branch}/datasets/WASSA23_essay_level_internal_train_preproc.tsv\"\nVAL_DATA = f\"{repo_path}{branch}/datasets/WASSA23_essay_level_internal_val_preproc.tsv\"\nDEV_DATA = f\"{repo_path}{branch}/datasets/WASSA23_essay_level_dev_preproc.tsv\"","metadata":{"id":"BjM03TscDwcz","execution":{"iopub.status.busy":"2023-07-06T18:04:28.659215Z","iopub.execute_input":"2023-07-06T18:04:28.659865Z","iopub.status.idle":"2023-07-06T18:04:28.667682Z","shell.execute_reply.started":"2023-07-06T18:04:28.659830Z","shell.execute_reply":"2023-07-06T18:04:28.666728Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Read dataframes","metadata":{"id":"EFI6AulQsYci"}},{"cell_type":"code","source":"train_df = pd.read_csv(TRAIN_DATA, sep='\\t')\nval_df = pd.read_csv(VAL_DATA, sep='\\t')\ndev_df = pd.read_csv(DEV_DATA, sep='\\t')","metadata":{"id":"Sppn6sPYsBqA","execution":{"iopub.status.busy":"2023-07-06T18:04:28.669244Z","iopub.execute_input":"2023-07-06T18:04:28.669620Z","iopub.status.idle":"2023-07-06T18:04:29.290441Z","shell.execute_reply.started":"2023-07-06T18:04:28.669587Z","shell.execute_reply":"2023-07-06T18:04:29.289461Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"label_encoder = EmotionsLabelEncoder()\nlabel_encoder.fit(train_df.emotion)","metadata":{"id":"1_tFO0RSlVm8","execution":{"iopub.status.busy":"2023-07-06T18:04:29.292048Z","iopub.execute_input":"2023-07-06T18:04:29.292437Z","iopub.status.idle":"2023-07-06T18:04:29.303911Z","shell.execute_reply.started":"2023-07-06T18:04:29.292400Z","shell.execute_reply":"2023-07-06T18:04:29.302340Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Optional subsample","metadata":{"id":"yn_tGpeFsNqb"}},{"cell_type":"code","source":"train_df = train_df\nval_df = val_df\ndev_df = dev_df","metadata":{"id":"wkCC492RsNSU","execution":{"iopub.status.busy":"2023-07-06T18:04:29.305634Z","iopub.execute_input":"2023-07-06T18:04:29.306728Z","iopub.status.idle":"2023-07-06T18:04:29.313885Z","shell.execute_reply.started":"2023-07-06T18:04:29.306691Z","shell.execute_reply":"2023-07-06T18:04:29.312894Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"y_train = label_encoder.encode(train_df.emotion)\ny_val = label_encoder.encode(val_df.emotion)\ny_dev = label_encoder.encode(dev_df.emotion)","metadata":{"id":"gMZ3v24xlgKt","execution":{"iopub.status.busy":"2023-07-06T18:04:29.315409Z","iopub.execute_input":"2023-07-06T18:04:29.315887Z","iopub.status.idle":"2023-07-06T18:04:29.342197Z","shell.execute_reply.started":"2023-07-06T18:04:29.315854Z","shell.execute_reply":"2023-07-06T18:04:29.341312Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Encode targets","metadata":{"id":"mDJWlHhwsfwL"}},{"cell_type":"code","source":"features_list = ['anger_count', 'disgust_count', 'fear_count', 'joy_count', 'sadness_count', 'surprise_count', 'hope_count']\nfeatures_train =  np.array(train_df[features_list])\nfeatures_val =  np.array(val_df[features_list])\nfeatures_dev =  np.array(dev_df[features_list])","metadata":{"id":"lteCDGLXtO7R","execution":{"iopub.status.busy":"2023-07-06T18:04:29.344247Z","iopub.execute_input":"2023-07-06T18:04:29.344630Z","iopub.status.idle":"2023-07-06T18:04:29.356508Z","shell.execute_reply.started":"2023-07-06T18:04:29.344596Z","shell.execute_reply":"2023-07-06T18:04:29.355468Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{"id":"oXIimY5ABmlc"}},{"cell_type":"code","source":"model_type ={\n  'distilroberta-emotion':'j-hartmann/emotion-english-distilroberta-base',\n  'roberta-emotion':'j-hartmann/emotion-english-roberta-large',\n  'bert-base':'bert-base-cased'\n}","metadata":{"id":"wtbV4tekujaC","execution":{"iopub.status.busy":"2023-07-06T18:04:29.358156Z","iopub.execute_input":"2023-07-06T18:04:29.358583Z","iopub.status.idle":"2023-07-06T18:04:29.370870Z","shell.execute_reply.started":"2023-07-06T18:04:29.358550Z","shell.execute_reply":"2023-07-06T18:04:29.369770Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"NUM_LABELS = 8\n\nmodel_config = {\n    'model_id': 'bert_lexicon',\n    'tokenizer_name': model_type.get('bert-base'),\n    'model_name': model_type.get('bert-base'),\n    'train_batch_size': 4,\n    'val_batch_size': 4,\n    'learning_rate': 5e-5,\n    'weight_decay': 0,\n    'epochs': 10,\n    'seed': 42,\n    'patience': 10,\n    'early_stopping_threshold': 0\n} # TODO: expand...","metadata":{"id":"dA224FxADpqd","execution":{"iopub.status.busy":"2023-07-06T18:04:29.372875Z","iopub.execute_input":"2023-07-06T18:04:29.373670Z","iopub.status.idle":"2023-07-06T18:04:29.386045Z","shell.execute_reply.started":"2023-07-06T18:04:29.373631Z","shell.execute_reply":"2023-07-06T18:04:29.385126Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_config['tokenizer_name'], truncation=True)","metadata":{"id":"wd7W2HhYsbai","execution":{"iopub.status.busy":"2023-07-06T18:04:29.395064Z","iopub.execute_input":"2023-07-06T18:04:29.395924Z","iopub.status.idle":"2023-07-06T18:04:30.234051Z","shell.execute_reply.started":"2023-07-06T18:04:29.395890Z","shell.execute_reply":"2023-07-06T18:04:30.232930Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d2eb3868b7d47808aae9f6cb413a871"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43d20973102047f3824900c5b81fb815"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4c29b9480d04904bd77c7d309fcb357"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e5c5803204d4c4fa1c90dcc695a7a0f"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Baseline model**","metadata":{"id":"FluTFZjYtzXo"}},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    'bert-base-cased',\n    num_labels=NUM_LABELS,\n    ignore_mismatched_sizes=True,\n    problem_type=\"multi_label_classification\")\nmodel.to(device)","metadata":{"id":"jT8tJyZusZ97","colab":{"base_uri":"https://localhost:8080/"},"outputId":"36dd8987-2c3a-4a94-9f1d-06cfd411f599","execution":{"iopub.status.busy":"2023-07-06T18:04:30.235779Z","iopub.execute_input":"2023-07-06T18:04:30.236170Z","iopub.status.idle":"2023-07-06T18:04:33.972790Z","shell.execute_reply.started":"2023-07-06T18:04:30.236133Z","shell.execute_reply":"2023-07-06T18:04:33.971638Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"786c4601392743d5b58ed9cf4c12c36d"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=8, bias=True)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"**Custom \"lexicon\" Bert model**","metadata":{"id":"w6aAhPift1UL"}},{"cell_type":"code","source":"\"\"\"num_features=0# features_train.shape[1]\nempathy_lexicon = True\ndistress_lexicon = True\nemotion_lexicon = True\nclass BertPerWordLexiconPooling(BertPreTrainedModel):\n  def __init__(self, config):\n    super().__init__(config)\n    self.num_labels = config.num_labels\n    self.config = config\n    self.n_features = num_features\n    self.empathy = 1 if empathy_lexicon else 0\n    self.distress = 1 if distress_lexicon else 0\n    self.emotion = 7 if emotion_lexicon else 0\n    self.bert = BertModel(config)\n    self.dropout = nn.Dropout(0.3)\n    self.classifier = nn.Linear(config.hidden_size+self.n_features+self.empathy+self.distress+self.emotion, config.num_labels)\n    self.post_init()\n\n  def forward(\n    self,\n    input_ids=None,\n    attention_mask=None,\n    token_type_ids=None,\n    position_ids=None,\n    head_mask=None,\n    inputs_embeds=None,\n    labels=None,\n    output_attentions=None,\n    output_hidden_states=None,\n    return_dict=None,\n    features=None,\n    empathy_values=None,\n    distress_values=None,\n    emotion_values=None,\n  ):\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.bert(\n      input_ids,\n      attention_mask=attention_mask,\n      token_type_ids=token_type_ids,\n      position_ids=position_ids,\n      head_mask=head_mask,\n      inputs_embeds=inputs_embeds,\n      output_attentions=output_attentions,\n      output_hidden_states=output_hidden_states,\n      return_dict=return_dict,\n    )\n\n    if empathy_values is not None:\n      output = torch.cat(\n          (\n            outputs.last_hidden_state,\n            empathy_values.reshape(outputs.last_hidden_state.shape[0], outputs.last_hidden_state.shape[1], 1)\n          ), dim=2)\n    if distress_values is not None:\n      output = torch.cat(\n          (\n            output,\n            distress_values.reshape(output.shape[0], output.shape[1], 1)\n          ), dim=2)\n    if emotion_values is not None:\n      output = torch.cat(\n          (\n            output,\n            emotion_values.reshape(output.shape[0], output.shape[1], 7)\n          ), dim=2)\n    #pooled_output = output.sum(axis=1) / attention_mask.sum(axis=-1).unsqueeze(-1) # Questo funziona peggio\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(output.size()).float()\n    sum_embeddings = torch.sum(output * input_mask_expanded, 1)\n    sum_mask = input_mask_expanded.sum(1)\n    sum_mask = torch.clamp(sum_mask, min = 1e-9)\n    pooled_output = sum_embeddings/sum_mask\n\n\n    pooled_output = self.dropout(pooled_output)\n    if features is not None:\n      pooled_output = torch.cat((pooled_output, features), dim=1)\n    logits = self.classifier(pooled_output)\n    loss = None\n    if labels is not None:\n      loss_fct = BCEWithLogitsLoss()\n      loss = loss_fct(logits, labels)\n    if not return_dict:\n      output = (logits,) + outputs[2:]\n      return ((loss,) + output) if loss is not None else output\n\n    return SequenceClassifierOutput(\n      loss=loss,\n      logits=logits,\n      hidden_states=outputs.hidden_states,\n      attentions=outputs.attentions,\n    )\"\"\"","metadata":{"id":"e-qwNumjsiDp","colab":{"base_uri":"https://localhost:8080/","height":175},"outputId":"0f35a836-ae58-4353-b4a6-1bd4cc5d0b0a","execution":{"iopub.status.busy":"2023-07-06T18:04:33.974648Z","iopub.execute_input":"2023-07-06T18:04:33.975037Z","iopub.status.idle":"2023-07-06T18:04:33.987198Z","shell.execute_reply.started":"2023-07-06T18:04:33.975003Z","shell.execute_reply":"2023-07-06T18:04:33.986062Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'num_features=0# features_train.shape[1]\\nempathy_lexicon = True\\ndistress_lexicon = True\\nemotion_lexicon = True\\nclass BertPerWordLexiconPooling(BertPreTrainedModel):\\n  def __init__(self, config):\\n    super().__init__(config)\\n    self.num_labels = config.num_labels\\n    self.config = config\\n    self.n_features = num_features\\n    self.empathy = 1 if empathy_lexicon else 0\\n    self.distress = 1 if distress_lexicon else 0\\n    self.emotion = 7 if emotion_lexicon else 0\\n    self.bert = BertModel(config)\\n    self.dropout = nn.Dropout(0.3)\\n    self.classifier = nn.Linear(config.hidden_size+self.n_features+self.empathy+self.distress+self.emotion, config.num_labels)\\n    self.post_init()\\n\\n  def forward(\\n    self,\\n    input_ids=None,\\n    attention_mask=None,\\n    token_type_ids=None,\\n    position_ids=None,\\n    head_mask=None,\\n    inputs_embeds=None,\\n    labels=None,\\n    output_attentions=None,\\n    output_hidden_states=None,\\n    return_dict=None,\\n    features=None,\\n    empathy_values=None,\\n    distress_values=None,\\n    emotion_values=None,\\n  ):\\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\\n\\n    outputs = self.bert(\\n      input_ids,\\n      attention_mask=attention_mask,\\n      token_type_ids=token_type_ids,\\n      position_ids=position_ids,\\n      head_mask=head_mask,\\n      inputs_embeds=inputs_embeds,\\n      output_attentions=output_attentions,\\n      output_hidden_states=output_hidden_states,\\n      return_dict=return_dict,\\n    )\\n\\n    if empathy_values is not None:\\n      output = torch.cat(\\n          (\\n            outputs.last_hidden_state,\\n            empathy_values.reshape(outputs.last_hidden_state.shape[0], outputs.last_hidden_state.shape[1], 1)\\n          ), dim=2)\\n    if distress_values is not None:\\n      output = torch.cat(\\n          (\\n            output,\\n            distress_values.reshape(output.shape[0], output.shape[1], 1)\\n          ), dim=2)\\n    if emotion_values is not None:\\n      output = torch.cat(\\n          (\\n            output,\\n            emotion_values.reshape(output.shape[0], output.shape[1], 7)\\n          ), dim=2)\\n    #pooled_output = output.sum(axis=1) / attention_mask.sum(axis=-1).unsqueeze(-1) # Questo funziona peggio\\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(output.size()).float()\\n    sum_embeddings = torch.sum(output * input_mask_expanded, 1)\\n    sum_mask = input_mask_expanded.sum(1)\\n    sum_mask = torch.clamp(sum_mask, min = 1e-9)\\n    pooled_output = sum_embeddings/sum_mask\\n\\n\\n    pooled_output = self.dropout(pooled_output)\\n    if features is not None:\\n      pooled_output = torch.cat((pooled_output, features), dim=1)\\n    logits = self.classifier(pooled_output)\\n    loss = None\\n    if labels is not None:\\n      loss_fct = BCEWithLogitsLoss()\\n      loss = loss_fct(logits, labels)\\n    if not return_dict:\\n      output = (logits,) + outputs[2:]\\n      return ((loss,) + output) if loss is not None else output\\n\\n    return SequenceClassifierOutput(\\n      loss=loss,\\n      logits=logits,\\n      hidden_states=outputs.hidden_states,\\n      attentions=outputs.attentions,\\n    )'"},"metadata":{}}]},{"cell_type":"code","source":"num_features=0 # features_train.shape[1]\nempathy_lexicon = True\ndistress_lexicon = True\nemotion_lexicon = True\nclass BertPerWordLexicon(BertPreTrainedModel):\n  def __init__(self, config):\n    super().__init__(config)\n    self.num_labels = config.num_labels\n    self.config = config\n    self.n_features = num_features\n    self.empathy = 1 if empathy_lexicon else 0\n    self.distress = 1 if distress_lexicon else 0\n    self.emotion = 7 if emotion_lexicon else 0\n    self.bert = BertModel(config)\n    self.dropout = nn.Dropout(0.3)\n    self.classifier_layer1 = nn.Linear((config.hidden_size+self.n_features+self.empathy+self.distress+self.emotion)*512,\n                                       (config.hidden_size+self.n_features+self.empathy+self.distress+self.emotion)*int(512/2)) #TODO: cambiare 512\n    self.tanh_layer = nn.Tanh()\n    self.classifier_layer2 = nn.Linear((config.hidden_size+self.n_features+self.empathy+self.distress+self.emotion)*int(512/2), config.num_labels)\n    self.post_init()\n\n  def forward(\n    self,\n    input_ids=None,\n    attention_mask=None,\n    token_type_ids=None,\n    position_ids=None,\n    head_mask=None,\n    inputs_embeds=None,\n    labels=None,\n    output_attentions=None,\n    output_hidden_states=None,\n    return_dict=None,\n    features=None,\n    empathy_values=None,\n    distress_values=None,\n    emotion_values=None,\n  ):\n    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n    outputs = self.bert(\n      input_ids,\n      attention_mask=attention_mask,\n      token_type_ids=token_type_ids,\n      position_ids=position_ids,\n      head_mask=head_mask,\n      inputs_embeds=inputs_embeds,\n      output_attentions=output_attentions,\n      output_hidden_states=output_hidden_states,\n      return_dict=return_dict,\n    )\n\n    if empathy_values is not None:\n      output = torch.cat(\n          (\n            outputs.last_hidden_state,\n            empathy_values.reshape(outputs.last_hidden_state.shape[0], outputs.last_hidden_state.shape[1], 1)\n          ), dim=2)\n    if distress_values is not None:\n      output = torch.cat(\n          (\n            output,\n            distress_values.reshape(output.shape[0], output.shape[1], 1)\n          ), dim=2)\n    if emotion_values is not None:\n      output = torch.cat(\n          (\n            output,\n            emotion_values.reshape(output.shape[0], output.shape[1], 7)\n          ), dim=2)\n    output_drop2 = self.dropout(output)\n    output_reshaped = output_drop2.reshape(-1, 512*777) #TODO: parametrizzare\n\n    output_drop2 = self.dropout(output_reshaped)\n    if features is not None:\n      output_drop = torch.cat((output_drop, features), dim=1)\n    logits1 = self.classifier_layer1(output_drop)\n    output1 = self.dropout(self.tanh_layer(logits1))\n    logits = self.classifier_layer2(output1)\n\n    loss = None\n    if labels is not None:\n      loss_fct = BCEWithLogitsLoss()\n      loss = loss_fct(logits, labels)\n    if not return_dict:\n      output = (logits,) + outputs[2:]\n      return ((loss,) + output) if loss is not None else output\n\n    return SequenceClassifierOutput(\n      loss=loss,\n      logits=logits,\n      hidden_states=outputs.hidden_states,\n      attentions=outputs.attentions,\n    )","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"ECvBNX3FLsim","outputId":"5cf8011f-733b-426b-94f1-006309899ae3","execution":{"iopub.status.busy":"2023-07-06T18:04:33.989007Z","iopub.execute_input":"2023-07-06T18:04:33.989379Z","iopub.status.idle":"2023-07-06T18:04:34.009455Z","shell.execute_reply.started":"2023-07-06T18:04:33.989346Z","shell.execute_reply":"2023-07-06T18:04:34.008368Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"model = BertPerWordLexicon.from_pretrained(model_type.get('bert-base'),num_labels=NUM_LABELS)\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":182},"id":"KCXMNCXHxJJu","outputId":"2e0a270f-d5ab-4fe6-9189-a58548416f14","execution":{"iopub.status.busy":"2023-07-06T18:06:42.239062Z","iopub.execute_input":"2023-07-06T18:06:42.239833Z","iopub.status.idle":"2023-07-06T18:06:42.641388Z","shell.execute_reply.started":"2023-07-06T18:06:42.239799Z","shell.execute_reply":"2023-07-06T18:06:42.639953Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertPerWordLexicon\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_type\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base\u001b[39m\u001b[38;5;124m'\u001b[39m),num_labels\u001b[38;5;241m=\u001b[39mNUM_LABELS)\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n","\u001b[0;31mNameError\u001b[0m: name 'BertPerWordLexicon' is not defined"],"ename":"NameError","evalue":"name 'BertPerWordLexicon' is not defined","output_type":"error"}]},{"cell_type":"code","source":"class WASSADataset(Dataset):\n    '''\n    This class is used to create a pytorch dataset for the EMO task.\n    '''\n\n    def __init__(\n        self,\n        tokenizer,\n        essay,\n        targets,\n        features=None, # np.array([[], []])\n        essay_EMP_lexicon=None, # {'empathy': [, ... ,], 'ditress': [, ..., ]}\n        essay_EMO_lexicon=None, # {'anger': [, ... ,], ..., 'sadness': [, ..., ]}\n        ):\n        self.tokenizer = tokenizer\n        self.essay = essay\n        self.targets = targets\n        self.essay_EMP_lexicon = essay_EMP_lexicon\n        self.essay_EMO_lexicon = essay_EMO_lexicon\n        self.features = features\n\n    def __len__(self):\n        return len(self.essay)\n\n    def __getitem__(self, index):\n      essay = str(self.essay[index])\n      inputs = tokenizer(\n          text=essay,\n          add_special_tokens=True,\n          padding='max_length',\n          truncation=True,\n          return_attention_mask=True,\n          return_tensors='pt',\n          return_token_type_ids=True\n        )\n\n      tokens_empathy = np.full(tokenizer.model_max_length, 4.0)\n      tokens_distress = np.zeros(tokenizer.model_max_length)\n      tokens_emotions = np.zeros((7, tokenizer.model_max_length))\n\n      item = {\n        'input_ids': inputs['input_ids'].flatten(),\n        'attention_mask': inputs['attention_mask'].flatten(),\n        'token_type_ids': inputs[\"token_type_ids\"].flatten()\n      }\n      if self.features is not None:\n        item['features'] = torch.FloatTensor(self.features[index])\n      if self.targets is not None:\n        item['labels'] = torch.FloatTensor(self.targets[index])\n      if self.essay_EMP_lexicon is None and self.essay_EMO_lexicon is None:\n        return item\n      word_count=0\n      first_char=True\n      last_char_is_space=False\n      for char_idx, char in enumerate(essay):\n        token_idx = inputs.char_to_token(char_idx)\n        if token_idx is None and not first_char:\n          if not last_char_is_space:\n            word_count+=1\n            last_char_is_space=True\n          continue\n        elif last_char_is_space:\n          last_char_is_space=False\n        first_char=False\n        if self.essay_EMP_lexicon is not None:\n          \"\"\"try:\n            tokens_empathy[token_idx] = self.essay_EMP_lexicon[index]['empathy'][word_count]\n          except IndexError:\n            print(f\"index: {index}\")\n            print(f\"token_idx: {token_idx}\")\n            print(f\"word count: {word_count} / {len(self.essay_EMP_lexicon[index]['empathy'])}\")\n            print(char_idx)\n            print(char)\"\"\"\n          tokens_empathy[token_idx] = self.essay_EMP_lexicon[index]['empathy'][word_count]\n          tokens_distress[token_idx] = self.essay_EMP_lexicon[index]['distress'][word_count]\n        if self.essay_EMO_lexicon is not None:\n          for i, emotion in enumerate(self.essay_EMO_lexicon[index]):\n            tokens_emotions[i][token_idx] = self.essay_EMO_lexicon[index][emotion][word_count]\n      if self.essay_EMP_lexicon is not None:\n        item['empathy_values'] = torch.FloatTensor(tokens_empathy)\n        item['distress_values'] = torch.FloatTensor(tokens_distress)\n        #print(tokens_empathy[:10])\n        #print(tokens_distress[:10])\n      if self.essay_EMO_lexicon is not None:\n        item['emotion_values'] = torch.FloatTensor(tokens_emotions)\n        #print(tokens_emotions[:,:10])\n      return item","metadata":{"id":"u3pK0ZBHWDZ1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EMO_json_path_train = 'https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/lexicon/train_per_word_lexicon_EMO.json'\nEMP_json_path_train = 'https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/lexicon/train_per_word_lexicon_EMP.json'\nEMO_json_path_dev = 'https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/lexicon/dev_per_word_lexicon_EMO.json'\nEMP_json_path_dev = 'https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/lexicon/dev_per_word_lexicon_EMP.json'\nEMO_json_path_test = 'https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/lexicon/test_per_word_lexicon_EMO.json'\nEMP_json_path_test = 'https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/lexicon/test_per_word_lexicon_EMP.json'\n\n!wget {EMO_json_path_train}\n!wget {EMP_json_path_train}\n!wget {EMO_json_path_dev}\n!wget {EMP_json_path_dev}\n!wget {EMO_json_path_test}\n!wget {EMP_json_path_test}","metadata":{"id":"a6T9cnjwZugv","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\n\nwith open('/content/train_per_word_lexicon_EMO.json') as json_file:\n  essay_EMO_lexicon_train_dict = json.load(json_file)\nwith open('/content/train_per_word_lexicon_EMP.json') as json_file:\n  essay_EMP_lexicon_train_dict = json.load(json_file)\n\nwith open('/content/dev_per_word_lexicon_EMO.json') as json_file:\n  essay_EMO_lexicon_dev_dict = json.load(json_file)\nwith open('/content/dev_per_word_lexicon_EMP.json') as json_file:\n  essay_EMP_lexicon_dev_dict = json.load(json_file)\n\nwith open('/content/test_per_word_lexicon_EMO.json') as json_file:\n  essay_EMO_lexicon_test_dict = json.load(json_file)\nwith open('/content/test_per_word_lexicon_EMP.json') as json_file:\n  essay_EMP_lexicon_test_dict = json.load(json_file)\n","metadata":{"id":"ReChwywEacF5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"essay_EMP_lexicon_train = [essay_EMP_lexicon_train_dict[str(id)] for id in train_df['essay_id']]\nessay_EMO_lexicon_train = [essay_EMO_lexicon_train_dict[str(id)] for id in train_df['essay_id']]\n\nessay_EMP_lexicon_val = [essay_EMP_lexicon_train_dict[str(id)] for id in val_df['essay_id']]\nessay_EMO_lexicon_val = [essay_EMO_lexicon_train_dict[str(id)] for id in val_df['essay_id']]\n\nessay_EMP_lexicon_dev = [essay_EMP_lexicon_dev_dict[str(id)] for id in dev_df['essay_id']]\nessay_EMO_lexicon_dev = [essay_EMO_lexicon_dev_dict[str(id)] for id in dev_df['essay_id']]","metadata":{"id":"v1mNoNH5bIwT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_set = WASSADataset(tokenizer=tokenizer, essay=train_df.essay, targets=y_train, features=None, essay_EMP_lexicon=essay_EMP_lexicon_train, essay_EMO_lexicon=essay_EMO_lexicon_train)\nval_set = WASSADataset(tokenizer=tokenizer, essay=val_df.essay, targets=y_val, features=None, essay_EMP_lexicon=essay_EMP_lexicon_val, essay_EMO_lexicon=essay_EMO_lexicon_val)\ndev_set = WASSADataset(tokenizer=tokenizer, essay=dev_df.essay, targets=y_dev, features=None, essay_EMP_lexicon=essay_EMP_lexicon_dev, essay_EMO_lexicon=essay_EMO_lexicon_dev)","metadata":{"id":"vzv9_pOaWReO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{"id":"9xvHbhOhhttG"}},{"cell_type":"markdown","source":"Prepare datasets","metadata":{"id":"bY0SLN6usjYs"}},{"cell_type":"code","source":"\"\"\"train_set = EMODataset(tokenizer=tokenizer, essay=train_df.essay, targets=y_train)#, features=features_train)\nval_set = EMODataset(tokenizer=tokenizer, essay=val_df.essay, targets=y_val)#, features=features_val)\ndev_set = EMODataset(tokenizer=tokenizer, essay=dev_df.essay, targets=y_dev)#, features=features_dev)\"\"\"","metadata":{"id":"vai1ZHsCHZI8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"train_set = EMODataset(tokenizer=tokenizer, essay=train_df.essay, targets=y_train, features=features_train)\nval_set = EMODataset(tokenizer=tokenizer, essay=val_df.essay, targets=y_val, features=features_val)\ndev_set = EMODataset(tokenizer=tokenizer, essay=dev_df.essay, targets=y_dev, features=features_dev)\"\"\"","metadata":{"id":"LSIofccfsTqg","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Set up training","metadata":{"id":"rvduRx8HsteS"}},{"cell_type":"code","source":"train_arguments = TrainingArguments(\n    output_dir=f\"./{model_config['model_name']}\",\n    per_device_train_batch_size=model_config['train_batch_size'],\n    per_device_eval_batch_size=model_config['val_batch_size'],\n    num_train_epochs=model_config['epochs'],\n    evaluation_strategy=\"steps\",\n    save_strategy=\"steps\",\n    eval_steps = 300,\n    save_steps = 300,\n    learning_rate=model_config['learning_rate'],\n    weight_decay=model_config['weight_decay'],\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_loss',\n    seed=model_config['seed'],\n    logging_strategy = \"epoch\"\n) # TODO: custom other params","metadata":{"id":"Z1DbZxhkD1R7","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=train_arguments,\n    train_dataset=train_set,\n    eval_dataset=val_set,\n    tokenizer=tokenizer,\n    compute_metrics=compute_EMO_metrics_trainer\n)","metadata":{"id":"hrI2rj4U3K2Q","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Callbacks","metadata":{"id":"A-sruHQdsraI"}},{"cell_type":"code","source":"class TrainerLoggingCallback(TrainerCallback):\n    def __init__(self, log_path):\n        self.log_path = log_path\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        _ = logs.pop(\"total_flos\", None)\n        if state.is_local_process_zero: # whether this process is the main one in a distributed setting\n            with open(self.log_path, \"a\") as f:\n                f.write(json.dumps(logs) + \"\\n\")\n\ntrainer.add_callback(EarlyStoppingCallback(\n    early_stopping_patience=model_config['patience'],\n    early_stopping_threshold=model_config['early_stopping_threshold']))\n\ntrainer.add_callback(TrainerLoggingCallback(model_config['model_id']+\"_log.json\"))","metadata":{"id":"4kUY7tU3sqr3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Start training","metadata":{"id":"dcCqSooFsw0X"}},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"L8xAPa81D5tu","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Access the training logs\ntrain_logs = trainer.state.log_history\n\n# Extract the loss values from the logs\ntrain_loss_values = [log.get('loss') for log in train_logs if log.get('loss') is not None]\neval_loss_values = [log.get('eval_loss') for log in train_logs if log.get('eval_loss') is not None]\ntrain_epochs = [log.get('epoch') for log in train_logs if log.get('loss') is not None]\neval_epochs = [log.get('epoch') for log in train_logs if log.get('eval_loss') is not None]","metadata":{"id":"Dd-3octehTlH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_logs","metadata":{"id":"C7mO8pWYK-y3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_loss_curve(train_loss_values, eval_loss_values, loss_epochs, eval_epochs,\"loss\", f\"{model_config['model_name']}_loss.png\")","metadata":{"id":"ECEu3ajRl6k3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation","metadata":{"id":"Rc5stfSah0Kj"}},{"cell_type":"code","source":"def predict_emotions(results, gold_emotions):\n\n  binarized_predictions = np.where(results.predictions >= 0.5, 1, 0)\n\n  for i, bin_pred in enumerate(binarized_predictions):\n    if np.all(bin_pred==0):\n      binarized_predictions[i][np.argmax(results.predictions[i])] = 1\n\n  predicted_emotions = label_encoder.decode(binarized_predictions)\n  return predicted_emotions\n","metadata":{"id":"19G80dVoH3HZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.state.best_model_checkpoint","metadata":{"id":"2YlWuaKoRH5r","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"outs = trainer.predict(dev_set)","metadata":{"id":"S35gRAI-D8cY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gold_emotions = gold_emotions = label_encoder.decode(outs.label_ids)\npredicted_emotions = predict_emotions(outs, gold_emotions)","metadata":{"id":"PIKGrBNlYDa1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_EMO_predictions(predicted_emotions, model_config['model_id']+\"_predictions_EMO.tsv\")\nchallenge_metrics = compute_EMO_metrics(golds=gold_emotions, predictions=predicted_emotions)\nwrite_dict_to_json(challenge_metrics, model_config['model_id']+\"_dev_metrics.json\")\nchallenge_metrics","metadata":{"id":"Im2VGnmQs7bY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(golds=gold_emotions, predictions=predicted_emotions, path=model_config['model_id']+\"_confusion_matrix.pdf\", title=model_config['model_id'])","metadata":{"id":"ITunC36XXLoN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model_graph(model=model, input_data=tokenizer(\"Hello world!\", return_tensors=\"pt\"), path=model_config['model_id']+\"_graph\")","metadata":{"id":"TJHxnmO7v8SJ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print_model_summary(model=model, path=model_config['model_id']+\"_summary.txt\")","metadata":{"id":"K33Q2SfFwR30","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Save model on Google Drive","metadata":{"id":"7DuCaju6tSHB"}},{"cell_type":"code","source":"trainer.state.best_model_checkpoint","metadata":{"id":"QtK-k5ITtRzN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv $trainer.state.best_model_checkpoint /content/drive/MyDrive/hlt","metadata":{"id":"b1KcxQQ0uUl6","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{"id":"9pkHv9w4tM1F"}},{"cell_type":"code","source":"MODEL_PATH = \"/content/drive/MyDrive/hlt/best-roberta\"","metadata":{"id":"v6hEtBLztMeC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, truncation=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_PATH,\n    num_labels=NUM_LABELS,\n    ignore_mismatched_sizes=True,\n    problem_type=\"multi_label_classification\")","metadata":{"id":"hWFw9keWvCy3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"load checkpoint","metadata":{"id":"Xi38gs-o1IGp"}},{"cell_type":"code","source":"# Load the checkpoint file\ncheckpoint_file = \"./bert-base-cased/checkpoint-1200\"\n#model = AutoModelForSequenceClassification.from_pretrained(checkpoint_file, num_labels=NUM_LABELS)\nmodel = BertLexicon.from_pretrained(checkpoint_file, num_labels=NUM_LABELS)\ntrainer = Trainer(model=model)\n# Perform prediction using the loaded checkpoint\npredictions = trainer.predict(dev_set)","metadata":{"id":"ncPpGans0XIG","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gold_emotions = label_encoder.decode(predictions.label_ids)\npredicted_emotions = predict_emotions(predictions, gold_emotions)","metadata":{"id":"dOOGuVrJ00R2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"write_EMO_predictions(predicted_emotions, model_config['model_id']+\"_predictions_EMO.tsv\")\nchallenge_metrics = compute_EMO_metrics(golds=gold_emotions, predictions=predicted_emotions)\nwrite_dict_to_json(challenge_metrics, model_config['model_id']+\"_dev_metrics.json\")\nchallenge_metrics","metadata":{"id":"rWWcct0E06bQ","trusted":true},"execution_count":null,"outputs":[]}]}