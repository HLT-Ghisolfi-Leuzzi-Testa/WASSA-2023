{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install accelerate -U\n!pip install datasets\n!pip install torch-summary\n!pip install graphviz\n!pip install torchview\n!pip install bertviz\n\nrepo_path = \"https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/\"\nbranch = \"lexicon\"","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gnWTVNYDMx2","outputId":"66181f14-9469-47bc-a399-47e14aa5a175","execution":{"iopub.status.busy":"2023-06-30T15:27:15.769661Z","iopub.execute_input":"2023-06-30T15:27:15.769926Z","iopub.status.idle":"2023-06-30T15:28:40.936606Z","shell.execute_reply.started":"2023-06-30T15:27:15.769903Z","shell.execute_reply":"2023-06-30T15:28:40.935364Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.12.0)\nCollecting accelerate\n  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.4.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\nSuccessfully installed accelerate-0.20.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.15.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting torch-summary\n  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\nInstalling collected packages: torch-summary\nSuccessfully installed torch-summary-1.4.5\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (0.20.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting torchview\n  Downloading torchview-0.2.6-py3-none-any.whl (25 kB)\nInstalling collected packages: torchview\nSuccessfully installed torchview-0.2.6\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting bertviz\n  Downloading bertviz-1.4.0-py3-none-any.whl (157 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: transformers>=2.0 in /opt/conda/lib/python3.10/site-packages (from bertviz) (4.30.1)\nRequirement already satisfied: torch>=1.0 in /opt/conda/lib/python3.10/site-packages (from bertviz) (2.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from bertviz) (4.64.1)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.10/site-packages (from bertviz) (1.26.100)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from bertviz) (2.28.2)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from bertviz) (2023.5.5)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from bertviz) (0.1.99)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.0->bertviz) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.0->bertviz) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.0->bertviz) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.0->bertviz) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.0->bertviz) (3.1.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (0.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (5.4.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=2.0->bertviz) (0.3.1)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3->bertviz)\n  Downloading botocore-1.29.164-py3-none-any.whl (11.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3->bertviz) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3->bertviz) (0.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->bertviz) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->bertviz) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->bertviz) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->bertviz) (2023.5.7)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.30.0,>=1.29.100->boto3->bertviz) (2.8.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=2.0->bertviz) (2023.6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=2.0->bertviz) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.0->bertviz) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.0->bertviz) (1.3.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.100->boto3->bertviz) (1.16.0)\nInstalling collected packages: botocore, bertviz\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.29.76\n    Uninstalling botocore-1.29.76:\n      Successfully uninstalled botocore-1.29.76\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.5.0 requires botocore<1.29.77,>=1.29.76, but you have botocore 1.29.164 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bertviz-1.4.0 botocore-1.29.164\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"utils_url = f\"{repo_path}{branch}/utils.py\"\nevaluation_url = f\"{repo_path}{branch}/evaluation.py\"\n\nimport os\nif os.path.exists(\"utils.py\"):\n  !rm \"utils.py\"\nif os.path.exists(\"evaluation.py\"):\n  !rm \"evaluation.py\"\n\n!wget {utils_url}\n!wget {evaluation_url}","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Xbo-Ei3IAYB","outputId":"04470e77-8968-45b2-efa9-1c18aa806dff","execution":{"iopub.status.busy":"2023-06-30T15:28:40.938985Z","iopub.execute_input":"2023-06-30T15:28:40.939371Z","iopub.status.idle":"2023-06-30T15:28:43.247210Z","shell.execute_reply.started":"2023-06-30T15:28:40.939336Z","shell.execute_reply":"2023-06-30T15:28:43.244350Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2023-06-30 15:28:41--  https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/lexicon/utils.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 14111 (14K) [text/plain]\nSaving to: ‘utils.py’\n\nutils.py            100%[===================>]  13.78K  --.-KB/s    in 0s      \n\n2023-06-30 15:28:42 (69.4 MB/s) - ‘utils.py’ saved [14111/14111]\n\n--2023-06-30 15:28:42--  https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/lexicon/evaluation.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 10675 (10K) [text/plain]\nSaving to: ‘evaluation.py’\n\nevaluation.py       100%[===================>]  10.42K  --.-KB/s    in 0s      \n\n2023-06-30 15:28:43 (53.5 MB/s) - ‘evaluation.py’ saved [10675/10675]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom transformers import BertModel, AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import BertPreTrainedModel, BertForSequenceClassification\nfrom transformers import TrainingArguments, Trainer, TrainerCallback, EarlyStoppingCallback, PretrainedConfig\nfrom transformers.modeling_outputs import SequenceClassifierOutput\nfrom torch.nn import BCEWithLogitsLoss\nimport importlib\nimport sys\nfrom utils import *\nimportlib.reload(sys.modules['utils'])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDmQslVxDhy1","outputId":"e438c13f-7d39-45a2-de2d-7791b5c245f9","execution":{"iopub.status.busy":"2023-06-30T15:28:43.249344Z","iopub.execute_input":"2023-06-30T15:28:43.249736Z","iopub.status.idle":"2023-06-30T15:28:58.101877Z","shell.execute_reply.started":"2023-06-30T15:28:43.249702Z","shell.execute_reply":"2023-06-30T15:28:58.100909Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"<module 'utils' from '/kaggle/working/utils.py'>"},"metadata":{}}]},{"cell_type":"code","source":"# set CUDA if available\n\"\"\"if torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(\"======= CUDA Available =======\")\nelse:\n    device = torch.device('cpu')\n    print(\"======= CUDA NOT Available, run on CPU =======\")\"\"\"\ndevice = torch.device('cpu') # otw goes out of memory","metadata":{"id":"quom7lWCDiiI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fbba6fc8-855a-4526-b470-0a62718d926e","execution":{"iopub.status.busy":"2023-06-30T15:28:58.104475Z","iopub.execute_input":"2023-06-30T15:28:58.105505Z","iopub.status.idle":"2023-06-30T15:28:58.110642Z","shell.execute_reply.started":"2023-06-30T15:28:58.105476Z","shell.execute_reply":"2023-06-30T15:28:58.109740Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"NUM_LABELS = 8\n\nmodel_config = {\n    'model_id': 'bert_custom_baseline',\n    'tokenizer_name': 'bert-base-cased',\n    'model_name': 'bert-base-cased',\n    'train_batch_size': 4,\n    'val_batch_size': 4,\n    'learning_rate': 2e-5,\n    'weight_decay': 0.01,\n    'epochs': 1,\n    'seed': 42,\n    'patience': 3,\n    'early_stopping_threshold': 0,\n} # TODO: expand...\n\ntokenizer = AutoTokenizer.from_pretrained(model_config['tokenizer_name'], truncation=True)","metadata":{"id":"dA224FxADpqd","execution":{"iopub.status.busy":"2023-06-30T15:28:58.112224Z","iopub.execute_input":"2023-06-30T15:28:58.112888Z","iopub.status.idle":"2023-06-30T15:28:59.293174Z","shell.execute_reply.started":"2023-06-30T15:28:58.112858Z","shell.execute_reply":"2023-06-30T15:28:59.292212Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dda845a0df634694a9a1b4a828b4fb76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dc0021abb36475cba5db0d9013373d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be2c297a9cf04e949413780fcd317600"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a97dc89a3be4a1eb7c55c0355441374"}},"metadata":{}}]},{"cell_type":"code","source":"TRAIN_DATA = f\"{repo_path}{branch}/datasets/internal_train_essay_level_preproc.tsv\"\nVAL_DATA = f\"{repo_path}{branch}/datasets/internal_val_essay_level_preproc.tsv\"\nDEV_DATA = f\"{repo_path}{branch}/datasets/dev_essay_level_preproc.tsv\"\n\ntrain_df = pd.read_csv(TRAIN_DATA, sep='\\t')\nval_df = pd.read_csv(VAL_DATA, sep='\\t')\ndev_df = pd.read_csv(DEV_DATA, sep='\\t')\n\n# TODO: come trattare unkown? inferirli?\n# per adesso gli unknown in gender, race e education sono trattati come se fossero una categoria a parte, in age e income sono settati a 0)\ntrain_df['age'] = train_df['age'].replace('unknown', '0')\nval_df['age'] = val_df['age'].replace('unknown', '0')\ndev_df['age'] = dev_df['age'].replace('unknown', '0')\ntrain_df['income'] = train_df['income'].replace('unknown', '0')\nval_df['income'] = val_df['income'].replace('unknown', '0')\ndev_df['income'] = dev_df['income'].replace('unknown', '0')\n\ntrain_df = train_df.astype({\"gender\": str, \"education\": str, \"race\": str, \"age\": int, \"income\": int})\nval_df = val_df.astype({\"gender\": str, \"education\": str, \"race\": str, \"age\": int, \"income\": int})\ndev_df = dev_df.astype({\"gender\": str, \"education\": str, \"race\": str, \"age\": int, \"income\": int})\n\nemotions_encoder = EmotionsLabelEncoder()\nemotions_encoder.fit(train_df.emotion)\ny_train = emotions_encoder.encode(train_df.emotion)\ny_val = emotions_encoder.encode(val_df.emotion)\ny_dev = emotions_encoder.encode(dev_df.emotion)\n\n#gender\teducation\trace\tage income\nfeatures_train =  np.array(train_df[['anger_count', 'disgust_count', 'fear_count', 'joy_count', 'sadness_count', 'surprise_count', 'hope_count']])\nfeatures_val =  np.array(val_df[['anger_count', 'disgust_count', 'fear_count', 'joy_count', 'sadness_count', 'surprise_count', 'hope_count']])\nfeatures_dev =  np.array(dev_df[['anger_count', 'disgust_count', 'fear_count', 'joy_count', 'sadness_count', 'surprise_count', 'hope_count']])\n\n\"\"\"features_encoder = FeaturesEncoder()\nfeatures_encoder.fit(train_df[['gender', 'age']])\nfeatures_train = features_encoder.encode(train_df[['gender', 'age',]])\nfeatures_val = features_encoder.encode(val_df[['gender', 'age']])\nfeatures_dev = features_encoder.encode(dev_df[['gender', 'age']])\"\"\"","metadata":{"id":"BjM03TscDwcz","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"af5fd468-89f4-492b-d7df-7e539a638c59","execution":{"iopub.status.busy":"2023-06-30T15:28:59.294758Z","iopub.execute_input":"2023-06-30T15:28:59.295139Z","iopub.status.idle":"2023-06-30T15:29:00.016770Z","shell.execute_reply.started":"2023-06-30T15:28:59.295105Z","shell.execute_reply":"2023-06-30T15:29:00.015878Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"\"features_encoder = FeaturesEncoder()\\nfeatures_encoder.fit(train_df[['gender', 'age']])\\nfeatures_train = features_encoder.encode(train_df[['gender', 'age',]])\\nfeatures_val = features_encoder.encode(val_df[['gender', 'age']])\\nfeatures_dev = features_encoder.encode(dev_df[['gender', 'age']])\""},"metadata":{}}]},{"cell_type":"code","source":"num_features=features_train.shape[1]\n\nclass BERTClassifier(BertPreTrainedModel):\n    #torch.nn.Module):\n    def __init__(self, config):#, num_features):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.num_features = num_features\n        self.config = config\n\n        self.bert = BertModel(config)\n        #self.bert = BertModel.from_pretrained('bert-base-uncased')\n        \"\"\"classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\n        self.dropout = nn.Dropout(classifier_dropout)\"\"\"\n        self.dropout = torch.nn.Dropout(0.3)\n        #self.dropout = torch.nn.Dropout(0.3)\n        self.linear = torch.nn.Linear(768+num_features, self.num_labels)\n        self.post_init()\n        \"\"\"classifier_dropout = (\n            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n        )\"\"\"\n        \"\"\"def __init__(self, num_labels, num_features):\n        super().__init__(num_labels)\n        self.num_labels = num_labels\n        #super(BERTClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.dropout = torch.nn.Dropout(0.3)\n        self.linear = torch.nn.Linear(768+num_features, self.num_labels)\n        \"\"\"\n    def forward(self, input_ids, attention_mask=None,  \n                token_type_ids=None, labels=None, \n                features=None, output_attentions=False):\n        outputs = self.bert(\n          input_ids,\n          attention_mask=attention_mask,\n          token_type_ids=token_type_ids,\n          output_attentions=True\n        ) # outputs type is BaseModelOutputWithPoolingAndCrossAttentions\n        # dropout_output = self.dropout(outputs[0]) # outputs[0]=last hidden state, with shape (batch_size, sequence_length, hidden_size)\n        # logits = self.linear(dropout_output[:,0,:].view(-1,768)) # from batch_size,1,768 to batch size, 768\n        dropout_output = self.dropout(outputs.pooler_output) # above the clf token there is linear and tanh\n        if features is not None:\n          dropout_output = torch.cat((dropout_output, features), dim=1)\n        logits = self.linear(dropout_output)\n        loss = None\n        if labels is not None:\n            #loss = BCEWithLogitsLoss()(logits, labels)\n            loss_fct = BCEWithLogitsLoss()\n            loss = loss_fct(logits, labels)\n        return SequenceClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)","metadata":{"id":"67Y-dSqvUiJ9","execution":{"iopub.status.busy":"2023-06-30T15:29:00.018239Z","iopub.execute_input":"2023-06-30T15:29:00.018600Z","iopub.status.idle":"2023-06-30T15:29:00.032292Z","shell.execute_reply.started":"2023-06-30T15:29:00.018567Z","shell.execute_reply":"2023-06-30T15:29:00.031123Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"############################ SUB-SAMPLE ############################\ntrain_df = train_df\nval_df = val_df\ndev_df = dev_df\n####################################################################\n\ntrain_set = EMODataset(tokenizer=tokenizer, essay=train_df.essay, targets=y_train, features=features_train)\nval_set = EMODataset(tokenizer=tokenizer, essay=val_df.essay, targets=y_val, features=features_val)\ndev_set = EMODataset(tokenizer=tokenizer, essay=dev_df.essay, targets=y_dev, features=features_dev)\n\nconfig = PretrainedConfig(pretrained_model_name_or_path=model_config['model_name'], \n                          num_labels=NUM_LABELS, \n                          ignore_mismatched_sizes=True, \n                          problem_type=\"multi_label_classification\")\n#model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\nmodel = BERTClassifier.from_pretrained('bert-base-uncased', num_labels=NUM_LABELS)#, num_features=features_train.shape[1])\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQUjENKA-yGm","outputId":"de94efc6-fa67-4bbc-d348-fbd47a45e89c","execution":{"iopub.status.busy":"2023-06-30T15:29:00.035550Z","iopub.execute_input":"2023-06-30T15:29:00.035933Z","iopub.status.idle":"2023-06-30T15:29:03.173339Z","shell.execute_reply.started":"2023-06-30T15:29:00.035908Z","shell.execute_reply":"2023-06-30T15:29:03.172267Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d24f945dcb54efeb552d0bf73c2dbc5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b1f3fd60f6a49169e723c0937653317"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BERTClassifier: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n- This IS expected if you are initializing BERTClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BERTClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BERTClassifier were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['linear.bias', 'linear.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"BERTClassifier(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.3, inplace=False)\n  (linear): Linear(in_features=775, out_features=8, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\nprogress_bar_train = tqdm(range(num_training_steps))\nprogress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)))\n\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch in train_dataloader:\n          batch = {k: v.to(device) for k, v in batch.items()}\n          outputs = model(**batch)\n          loss = outputs.loss\n          loss.backward()\n\n          optimizer.step()\n          lr_scheduler.step()\n          optimizer.zero_grad()\n          progress_bar_train.update(1)\n\nmodel.eval()\nfor batch in eval_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n    progress_bar_eval.update(1)\n    \n  print(metric.compute())\n\n      ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.eval()\n\ntest_dataloader = DataLoader(\n    tokenized_dataset[\"test\"], batch_size=32, collate_fn=data_collator\n)\n\nfor batch in test_dataloader:\n    batch = {k: v.to(device) for k, v in batch.items()}\n    with torch.no_grad():\n        outputs = model(**batch)\n\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=-1)\n    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n\nmetric.compute()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_arguments = TrainingArguments(\n    output_dir=\"./\",\n    per_device_train_batch_size=model_config['train_batch_size'],\n    per_device_eval_batch_size=model_config['val_batch_size'],\n    num_train_epochs=model_config['epochs'],\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=model_config['learning_rate'],\n    weight_decay=model_config['weight_decay'],\n    load_best_model_at_end=True,\n    metric_for_best_model='eval_loss',\n    seed=model_config['seed'],\n    #eval_accumulation_steps=10\n) # TODO: custom other params\n\ntrainer = Trainer(\n    model=model,\n    args=train_arguments,\n    train_dataset=train_set,\n    eval_dataset=val_set,\n    tokenizer=tokenizer,\n    compute_metrics=compute_EMO_metrics_trainer\n)\n\nclass TrainerLoggingCallback(TrainerCallback):\n    def __init__(self, log_path):\n        self.log_path = log_path\n\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        _ = logs.pop(\"total_flos\", None)\n        if state.is_local_process_zero: # whether this process is the main one in a distributed setting\n            with open(self.log_path, \"a\") as f:\n                f.write(json.dumps(logs) + \"\\n\")\n\ntrainer.add_callback(EarlyStoppingCallback(\n    early_stopping_patience=model_config['patience'],\n    early_stopping_threshold=model_config['early_stopping_threshold']))\ntrainer.add_callback(TrainerLoggingCallback(model_config['model_id']+\"_log.json\"))","metadata":{"id":"Z1DbZxhkD1R7","execution":{"iopub.status.busy":"2023-06-30T15:29:03.174877Z","iopub.execute_input":"2023-06-30T15:29:03.175484Z","iopub.status.idle":"2023-06-30T15:29:08.331856Z","shell.execute_reply.started":"2023-06-30T15:29:03.175437Z","shell.execute_reply":"2023-06-30T15:29:08.330830Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\"\"\"import torch\ntorch.cuda.empty_cache()\"\"\"\n#!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256'","metadata":{"execution":{"iopub.status.busy":"2023-06-30T15:29:08.335788Z","iopub.execute_input":"2023-06-30T15:29:08.336176Z","iopub.status.idle":"2023-06-30T15:29:09.322499Z","shell.execute_reply.started":"2023-06-30T15:29:08.336149Z","shell.execute_reply":"2023-06-30T15:29:09.321103Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"L8xAPa81D5tu","outputId":"b6911b84-18be-456f-ce0b-06969e3e2f20","execution":{"iopub.status.busy":"2023-06-30T15:29:09.339164Z","iopub.execute_input":"2023-06-30T15:29:09.339437Z","iopub.status.idle":"2023-06-30T15:30:42.801299Z","shell.execute_reply.started":"2023-06-30T15:29:09.339414Z","shell.execute_reply":"2023-06-30T15:30:42.799933Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230630_152925-w5iinw3z</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/giuliaghisolfi/huggingface/runs/w5iinw3z' target=\"_blank\">avid-darkness-7</a></strong> to <a href='https://wandb.ai/giuliaghisolfi/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/giuliaghisolfi/huggingface' target=\"_blank\">https://wandb.ai/giuliaghisolfi/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/giuliaghisolfi/huggingface/runs/w5iinw3z' target=\"_blank\">https://wandb.ai/giuliaghisolfi/huggingface/runs/w5iinw3z</a>"},"metadata":{}},{"name":"stderr","text":"You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='158' max='157' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [157/157 00:38, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='11' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11/42 00:01 < 00:05, 5.79 it/s]\n    </div>\n    "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/tmp/ipykernel_27/\u001b[0m\u001b[1;33m4032920361.py\u001b[0m:\u001b[94m1\u001b[0m in \u001b[92m<module>\u001b[0m                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[3;31m[Errno 2] No such file or directory: '/tmp/ipykernel_27/4032920361.py'\u001b[0m                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1645\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1642 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1643 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1644 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1645 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1646 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1647 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1648 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2035\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2032 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.control.should_training_stop = \u001b[94mTrue\u001b[0m                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2033 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2034 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_epoch_end(args, \u001b[96mself\u001b[0m.state, \u001b[96mself\u001b[0m.con  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2035 \u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2036 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2037 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m DebugOption.TPU_METRICS_DEBUG \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.args.debug:                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2038 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m is_torch_tpu_available():                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2321\u001b[0m in \u001b[92m_maybe_log_save_evaluate\u001b[0m \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2318 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m)                                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2319 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mmetrics.update(dataset_metrics)                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2320 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2321 \u001b[2m│   │   │   │   \u001b[0mmetrics = \u001b[96mself\u001b[0m.evaluate(ignore_keys=ignore_keys_for_eval)                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2322 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._report_to_hp_search(trial, \u001b[96mself\u001b[0m.state.global_step, metrics)             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2323 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2324 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Run delayed LR scheduler now that metrics are populated\u001b[0m                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m3053\u001b[0m in \u001b[92mevaluate\u001b[0m                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m3050 \u001b[0m\u001b[2m│   │   \u001b[0mstart_time = time.time()                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m3051 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m3052 \u001b[0m\u001b[2m│   │   \u001b[0meval_loop = \u001b[96mself\u001b[0m.prediction_loop \u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.use_legacy_prediction_loop \u001b[94melse\u001b[0m \u001b[96mse\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3053 \u001b[2m│   │   \u001b[0moutput = eval_loop(                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m3054 \u001b[0m\u001b[2m│   │   │   \u001b[0meval_dataloader,                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m3055 \u001b[0m\u001b[2m│   │   │   \u001b[0mdescription=\u001b[33m\"\u001b[0m\u001b[33mEvaluation\u001b[0m\u001b[33m\"\u001b[0m,                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m3056 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# No point gathering the predictions if there are no metrics, otherwise we d\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m3270\u001b[0m in \u001b[92mevaluation_loop\u001b[0m          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m3267 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.preprocess_logits_for_metrics \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m3268 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mlogits = \u001b[96mself\u001b[0m.preprocess_logits_for_metrics(logits, labels)           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m3269 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogits = \u001b[96mself\u001b[0m._nested_gather(logits)                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3270 \u001b[2m│   │   │   │   \u001b[0mpreds_host = logits \u001b[94mif\u001b[0m preds_host \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m nested_concat(preds_host,  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m3271 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m labels \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m3272 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlabels = \u001b[96mself\u001b[0m._nested_gather(labels)                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m3273 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlabels_host = labels \u001b[94mif\u001b[0m labels_host \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m nested_concat(labels_ho  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer_pt_utils.py\u001b[0m:\u001b[94m114\u001b[0m in \u001b[92mnested_concat\u001b[0m    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 111 \u001b[0m\u001b[2m│   │   \u001b[0mnew_tensors                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 112 \u001b[0m\u001b[2m│   \u001b[0m), \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mExpected `tensors` and `new_tensors` to have the same type but found \u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(tens  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, (\u001b[96mlist\u001b[0m, \u001b[96mtuple\u001b[0m)):                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mtype\u001b[0m(tensors)(nested_concat(t, n, padding_index=padding_index) \u001b[94mfor\u001b[0m t, n \u001b[95mi\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 115 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, torch.Tensor):                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 116 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_ind  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, Mapping):                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer_pt_utils.py\u001b[0m:\u001b[94m114\u001b[0m in \u001b[92m<genexpr>\u001b[0m        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 111 \u001b[0m\u001b[2m│   │   \u001b[0mnew_tensors                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 112 \u001b[0m\u001b[2m│   \u001b[0m), \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mExpected `tensors` and `new_tensors` to have the same type but found \u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(tens  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, (\u001b[96mlist\u001b[0m, \u001b[96mtuple\u001b[0m)):                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mtype\u001b[0m(tensors)(nested_concat(t, n, padding_index=padding_index) \u001b[94mfor\u001b[0m t, n \u001b[95mi\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 115 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, torch.Tensor):                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 116 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_ind  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, Mapping):                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer_pt_utils.py\u001b[0m:\u001b[94m114\u001b[0m in \u001b[92mnested_concat\u001b[0m    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 111 \u001b[0m\u001b[2m│   │   \u001b[0mnew_tensors                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 112 \u001b[0m\u001b[2m│   \u001b[0m), \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mExpected `tensors` and `new_tensors` to have the same type but found \u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(tens  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, (\u001b[96mlist\u001b[0m, \u001b[96mtuple\u001b[0m)):                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mtype\u001b[0m(tensors)(nested_concat(t, n, padding_index=padding_index) \u001b[94mfor\u001b[0m t, n \u001b[95mi\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 115 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, torch.Tensor):                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 116 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_ind  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, Mapping):                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer_pt_utils.py\u001b[0m:\u001b[94m114\u001b[0m in \u001b[92m<genexpr>\u001b[0m        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 111 \u001b[0m\u001b[2m│   │   \u001b[0mnew_tensors                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 112 \u001b[0m\u001b[2m│   \u001b[0m), \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mExpected `tensors` and `new_tensors` to have the same type but found \u001b[0m\u001b[33m{\u001b[0m\u001b[96mtype\u001b[0m(tens  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, (\u001b[96mlist\u001b[0m, \u001b[96mtuple\u001b[0m)):                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mtype\u001b[0m(tensors)(nested_concat(t, n, padding_index=padding_index) \u001b[94mfor\u001b[0m t, n \u001b[95mi\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 115 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, torch.Tensor):                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 116 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_ind  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, Mapping):                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer_pt_utils.py\u001b[0m:\u001b[94m116\u001b[0m in \u001b[92mnested_concat\u001b[0m    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, (\u001b[96mlist\u001b[0m, \u001b[96mtuple\u001b[0m)):                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mtype\u001b[0m(tensors)(nested_concat(t, n, padding_index=padding_index) \u001b[94mfor\u001b[0m t, n \u001b[95mi\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 115 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, torch.Tensor):                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 116 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_ind  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(tensors, Mapping):                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 118 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mtype\u001b[0m(tensors)(                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 119 \u001b[0m\u001b[2m│   │   │   \u001b[0m{k: nested_concat(t, new_tensors[k], padding_index=padding_index) \u001b[94mfor\u001b[0m k, t \u001b[95mi\u001b[0m  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer_pt_utils.py\u001b[0m:\u001b[94m75\u001b[0m in                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92mtorch_pad_and_concatenate\u001b[0m                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m  72 \u001b[0m\u001b[2m│   \u001b[0mtensor2 = atleast_1d(tensor2)                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m  73 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m  74 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(tensor1.shape) == \u001b[94m1\u001b[0m \u001b[95mor\u001b[0m tensor1.shape[\u001b[94m1\u001b[0m] == tensor2.shape[\u001b[94m1\u001b[0m]:                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m  75 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m torch.cat((tensor1, tensor2), dim=\u001b[94m0\u001b[0m)                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m  76 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m  77 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# Let's figure out the new shape\u001b[0m                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m  78 \u001b[0m\u001b[2m│   \u001b[0mnew_shape = (tensor1.shape[\u001b[94m0\u001b[0m] + tensor2.shape[\u001b[94m0\u001b[0m], \u001b[96mmax\u001b[0m(tensor1.shape[\u001b[94m1\u001b[0m], tensor2.shap  \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m576.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m15.90\u001b[0m GiB total capacity; \u001b[1;36m13.62\u001b[0m GiB \nalready allocated; \u001b[1;36m239.75\u001b[0m MiB free; \u001b[1;36m14.79\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated \nmemory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \nPYTORCH_CUDA_ALLOC_CONF\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/tmp/ipykernel_27/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">4032920361.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000; font-style: italic\">[Errno 2] No such file or directory: '/tmp/ipykernel_27/4032920361.py'</span>                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1645</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1642 │   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1643 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1644 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1645 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1646 │   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1647 │   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1648 │   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2035</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2032 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.control.should_training_stop = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2033 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2034 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.control = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.callback_handler.on_epoch_end(args, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.con  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2035 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2036 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2037 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> DebugOption.TPU_METRICS_DEBUG <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.debug:                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2038 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> is_torch_tpu_available():                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2321</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_maybe_log_save_evaluate</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2318 │   │   │   │   │   </span>)                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2319 │   │   │   │   │   </span>metrics.update(dataset_metrics)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2320 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2321 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>metrics = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.evaluate(ignore_keys=ignore_keys_for_eval)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2322 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._report_to_hp_search(trial, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state.global_step, metrics)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2323 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2324 │   │   │   # Run delayed LR scheduler now that metrics are populated</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3053</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">evaluate</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3050 │   │   </span>start_time = time.time()                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3051 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3052 │   │   </span>eval_loop = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prediction_loop <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.use_legacy_prediction_loop <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">se</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3053 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>output = eval_loop(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3054 │   │   │   </span>eval_dataloader,                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3055 │   │   │   </span>description=<span style=\"color: #808000; text-decoration-color: #808000\">\"Evaluation\"</span>,                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3056 │   │   │   # No point gathering the predictions if there are no metrics, otherwise we d</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3270</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">evaluation_loop</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3267 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.preprocess_logits_for_metrics <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3268 │   │   │   │   │   </span>logits = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.preprocess_logits_for_metrics(logits, labels)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3269 │   │   │   │   </span>logits = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._nested_gather(logits)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3270 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>preds_host = logits <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> preds_host <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> nested_concat(preds_host,  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3271 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> labels <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3272 │   │   │   │   </span>labels = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._nested_gather(labels)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3273 │   │   │   │   </span>labels_host = labels <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> labels_host <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> nested_concat(labels_ho  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer_pt_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">114</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">nested_concat</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 111 │   │   </span>new_tensors                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 112 │   </span>), <span style=\"color: #808000; text-decoration-color: #808000\">f\"Expected `tensors` and `new_tensors` to have the same type but found {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tens  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 113 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">tuple</span>)):                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tensors)(nested_concat(t, n, padding_index=padding_index) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> t, n <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">i</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 115 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, torch.Tensor):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 116 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_ind  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 117 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, Mapping):                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer_pt_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">114</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;genexpr&gt;</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 111 │   │   </span>new_tensors                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 112 │   </span>), <span style=\"color: #808000; text-decoration-color: #808000\">f\"Expected `tensors` and `new_tensors` to have the same type but found {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tens  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 113 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">tuple</span>)):                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tensors)(nested_concat(t, n, padding_index=padding_index) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> t, n <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">i</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 115 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, torch.Tensor):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 116 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_ind  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 117 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, Mapping):                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer_pt_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">114</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">nested_concat</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 111 │   │   </span>new_tensors                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 112 │   </span>), <span style=\"color: #808000; text-decoration-color: #808000\">f\"Expected `tensors` and `new_tensors` to have the same type but found {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tens  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 113 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">tuple</span>)):                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tensors)(nested_concat(t, n, padding_index=padding_index) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> t, n <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">i</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 115 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, torch.Tensor):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 116 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_ind  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 117 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, Mapping):                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer_pt_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">114</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;genexpr&gt;</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 111 │   │   </span>new_tensors                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 112 │   </span>), <span style=\"color: #808000; text-decoration-color: #808000\">f\"Expected `tensors` and `new_tensors` to have the same type but found {</span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tens  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 113 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">tuple</span>)):                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tensors)(nested_concat(t, n, padding_index=padding_index) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> t, n <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">i</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 115 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, torch.Tensor):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 116 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_ind  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 117 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, Mapping):                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer_pt_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">116</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">nested_concat</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 113 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">tuple</span>)):                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 114 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tensors)(nested_concat(t, n, padding_index=padding_index) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> t, n <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">i</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 115 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, torch.Tensor):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 116 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch_pad_and_concatenate(tensors, new_tensors, padding_index=padding_ind  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 117 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(tensors, Mapping):                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 118 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">type</span>(tensors)(                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 119 │   │   │   </span>{k: nested_concat(t, new_tensors[k], padding_index=padding_index) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> k, t <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">i</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer_pt_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">75</span> in                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">torch_pad_and_concatenate</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  72 │   </span>tensor2 = atleast_1d(tensor2)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  73 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  74 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(tensor1.shape) == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> tensor1.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] == tensor2.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]:                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>  75 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch.cat((tensor1, tensor2), dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  76 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  77 │   # Let's figure out the new shape</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  78 │   </span>new_shape = (tensor1.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>] + tensor2.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>], <span style=\"color: #00ffff; text-decoration-color: #00ffff\">max</span>(tensor1.shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>], tensor2.shap  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">576.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.90</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.62</span> GiB \nalready allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">239.75</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.79</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated \nmemory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \nPYTORCH_CUDA_ALLOC_CONF\n</pre>\n"},"metadata":{}}]},{"cell_type":"code","source":"plot_loss_curve(train_loss_values, eval_loss_values, \"loss\", f\"{model_config['model_name']}_loss.png\")","metadata":{"id":"VdqZ1uQs6Gws","execution":{"iopub.status.busy":"2023-06-30T15:30:42.802325Z","iopub.status.idle":"2023-06-30T15:30:42.802687Z","shell.execute_reply.started":"2023-06-30T15:30:42.802515Z","shell.execute_reply":"2023-06-30T15:30:42.802532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = BERTClassifier(num_labels=NUM_LABELS, num_features=0)\ninput_str = \"Hello world!\"\nplot_attentions(input_str, model, tokenizer, \"./attention.pdf\", \"bert-base-cased\")","metadata":{"id":"Ao12rsvRDwCy","execution":{"iopub.status.busy":"2023-06-30T15:30:42.804007Z","iopub.status.idle":"2023-06-30T15:30:42.804734Z","shell.execute_reply.started":"2023-06-30T15:30:42.804491Z","shell.execute_reply":"2023-06-30T15:30:42.804515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define your gold labels\ngold_emotions = ['Hope/Sadness', 'Anger', 'Sadness', 'Neutral', 'Disgust/Sadness',\n       'Anger/Disgust', 'Fear/Sadness', 'Joy', 'Hope', 'Joy/Neutral',\n       'Disgust', 'Neutral/Sadness', 'Neutral/Surprise', 'Anger/Neutral',\n       'Hope/Neutral', 'Surprise', 'Anger/Sadness', 'Fear', 'Anger/Joy',\n       'Disgust/Fear', 'Fear/Neutral', 'Fear/Hope', 'Joy/Sadness',\n       'Anger/Disgust/Sadness', 'Anger/Surprise', 'Disgust/Neutral',\n       'Anger/Fear', 'Sadness/Surprise', 'Disgust/Surprise', 'Anger/Hope']\n\n# Define the desired label names\nlabel_names = ['Anger', 'Disgust', 'Fear', 'Hope', 'Joy', 'Neutral', 'Sadness', 'Surprise']\n\n# Compute the confusion matrix\ncm = confusion_matrix(gold_emotions, gold_emotions, labels=label_names)\n\n# Plot the confusion matrix\nfig, ax = plt.subplots()\nim = ax.imshow(cm, cmap='Blues')\n\n# Customize the plot\nax.set_xticks(np.arange(len(label_names)))\nax.set_yticks(np.arange(len(label_names)))\nax.set_xticklabels(label_names, rotation=45)\nax.set_yticklabels(label_names)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\n\n# Add colorbar\ncbar = ax.figure.colorbar(im, ax=ax)\n\n# Show the plot\nplt.tight_layout()\nplt.show()","metadata":{"id":"jwuhm8kjxDTj","execution":{"iopub.status.busy":"2023-06-30T15:30:42.806390Z","iopub.status.idle":"2023-06-30T15:30:42.808325Z","shell.execute_reply.started":"2023-06-30T15:30:42.808085Z","shell.execute_reply":"2023-06-30T15:30:42.808108Z"},"trusted":true},"execution_count":null,"outputs":[]}]}