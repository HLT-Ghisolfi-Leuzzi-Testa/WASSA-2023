# -*- coding: utf-8 -*-
"""baseline-costum-count1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G67X_CKgASkKHoGNzmvG63TB2xn82c6X
"""

"""!pip install transformers
!pip install accelerate -U
!pip install datasets
!pip install torch-summary
!pip install graphviz
!pip install torchview
!pip install bertviz"""

repo_path = "https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/"
branch = "lexicon"

"""utils_url = f"{repo_path}{branch}/utils.py"
evaluation_url = f"{repo_path}{branch}/evaluation.py"""


"""import os
if os.path.exists("utils.py"):
  !rm "utils.py"
if os.path.exists("evaluation.py"):
  !rm "evaluation.py"

!wget {utils_url}
!wget {evaluation_url}"""

import json
import torch
import pandas as pd
import numpy as np
from transformers import BertModel, AutoTokenizer, AutoModelForSequenceClassification
from transformers import BertPreTrainedModel, BertForSequenceClassification
from transformers import TrainingArguments, Trainer, TrainerCallback, EarlyStoppingCallback, PretrainedConfig
from transformers.modeling_outputs import SequenceClassifierOutput
from torch.nn import BCEWithLogitsLoss
import importlib
import sys
from utils import *
from evaluation import *
importlib.reload(sys.modules['utils'])

# set CUDA if available
"""if torch.cuda.is_available():
    device = torch.device('cuda')
    print("======= CUDA Available =======")
else:
    device = torch.device('cpu')
    print("======= CUDA NOT Available, run on CPU =======")"""
device = torch.device('cpu') # otw goes out of memory

NUM_LABELS = 8

model_config = {
    'model_id': 'bert_custom_baseline',
    'tokenizer_name': 'bert-base-cased',
    'model_name': 'bert-base-cased',
    'train_batch_size': 4,
    'val_batch_size': 4,
    'learning_rate': 2e-5,
    'weight_decay': 0.01,
    'epochs': 1,
    'seed': 42,
    'patience': 3,
    'early_stopping_threshold': 0,
} # TODO: expand...

tokenizer = AutoTokenizer.from_pretrained(model_config['tokenizer_name'], truncation=True)

TRAIN_DATA = f"{repo_path}{branch}/datasets/internal_train_essay_level_preproc.tsv"
VAL_DATA = f"{repo_path}{branch}/datasets/internal_val_essay_level_preproc.tsv"
DEV_DATA = f"{repo_path}{branch}/datasets/dev_essay_level_preproc.tsv"

train_df = pd.read_csv(TRAIN_DATA, sep='\t')
val_df = pd.read_csv(VAL_DATA, sep='\t')
dev_df = pd.read_csv(DEV_DATA, sep='\t')

# TODO: come trattare unkown? inferirli?
# per adesso gli unknown in gender, race e education sono trattati come se fossero una categoria a parte, in age e income sono settati a 0)
train_df['age'] = train_df['age'].replace('unknown', '0')
val_df['age'] = val_df['age'].replace('unknown', '0')
dev_df['age'] = dev_df['age'].replace('unknown', '0')
train_df['income'] = train_df['income'].replace('unknown', '0')
val_df['income'] = val_df['income'].replace('unknown', '0')
dev_df['income'] = dev_df['income'].replace('unknown', '0')

train_df = train_df.astype({"gender": str, "education": str, "race": str, "age": int, "income": int})
val_df = val_df.astype({"gender": str, "education": str, "race": str, "age": int, "income": int})
dev_df = dev_df.astype({"gender": str, "education": str, "race": str, "age": int, "income": int})

emotions_encoder = EmotionsLabelEncoder()
emotions_encoder.fit(train_df.emotion)
y_train = emotions_encoder.encode(train_df.emotion)
y_val = emotions_encoder.encode(val_df.emotion)
y_dev = emotions_encoder.encode(dev_df.emotion)

#gender	education	race	age income
features_train =  np.array(train_df[['anger_count', 'disgust_count', 'fear_count', 'joy_count', 'sadness_count', 'surprise_count', 'hope_count']])
features_val =  np.array(val_df[['anger_count', 'disgust_count', 'fear_count', 'joy_count', 'sadness_count', 'surprise_count', 'hope_count']])
features_dev =  np.array(dev_df[['anger_count', 'disgust_count', 'fear_count', 'joy_count', 'sadness_count', 'surprise_count', 'hope_count']])

"""features_encoder = FeaturesEncoder()
features_encoder.fit(train_df[['gender', 'age']])
features_train = features_encoder.encode(train_df[['gender', 'age',]])
features_val = features_encoder.encode(val_df[['gender', 'age']])
features_dev = features_encoder.encode(dev_df[['gender', 'age']])"""

num_features=features_train.shape[1]

class BERTClassifier(BertPreTrainedModel):
    #torch.nn.Module):
    def __init__(self, config):#, num_features):
        super().__init__(config)
        self.num_labels = config.num_labels
        self.num_features = num_features
        self.config = config

        self.bert = BertModel(config)
        #self.bert = BertModel.from_pretrained('bert-base-uncased')
        """classifier_dropout = (
            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob
        )
        self.dropout = nn.Dropout(classifier_dropout)"""
        self.dropout = torch.nn.Dropout(0.3)
        #self.dropout = torch.nn.Dropout(0.3)
        self.linear = torch.nn.Linear(768+num_features, self.num_labels)
        self.post_init()
        """classifier_dropout = (
            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob
        )"""
        """def __init__(self, num_labels, num_features):
        super().__init__(num_labels)
        self.num_labels = num_labels
        #super(BERTClassifier, self).__init__()
        self.bert = BertModel.from_pretrained('bert-base-uncased')
        self.dropout = torch.nn.Dropout(0.3)
        self.linear = torch.nn.Linear(768+num_features, self.num_labels)
        """
    def forward(self, input_ids, attention_mask=None,
                token_type_ids=None, labels=None,
                features=None, output_attentions=False):
        outputs = self.bert(
          input_ids,
          attention_mask=attention_mask,
          token_type_ids=token_type_ids,
          output_attentions=True
        ) # outputs type is BaseModelOutputWithPoolingAndCrossAttentions
        # dropout_output = self.dropout(outputs[0]) # outputs[0]=last hidden state, with shape (batch_size, sequence_length, hidden_size)
        #Â logits = self.linear(dropout_output[:,0,:].view(-1,768)) # from batch_size,1,768 to batch size, 768
        dropout_output = self.dropout(outputs.pooler_output) # above the clf token there is linear and tanh
        if features is not None:
          dropout_output = torch.cat((dropout_output, features), dim=1)
        logits = self.linear(dropout_output)
        loss = None
        if labels is not None:
            #loss = BCEWithLogitsLoss()(logits, labels)
            loss_fct = BCEWithLogitsLoss()
            loss = loss_fct(logits, labels)
        return SequenceClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states,attentions=outputs.attentions)

############################ SUB-SAMPLE ############################
train_df = train_df[:2]
val_df = val_df[:20]
dev_df = dev_df[:20]
####################################################################

train_set = EMODataset(tokenizer=tokenizer, essay=train_df.essay, targets=y_train, features=features_train)
val_set = EMODataset(tokenizer=tokenizer, essay=val_df.essay, targets=y_val, features=features_val)
dev_set = EMODataset(tokenizer=tokenizer, essay=dev_df.essay, targets=y_dev, features=features_dev)

config = PretrainedConfig(pretrained_model_name_or_path=model_config['model_name'],
                          num_labels=NUM_LABELS,
                          ignore_mismatched_sizes=True,
                          problem_type="multi_label_classification")
#model = BertForSequenceClassification.from_pretrained('bert-base-uncased')
model = BERTClassifier.from_pretrained('bert-base-uncased', num_labels=NUM_LABELS)#, num_features=features_train.shape[1])
model.to(device)

"""from tqdm.auto import tqdm

progress_bar_train = tqdm(range(num_training_steps))
progress_bar_eval = tqdm(range(num_epochs * len(eval_dataloader)))


for epoch in range(num_epochs):
    model.train()
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar_train.update(1)

model.eval()
for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])
    progress_bar_eval.update(1)

  print(metric.compute())

model.eval()

test_dataloader = DataLoader(
    tokenized_dataset["test"], batch_size=32, collate_fn=data_collator
)

for batch in test_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

metric.compute()"""
train_arguments = TrainingArguments(
    output_dir="./",
    per_device_train_batch_size=model_config['train_batch_size'],
    per_device_eval_batch_size=model_config['val_batch_size'],
    num_train_epochs=model_config['epochs'],
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=model_config['learning_rate'],
    weight_decay=model_config['weight_decay'],
    load_best_model_at_end=True,
    metric_for_best_model='eval_loss',
    seed=model_config['seed'],
    #eval_accumulation_steps=10
) # TODO: custom other params

trainer = Trainer(
    model=model,
    args=train_arguments,
    train_dataset=train_set,
    eval_dataset=val_set,
    tokenizer=tokenizer,
    compute_metrics=compute_EMO_metrics_trainer
)

class TrainerLoggingCallback(TrainerCallback):
    def __init__(self, log_path):
        self.log_path = log_path

    def on_log(self, args, state, control, logs=None, **kwargs):
        _ = logs.pop("total_flos", None)
        if state.is_local_process_zero: # whether this process is the main one in a distributed setting
            with open(self.log_path, "a") as f:
                f.write(json.dumps(logs) + "\n")

trainer.add_callback(EarlyStoppingCallback(
    early_stopping_patience=model_config['patience'],
    early_stopping_threshold=model_config['early_stopping_threshold']))
trainer.add_callback(TrainerLoggingCallback(model_config['model_id']+"_log.json"))

"""import torch
torch.cuda.empty_cache()"""
#!export 'PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256'

trainer.train()

plot_loss_curve(train_loss_values, eval_loss_values, "loss", f"{model_config['model_name']}_loss.png")

model = BERTClassifier(num_labels=NUM_LABELS, num_features=0)
input_str = "Hello world!"
plot_attentions(input_str, model, tokenizer, "./attention.pdf", "bert-base-cased")

# Define your gold labels
gold_emotions = ['Hope/Sadness', 'Anger', 'Sadness', 'Neutral', 'Disgust/Sadness',
       'Anger/Disgust', 'Fear/Sadness', 'Joy', 'Hope', 'Joy/Neutral',
       'Disgust', 'Neutral/Sadness', 'Neutral/Surprise', 'Anger/Neutral',
       'Hope/Neutral', 'Surprise', 'Anger/Sadness', 'Fear', 'Anger/Joy',
       'Disgust/Fear', 'Fear/Neutral', 'Fear/Hope', 'Joy/Sadness',
       'Anger/Disgust/Sadness', 'Anger/Surprise', 'Disgust/Neutral',
       'Anger/Fear', 'Sadness/Surprise', 'Disgust/Surprise', 'Anger/Hope']

# Define the desired label names
label_names = ['Anger', 'Disgust', 'Fear', 'Hope', 'Joy', 'Neutral', 'Sadness', 'Surprise']

# Compute the confusion matrix
cm = confusion_matrix(gold_emotions, gold_emotions, labels=label_names)

# Plot the confusion matrix
fig, ax = plt.subplots()
im = ax.imshow(cm, cmap='Blues')

# Customize the plot
ax.set_xticks(np.arange(len(label_names)))
ax.set_yticks(np.arange(len(label_names)))
ax.set_xticklabels(label_names, rotation=45)
ax.set_yticklabels(label_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")

# Add colorbar
cbar = ax.figure.colorbar(im, ax=ax)

# Show the plot
plt.tight_layout()
plt.show()