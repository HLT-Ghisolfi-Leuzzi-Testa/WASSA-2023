{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_B-_QwdhWLe"
      },
      "source": [
        "# WASSA2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:04:39.939508Z",
          "iopub.status.busy": "2023-07-11T19:04:39.938937Z",
          "iopub.status.idle": "2023-07-11T19:04:39.955669Z",
          "shell.execute_reply": "2023-07-11T19:04:39.954582Z",
          "shell.execute_reply.started": "2023-07-11T19:04:39.939467Z"
        },
        "id": "9sDeMksltGSk",
        "outputId": "8a0a9272-21b9-48a8-c523-bbbe5c541046",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15KoBLrVhbI8"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTWi38mQKxUK"
      },
      "outputs": [],
      "source": [
        "repo_path = \"https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/\"\n",
        "branch = \"main\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:04:39.958361Z",
          "iopub.status.busy": "2023-07-11T19:04:39.958022Z",
          "iopub.status.idle": "2023-07-11T19:06:28.298280Z",
          "shell.execute_reply": "2023-07-11T19:06:28.297057Z",
          "shell.execute_reply.started": "2023-07-11T19:04:39.958330Z"
        },
        "id": "1gnWTVNYDMx2",
        "outputId": "eba46524-972b-4e75-b682-f85b35254124",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2000 is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "Finished.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers -q\n",
        "!pip install accelerate -U -q\n",
        "!pip install datasets -q\n",
        "!pip install torch-summary -q\n",
        "!pip install bertviz -q\n",
        "!pip install NRCLex -q\n",
        "!pip install textblob -q\n",
        "!python -m textblob.download_corpora -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:06:28.300542Z",
          "iopub.status.busy": "2023-07-11T19:06:28.299931Z",
          "iopub.status.idle": "2023-07-11T19:06:30.619028Z",
          "shell.execute_reply": "2023-07-11T19:06:30.617896Z",
          "shell.execute_reply.started": "2023-07-11T19:06:28.300504Z"
        },
        "id": "9Xbo-Ei3IAYB",
        "outputId": "b1c16dd2-1ae8-48e2-8aef-3bd77210ba24",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-07-20 09:34:10--  https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40064 (39K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]  39.12K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2023-07-20 09:34:10 (11.5 MB/s) - ‘utils.py’ saved [40064/40064]\n",
            "\n",
            "--2023-07-20 09:34:10--  https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/evaluation.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10675 (10K) [text/plain]\n",
            "Saving to: ‘evaluation.py’\n",
            "\n",
            "evaluation.py       100%[===================>]  10.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-20 09:34:10 (70.3 MB/s) - ‘evaluation.py’ saved [10675/10675]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "utils_url = f\"{repo_path}{branch}/utils.py\"\n",
        "evaluation_url = f\"{repo_path}{branch}/evaluation.py\"\n",
        "\n",
        "import os\n",
        "if os.path.exists(\"utils.py\"):\n",
        "  !rm \"utils.py\"\n",
        "if os.path.exists(\"evaluation.py\"):\n",
        "  !rm \"evaluation.py\"\n",
        "\n",
        "!wget {utils_url}\n",
        "!wget {evaluation_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:06:30.622919Z",
          "iopub.status.busy": "2023-07-11T19:06:30.622582Z",
          "iopub.status.idle": "2023-07-11T19:06:38.755450Z",
          "shell.execute_reply": "2023-07-11T19:06:38.754266Z",
          "shell.execute_reply.started": "2023-07-11T19:06:30.622890Z"
        },
        "id": "kEVk5afqZjf1",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "EMO_json_path_test = f\"{repo_path}{branch}/datasets/EMO23_lexicon_per_word_test.json\"\n",
        "EMP_json_path_test = f\"{repo_path}{branch}/datasets/EMP23_lexicon_per_word_test.json\"\n",
        "\n",
        "!wget {EMO_json_path_test}\n",
        "!wget {EMP_json_path_test}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:06:38.757963Z",
          "iopub.status.busy": "2023-07-11T19:06:38.757555Z",
          "iopub.status.idle": "2023-07-11T19:06:52.358142Z",
          "shell.execute_reply": "2023-07-11T19:06:52.357177Z",
          "shell.execute_reply.started": "2023-07-11T19:06:38.757924Z"
        },
        "id": "XDmQslVxDhy1",
        "outputId": "884b7a06-82b3-4bed-8b02-96d54434ea59",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'utils' from '/content/utils.py'>"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "from typing import Optional, Union, Tuple\n",
        "import json\n",
        "import random\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "\tTrainingArguments,\n",
        "\tTrainer,\n",
        "\tTrainerCallback,\n",
        "\tEarlyStoppingCallback,\n",
        "\tAutoTokenizer,\n",
        "\tBertModel,\n",
        "\tRobertaModel,\n",
        "\tBertForSequenceClassification,\n",
        "\tRobertaForSequenceClassification\n",
        "\t)\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from nrclex import NRCLex\n",
        "import importlib\n",
        "from utils import *\n",
        "importlib.reload(sys.modules['utils'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:06:52.360520Z",
          "iopub.status.busy": "2023-07-11T19:06:52.359527Z",
          "iopub.status.idle": "2023-07-11T19:06:52.392731Z",
          "shell.execute_reply": "2023-07-11T19:06:52.391807Z",
          "shell.execute_reply.started": "2023-07-11T19:06:52.360482Z"
        },
        "id": "quom7lWCDiiI",
        "outputId": "2de2a830-ee59-44e0-efcb-34b852744046",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======= CUDA Available =======\n"
          ]
        }
      ],
      "source": [
        "# set CUDA if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"======= CUDA Available =======\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"======= CUDA NOT Available, run on CPU =======\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLrhz2zGdhJT"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:06:52.394814Z",
          "iopub.status.busy": "2023-07-11T19:06:52.394204Z",
          "iopub.status.idle": "2023-07-11T19:06:52.406593Z",
          "shell.execute_reply": "2023-07-11T19:06:52.405612Z",
          "shell.execute_reply.started": "2023-07-11T19:06:52.394779Z"
        },
        "id": "S2o5IfHdgXXf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "EMOTIONS_LEX = [\n",
        "    'fear',\n",
        "    'anger',\n",
        "    'anticipation',\n",
        "    'trust',\n",
        "    'surprise',\n",
        "    'positive',\n",
        "    'negative',\n",
        "    'sadness',\n",
        "    'disgust',\n",
        "    'joy',\n",
        "    'hope'\n",
        "]\n",
        "\n",
        "EMOTIONS_TO_PREDICT = [\n",
        "    'sadness',\n",
        "    'neutral',\n",
        "    'fear',\n",
        "    'anger',\n",
        "    'surprise',\n",
        "    'joy',\n",
        "    'hope',\n",
        "    'disgust'\n",
        "]\n",
        "\n",
        "config = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GMoRVEFkQIPx"
      },
      "outputs": [],
      "source": [
        "#@title Task type\n",
        "TASK = \"EMO\" #@param [\"EMO\", \"EMP\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJpUGgCa2MZt"
      },
      "outputs": [],
      "source": [
        "#zip unzip\n",
        "best_model_path = \"\"\n",
        "checkpoint_path = f\"{best_model_path}/checkpoint\"\n",
        "with open(f\"{best_model_path}/config.json\") as file:\n",
        "  config = json.load(file)\n",
        "\n",
        "print(\"\\nCONFIGURATION\")\n",
        "for k,v in config.items():\n",
        "  print(f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gsXzUtCBv-j"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIUOnGp-_6lV"
      },
      "source": [
        "### WASSA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:06:52.410007Z",
          "iopub.status.busy": "2023-07-11T19:06:52.409372Z",
          "iopub.status.idle": "2023-07-11T19:06:52.429476Z",
          "shell.execute_reply": "2023-07-11T19:06:52.428716Z",
          "shell.execute_reply.started": "2023-07-11T19:06:52.409972Z"
        },
        "id": "WOuBgl-TmlM-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class WASSADataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        essay,\n",
        "        essay_id,\n",
        "        targets,\n",
        "        prompt_before_SEP=None,\n",
        "        prompt_after_SEP=None,\n",
        "        prompt_inlcusion_prob=0.5,\n",
        "        EMP_lexicon = None,\n",
        "        EMO_lexicon = None,\n",
        "        global_features = None,\n",
        "        local_emotions = False,\n",
        "        local_empathy = False,\n",
        "        local_distress = False,\n",
        "        max_len=None\n",
        "        ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.essay = essay\n",
        "        self.essay_id = essay_id\n",
        "        self.targets = targets\n",
        "        self.EMP_lexicon = EMP_lexicon\n",
        "        self.EMO_lexicon = EMO_lexicon\n",
        "        self.prompt_before_SEP = prompt_before_SEP\n",
        "        self.prompt_after_SEP = prompt_after_SEP\n",
        "        self.prompt_inlcusion_prob = prompt_inlcusion_prob\n",
        "        self.global_features = global_features\n",
        "        self.local_emotions = local_emotions\n",
        "        self.local_empathy = local_empathy\n",
        "        self.local_distress = local_distress\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.essay)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        essay = str(self.essay[index])\n",
        "        essay_id = self.essay_id[index]\n",
        "        mask = self.tokenizer.convert_ids_to_tokens(self.tokenizer.mask_token_id)\n",
        "        words_to_mask = EMOTIONS_TO_PREDICT + ['low', 'high', 'medium', 'medium-low', 'medium-high']\n",
        "\n",
        "        prompt_before_SEP = \"\"\n",
        "        if self.prompt_before_SEP is not None:\n",
        "          for p in self.prompt_before_SEP[index]:\n",
        "            prompt_before_SEP += \" \" + str(p)\n",
        "\n",
        "        prompt_after_SEP = \"\"\n",
        "        random_float = random.random()\n",
        "        if self.prompt_after_SEP is not None:\n",
        "          for p in self.prompt_after_SEP[index]:\n",
        "            prompt_after_SEP += \" \" + str(p)\n",
        "          if random_float < self.prompt_inlcusion_prob:\n",
        "            for word in words_to_mask:\n",
        "              prompt_after_SEP = prompt_after_SEP.replace(word, mask)\n",
        "\n",
        "        text = essay\n",
        "        text_pair = None\n",
        "        if prompt_before_SEP != \"\":\n",
        "          text += str(prompt_before_SEP)\n",
        "        if prompt_after_SEP != \"\":\n",
        "          text_pair = str(prompt_after_SEP)\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text=essay,\n",
        "            text_pair=text_pair,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "          'input_ids': inputs['input_ids'].flatten(),\n",
        "          'attention_mask': inputs['attention_mask'].flatten(),\n",
        "          'token_type_ids': inputs[\"token_type_ids\"].flatten()\n",
        "        }\n",
        "\n",
        "        if self.targets is not None:\n",
        "          item['labels'] = torch.FloatTensor(self.targets[index])\n",
        "\n",
        "        if self.global_features is not None:\n",
        "          item['global_features'] = self.global_features[index]\n",
        "\n",
        "        n_local_features = 0\n",
        "        features_tokens_row = []\n",
        "        if self.local_emotions:\n",
        "          n_local_features += len(EMOTIONS_LEX)\n",
        "          for i in range(len(EMOTIONS_LEX)):\n",
        "            features_tokens_row.append(0)\n",
        "        if self.local_empathy:\n",
        "          n_local_features += 1\n",
        "          features_tokens_row.append(4)\n",
        "        if self.local_distress:\n",
        "          n_local_features += 1\n",
        "          features_tokens_row.append(0)\n",
        "\n",
        "        if n_local_features > 0:\n",
        "          features_tokens = np.full((self.tokenizer.model_max_length, n_local_features), features_tokens_row)\n",
        "        else:\n",
        "          features_tokens = None\n",
        "\n",
        "        word_count=0\n",
        "        first_char=True\n",
        "        last_char_is_space=False\n",
        "        for char_idx, char in enumerate(essay):\n",
        "          token_idx = inputs.char_to_token(char_idx)\n",
        "          if token_idx is None:\n",
        "            if first_char: last_char_is_space=True\n",
        "            if not last_char_is_space and not first_char:\n",
        "              word_count+=1\n",
        "              last_char_is_space=True\n",
        "            continue\n",
        "          elif last_char_is_space:\n",
        "            last_char_is_space=False\n",
        "          first_char=False\n",
        "\n",
        "          j = 0\n",
        "          if char not in string.punctuation:\n",
        "            if self.local_emotions:\n",
        "              for i, emo in enumerate(EMOTIONS_LEX):\n",
        "                features_tokens[token_idx][i] = self.EMO_lexicon[str(essay_id)][emo][word_count]\n",
        "              j += len(EMOTIONS_LEX)\n",
        "\n",
        "            if self.local_empathy:\n",
        "              features_tokens[token_idx][j] = self.EMP_lexicon[str(essay_id)]['empathy'][word_count]\n",
        "              j += 1\n",
        "\n",
        "\n",
        "            if self.local_distress:\n",
        "              features_tokens[token_idx][j] = self.EMP_lexicon[str(essay_id)]['distress'][word_count]\n",
        "\n",
        "        if features_tokens is not None:\n",
        "            item['local_features'] = torch.FloatTensor(features_tokens)\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFI6AulQsYci"
      },
      "source": [
        "Read dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:09:57.899510Z",
          "iopub.status.busy": "2023-07-11T19:09:57.899102Z",
          "iopub.status.idle": "2023-07-11T19:09:57.904814Z",
          "shell.execute_reply": "2023-07-11T19:09:57.903813Z",
          "shell.execute_reply.started": "2023-07-11T19:09:57.899476Z"
        },
        "id": "BjM03TscDwcz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "TEST_DATA = f\"{repo_path}{branch}/datasets/WASSA23_essay_level_test_preproc.tsv\"\n",
        "\n",
        "test_df = pd.read_csv(TEST_DATA, sep='\\t')\n",
        "\n",
        "if TASK==\"EMP\":\n",
        "  train_df = add_emp_dist_levels(train_df)\n",
        "  train_df = add_prompt_truth(train_df, TASK, \"3\")\n",
        "  val_df = add_emp_dist_levels(val_df)\n",
        "  val_df = add_prompt_truth(val_df, TASK, \"3\")\n",
        "  dev_df = add_emp_dist_levels(dev_df)\n",
        "  dev_df = add_prompt_truth(dev_df, TASK, \"3\")\n",
        "if TASK==\"EMO\":\n",
        "  train_df = add_prompt_truth(train_df, TASK)\n",
        "  val_df = add_prompt_truth(val_df, TASK)\n",
        "  dev_df = add_prompt_truth(dev_df, TASK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDJWlHhwsfwL"
      },
      "source": [
        "Encode targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:10:06.006808Z",
          "iopub.status.busy": "2023-07-11T19:10:06.006358Z",
          "iopub.status.idle": "2023-07-11T19:10:06.021824Z",
          "shell.execute_reply": "2023-07-11T19:10:06.020868Z",
          "shell.execute_reply.started": "2023-07-11T19:10:06.006776Z"
        },
        "id": "6LKqRnKdsJdu",
        "outputId": "dfd19d2a-60e0-4470-ce70-be24bd4c2ebb",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:895: UserWarning: unknown class(es) ['neutral'] will be ignored\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "if TASK ==\"EMO\":\n",
        "  if TASK == \"EMO\" and config.get('num_labels') == 7:\n",
        "    label_encoder = EmotionsLabelEncoderNeutral()\n",
        "  else:\n",
        "    label_encoder = EmotionsLabelEncoder()\n",
        "  label_encoder.fit(train_df.emotion)\n",
        "  y_train = label_encoder.encode(train_df.emotion)\n",
        "  y_val = label_encoder.encode(val_df.emotion)\n",
        "  y_dev = label_encoder.encode(dev_df.emotion)\n",
        "\n",
        "if TASK == \"EMP\":\n",
        "  y_train = np.array(train_df[['empathy', 'distress']])\n",
        "  y_val = np.array(val_df[['empathy', 'distress']])\n",
        "  y_dev = np.array(dev_df[['empathy', 'distress']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbCKVWh7-gpg"
      },
      "source": [
        "Extra global features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VpOAsXL6xHa"
      },
      "outputs": [],
      "source": [
        "global_features_train = None\n",
        "global_features_val = None\n",
        "global_features_dev = None\n",
        "\n",
        "if len(config.get('global_features_names')) > 0:\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(np.array(train_df[config.get('global_features_names')]))\n",
        "\n",
        "  def standard_scalar_features(features):\n",
        "    return scaler.transform(features)\n",
        "\n",
        "  global_features_train =  standard_scalar_features(np.array(train_df[config.get('global_features_names')]))\n",
        "  global_features_val =  standard_scalar_features(np.array(val_df[config.get('global_features_names')]))\n",
        "  global_features_dev =  standard_scalar_features(np.array(dev_df[config.get('global_features_names')]))\n",
        "\n",
        "  if TASK == \"EMP\" and config.get('gold_emotions'):\n",
        "    label_encoder = EmotionsLabelEncoder()\n",
        "    label_encoder.fit(train_df.emotion)\n",
        "    gold_emotions_train = label_encoder.encode(train_df.emotion)\n",
        "    gold_emotions_val = label_encoder.encode(val_df.emotion)\n",
        "    gold_emotions_dev = label_encoder.encode(dev_df.emotion)\n",
        "    global_features_train = np.concatenate((global_features_train, gold_emotions_train), axis = 1)\n",
        "    global_features_val = np.concatenate((global_features_val, gold_emotions_val), axis = 1)\n",
        "    global_features_dev = np.concatenate((global_features_dev, gold_emotions_dev), axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfm0n2Mk6xHb"
      },
      "source": [
        "Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WnGSQwv6xHb"
      },
      "outputs": [],
      "source": [
        "prompt_before_SEP_train = None\n",
        "prompt_after_SEP_train = None\n",
        "prompt_before_SEP_val = None\n",
        "prompt_after_SEP_val = None\n",
        "prompt_before_SEP_dev = None\n",
        "prompt_after_SEP_dev = None\n",
        "\n",
        "if len(config.get('prompt_names_before_SEP')) > 0:\n",
        "  prompt_before_SEP_train = np.array(train_df[config.get('prompt_names_before_SEP')])\n",
        "  prompt_before_SEP_val = np.array(val_df[config.get('prompt_names_before_SEP')])\n",
        "  prompt_before_SEP_dev = np.array(dev_df[config.get('prompt_names_before_SEP')])\n",
        "\n",
        "if len(config.get('prompt_names_after_SEP')) > 0:\n",
        "  prompt_after_SEP_train = np.array(train_df[config.get('prompt_names_after_SEP')])\n",
        "  prompt_after_SEP_val = np.array(val_df[config.get('prompt_names_after_SEP')])\n",
        "  prompt_after_SEP_dev = np.array(dev_df[config.get('prompt_names_after_SEP')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwYXJId_-Sb5"
      },
      "source": [
        "Lexicons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:09.185627Z",
          "iopub.status.busy": "2023-07-11T19:11:09.185261Z",
          "iopub.status.idle": "2023-07-11T19:11:09.667265Z",
          "shell.execute_reply": "2023-07-11T19:11:09.666254Z",
          "shell.execute_reply.started": "2023-07-11T19:11:09.185595Z"
        },
        "id": "Qg0D38kk-Rrv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "EMO_lexicon_train_dict = None\n",
        "EMP_lexicon_train_dict = None\n",
        "EMO_lexicon_dev_dict = None\n",
        "EMP_lexicon_dev_dict = None\n",
        "EMO_lexicon_test_dict = None\n",
        "EMP_lexicon_test_dict = None\n",
        "if config.get('local_features_names') is not None:\n",
        "  if 'emotions' in config.get('local_features_names'):\n",
        "    with open(\"/content/EMO23_lexicon_per_word_.json\") as json_file:\n",
        "      EMO_lexicon_train_dict = json.load(json_file)\n",
        "    with open(\"/content/EMO23_lexicon_per_word_test.json\") as json_file:\n",
        "      EMO_lexicon_test_dict = json.load(json_file)\n",
        "\n",
        "  if 'empathy' in config.get('local_features_names') or 'distress' in config.get('local_features_names'):\n",
        "    with open(\"/content/EMP23_lexicon_per_word_.json\") as json_file:\n",
        "      EMP_lexicon_train_dict = json.load(json_file)\n",
        "    with open(\"/content/EMP23_lexicon_per_word_test.json\") as json_file:\n",
        "      EMP_lexicon_test_dict = json.load(json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXIimY5ABmlc"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:17.243014Z",
          "iopub.status.busy": "2023-07-11T19:11:17.241592Z",
          "iopub.status.idle": "2023-07-11T19:11:17.253391Z",
          "shell.execute_reply": "2023-07-11T19:11:17.252383Z",
          "shell.execute_reply.started": "2023-07-11T19:11:17.242979Z"
        },
        "id": "hfwVaaj-0qDq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_loss_weights(y, method):\n",
        "  if method == 'balanced':\n",
        "    weights_train = y.shape[0] / (y.shape[1] * np.sum(y, axis=0))\n",
        "  else:\n",
        "    inverse_n_samples = 1 / np.sum(y, axis=0)\n",
        "    sum_inverses = sum(inverse_n_samples)\n",
        "    weights_train = inverse_n_samples / sum_inverses\n",
        "  return torch.cuda.FloatTensor(weights_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:17.257384Z",
          "iopub.status.busy": "2023-07-11T19:11:17.255861Z",
          "iopub.status.idle": "2023-07-11T19:11:22.360433Z",
          "shell.execute_reply": "2023-07-11T19:11:22.359379Z",
          "shell.execute_reply.started": "2023-07-11T19:11:17.257351Z"
        },
        "id": "wVh3_jCFzmK7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "loss_weights_train = None\n",
        "if model_config.get('weighted_loss')!='None':\n",
        "  loss_weights_train = get_loss_weights(y_train, model_config.get('weighted_loss'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6aAhPift1UL"
      },
      "source": [
        "### Custom model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:23.702374Z",
          "iopub.status.busy": "2023-07-11T19:11:23.702059Z",
          "iopub.status.idle": "2023-07-11T19:11:23.732141Z",
          "shell.execute_reply": "2023-07-11T19:11:23.730692Z",
          "shell.execute_reply.started": "2023-07-11T19:11:23.702348Z"
        },
        "id": "2Vj3W-oUhBMZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "    #Head for sentence-level classification tasks.\n",
        "\n",
        "    def __init__(self, config, dim_extra_features, hidden_layers_to_concat, classifier_dropout, local_features, mean_last_cls):\n",
        "        super().__init__()\n",
        "        self.local_features = local_features\n",
        "\n",
        "        if mean_last_cls:\n",
        "          total_dims = config.hidden_size + dim_extra_features\n",
        "        else:\n",
        "          total_dims = config.hidden_size*hidden_layers_to_concat + dim_extra_features\n",
        "        if self.local_features:\n",
        "          total_dims += config.hidden_size\n",
        "        self.dense = nn.Linear(total_dims, total_dims)\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.out_proj = nn.Linear(total_dims, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        features = features.to(torch.float32) # by default float32 is used as the dtype\n",
        "        x = self.dense(features)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "class CustomSequenceClassification(model_class):\n",
        "\n",
        "    def __init__(self, config, dim_extra_features=0, model_class = None, local_features_names=None, loss_weights=None, n_last_cls = 1, mean_last_cls = False, concat_local_features = False):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "        self.local_features_names = local_features_names\n",
        "        self.loss_weights = loss_weights\n",
        "        self.n_last_cls = n_last_cls\n",
        "        self.mean_last_cls = mean_last_cls\n",
        "        self.concat_local_features = concat_local_features\n",
        "        self.model_class = model_class\n",
        "        if self.model_class == \"BertPreTrainedModel\":\n",
        "          self.bert = BertModel(config)\n",
        "\n",
        "        if self.model_class == \"RobertaPreTrainedModel\":\n",
        "          self.roberta = RobertaModel(config)\n",
        "\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = ClassificationHead(config,\n",
        "                                            dim_extra_features,\n",
        "                                            n_last_cls,\n",
        "                                            classifier_dropout,\n",
        "                                            local_features_names is not None,\n",
        "                                            mean_last_cls)\n",
        "        self.post_init()\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        local_features = None,\n",
        "        global_features = None,\n",
        "        token_type_ids: Optional[torch.LongTensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = True,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, SequenceClassifierOutput]:\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if self.model_class == \"BertPreTrainedModel\":\n",
        "          outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        if self.model_class == \"RobertaPreTrainedModel\":\n",
        "          outputs = self.roberta(\n",
        "              input_ids,\n",
        "              attention_mask=attention_mask,\n",
        "              token_type_ids=token_type_ids,\n",
        "              position_ids=position_ids,\n",
        "              head_mask=head_mask,\n",
        "              inputs_embeds=inputs_embeds,\n",
        "              output_attentions=output_attentions,\n",
        "              output_hidden_states=output_hidden_states,\n",
        "              return_dict=return_dict,\n",
        "          )\n",
        "\n",
        "        cls_tokens = []\n",
        "        for i in range(1, self.n_last_cls + 1):\n",
        "            cls_tokens.append(outputs.hidden_states[-1 * i][:, 0, :])\n",
        "        if self.mean_last_cls:\n",
        "          # average cls tokens\n",
        "          output = torch.mean(torch.stack(cls_tokens), dim=0)\n",
        "        else:\n",
        "          # concat cls tokens\n",
        "          output = torch.cat(cls_tokens, dim=1)\n",
        "\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        if local_features is not None:\n",
        "          tokens_output = outputs.last_hidden_state\n",
        "          tokens_output = self.dropout(tokens_output)\n",
        "\n",
        "          if self.concat_local_features:\n",
        "            tokens_output = torch.cat((\n",
        "                    tokens_output,\n",
        "                    local_features.reshape(outputs.last_hidden_state.shape[0], outputs.last_hidden_state.shape[1], -1)),\n",
        "                              dim=2)\n",
        "\n",
        "          mask = torch.zeros_like(attention_mask)\n",
        "          # unmask tokens with high or low empathy, high distress levels, or expressing at least one emotion\n",
        "          j = 0\n",
        "          if 'emotions' in self.local_features_names:\n",
        "            emotion_values = local_features[:,:,:11]\n",
        "            mask[emotion_values.sum(dim=-1)>=1] = 1.0\n",
        "            j += 11\n",
        "\n",
        "          if 'empathy' in self.local_features_names:\n",
        "            empathy_values = local_features[:,:,j]\n",
        "            mask[(empathy_values>5) | ((empathy_values<3) & (empathy_values>=1))] = 1.0\n",
        "            j += 1\n",
        "\n",
        "          if 'distress' in self.local_features_names:\n",
        "            distress_values = local_features[:,:,j]\n",
        "            mask[distress_values>4] = 1.0\n",
        "\n",
        "          # mean pooling of unmasked tokens\n",
        "          input_mask_expanded = mask.unsqueeze(-1).expand(tokens_output.size()).float()\n",
        "          sum_embeddings = torch.sum(tokens_output * input_mask_expanded, 1)\n",
        "          sum_mask = input_mask_expanded.sum(1)\n",
        "          sum_mask = torch.clamp(sum_mask, min = 1e-9)\n",
        "          tokens_output = sum_embeddings/sum_mask\n",
        "\n",
        "          # concat pooled tokens lexically relevant with cls token\n",
        "          output = torch.cat((output, tokens_output), dim=-1)\n",
        "\n",
        "\n",
        "        if global_features is not None: # global\n",
        "          output = torch.cat((output, global_features), dim=-1)\n",
        "\n",
        "        logits = self.classifier(output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = nn.MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = nn.CrossEntropyLoss(weight = self.loss_weights)\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = nn.BCEWithLogitsLoss(weight = self.loss_weights)\n",
        "                loss = loss_fct(logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:22.362620Z",
          "iopub.status.busy": "2023-07-11T19:11:22.361776Z",
          "iopub.status.idle": "2023-07-11T19:11:23.697235Z",
          "shell.execute_reply": "2023-07-11T19:11:23.696230Z",
          "shell.execute_reply.started": "2023-07-11T19:11:22.362585Z"
        },
        "id": "wd7W2HhYsbai",
        "outputId": "cc03f2d8-40a2-482a-8791-08703289314e",
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaTokenizerFast(name_or_path='SamLowe/roberta-base-go_emotions', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True)"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(config.get('tokenizer_name'), truncation=True)\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:23.734900Z",
          "iopub.status.busy": "2023-07-11T19:11:23.734558Z",
          "iopub.status.idle": "2023-07-11T19:11:27.562951Z",
          "shell.execute_reply": "2023-07-11T19:11:27.562030Z",
          "shell.execute_reply.started": "2023-07-11T19:11:23.734871Z"
        },
        "id": "GvJbF80XhSDE",
        "outputId": "8e647d32-0610-40f1-f6dc-e9bf2139286c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of CustomSequenceClassification were not initialized from the model checkpoint at SamLowe/roberta-base-go_emotions and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of CustomSequenceClassification were not initialized from the model checkpoint at SamLowe/roberta-base-go_emotions and are newly initialized because the shapes did not match:\n",
            "- classifier.dense.weight: found shape torch.Size([768, 768]) in the checkpoint and torch.Size([773, 773]) in the model instantiated\n",
            "- classifier.dense.bias: found shape torch.Size([768]) in the checkpoint and torch.Size([773]) in the model instantiated\n",
            "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([7, 773]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CustomSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (classifier): ClassificationHead(\n",
              "    (dense): Linear(in_features=773, out_features=773, bias=True)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "    (out_proj): Linear(in_features=773, out_features=7, bias=True)\n",
              "  )\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              ")"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if TASK == \"EMO\":\n",
        "  problem_type = \"multi_label_classification\"\n",
        "if TASK == \"EMP\":\n",
        "  problem_type = \"regression\"\n",
        "\n",
        "model = CustomSequenceClassification.from_pretrained(\n",
        "                                            checkpoint_path,\n",
        "                                            problem_type = problem_type,\n",
        "                                            classifier_dropout = config.get('dropout'),\n",
        "                                            model_class = config.get('model_class_string'),\n",
        "                                            num_labels=config.get('num_labels'),\n",
        "                                            dim_extra_features = config.get('dim_extra_features'),\n",
        "                                            local_features_names = config.get('local_features_names'),\n",
        "                                            n_last_cls = config.get('n_last_cls'),\n",
        "                                            mean_last_cls = config.get('mean_last_cls'),\n",
        "                                            concat_local_features = config.get('concat_local_features'),\n",
        "                                            loss_weights = loss_weights_train,\n",
        "                                            ignore_mismatched_sizes=True\n",
        "                                            )\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc5stfSah0Kj"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsAa9g5j5qKJ"
      },
      "outputs": [],
      "source": [
        "#TODO: if??\n",
        "\n",
        "test_df = add_prompt_truth(test_df, TASK)\n",
        "\n",
        "num_missing_features = 0\n",
        "if config['gold_emotions']:\n",
        "    num_missing_features += 8\n",
        "if config['gold_empathy']:\n",
        "    num_missing_features += 1\n",
        "if config['gold_distress']:\n",
        "    num_missing_features += 1\n",
        "\n",
        "global_features = standard_scalar_features(np.array(test_df[config.get('global_features_names')]))\n",
        "if num_missing_features > 0:\n",
        "    global_features = np.concatenate((global_features, np.zeros((100, num_missing_features), int)), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:27.565429Z",
          "iopub.status.busy": "2023-07-11T19:11:27.564800Z",
          "iopub.status.idle": "2023-07-11T19:11:27.572683Z",
          "shell.execute_reply": "2023-07-11T19:11:27.571493Z",
          "shell.execute_reply.started": "2023-07-11T19:11:27.565394Z"
        },
        "id": "DUIqO6AiZ8al",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "test_set = WASSADataset(\n",
        "                        tokenizer=tokenizer,\n",
        "                        essay = test_df.essay,\n",
        "                        prompt_before_SEP = prompt_before_SEP_test,\n",
        "                        prompt_after_SEP = prompt_after_SEP_test,\n",
        "                        essay_id = test_df.essay_id,\n",
        "                        targets = None,\n",
        "                        global_features = global_features_test,\n",
        "                        prompt_inlcusion_prob = 1,\n",
        "                        EMO_lexicon = EMO_lexicon_test_dict,\n",
        "                        EMP_lexicon = EMP_lexicon_test_dict,\n",
        "                        local_emotions = config.get('emo_count_local'),\n",
        "                        local_empathy = config.get('empathy_count_local'),\n",
        "                        local_distress = config.get('distress_count_local')\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abCztdbQ45F-"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=model)\n",
        "outs = trainer.predict(test_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXgFzu32_RBo"
      },
      "outputs": [],
      "source": [
        "if TASK == \"EMO\":\n",
        "  golds = label_encoder.decode(outs.label_ids)\n",
        "  predictions = predict_emotions(outs.predictions[0], False)\n",
        "if TASK == \"EMP\":\n",
        "  golds = outs.label_ids\n",
        "  predictions = outs.predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T19:20:18.388286Z",
          "iopub.status.idle": "2023-07-11T19:20:18.389135Z",
          "shell.execute_reply": "2023-07-11T19:20:18.388900Z",
          "shell.execute_reply.started": "2023-07-11T19:20:18.388873Z"
        },
        "id": "Im2VGnmQs7bY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "path_predictions = f\"predictions_{TASK}.tsv\"\n",
        "write_predictions(predictions, path_predictions)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(path_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pkHv9w4tM1F"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "WX2a-YLE6pWz"
      },
      "outputs": [],
      "source": [
        "#@title Predict\n",
        "\n",
        "text = 'WRITE HERE' #@param {type:\"string\"}\n",
        "\n",
        "model(text)\n",
        "\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "# Make the prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "if TASK == \"EMO\":\n",
        "  prediction = predict_emotions(outputs.predictions[0], False)\n",
        "if TASK == \"EMP\":\n",
        "  prediction = outputs.predictions[0]\n",
        "\n",
        "print(f\"Predicted: {prediction}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
