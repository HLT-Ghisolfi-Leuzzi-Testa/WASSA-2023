{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Bidirectional, Dense, Embedding, LSTM\n",
    "from tensorflow.keras.saving import load_model, save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bidirectional_LSTM:\n",
    "    def __init__(self, X_train, y_train, embeddings_matrix, n_hidden_units=[10, 20, 10], dropuot_fraction=0.3, \n",
    "                 epochs=500, batch_size=200, validation_fraction=0.2):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.embeddings_matrix = embeddings_matrix\n",
    "\n",
    "        self.n_hidden_units = n_hidden_units\n",
    "        self.dropuot_fraction = dropuot_fraction\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_fraction = validation_fraction\n",
    "\n",
    "        self.__init_model()\n",
    "\n",
    "    def __init_model(self):\n",
    "        # Bidirectional LSTM\n",
    "        self.model = Sequential(name='Bidirectional_LSTM')\n",
    "        \n",
    "        # input layer\n",
    "        self.model.add(Embedding(input_dim=self.embeddings_matrix.shape[0],\n",
    "                            output_dim=self.embeddings_matrix.shape[1],\n",
    "                            embeddings_initializer=Constant(self.embeddings_matrix),\n",
    "                            mask_zero=True,\n",
    "                            sparse=False))\n",
    "    # hidden layer\n",
    "        for n_units in self.n_hidden_units[:-1]:\n",
    "            self.__add_bidirectional_layer(n_units, return_sequences=True)\n",
    "        \n",
    "        self.__add_bidirectional_layer(self.n_hidden_units[-1], return_sequences=False)\n",
    "\n",
    "        # output layer\n",
    "        self.model.add(Dense(units=1,\n",
    "                        activation='sigmoid'))\n",
    "        \n",
    "        self.model.compile(loss='binary_crossentropy', \n",
    "                    optimizer='adam', \n",
    "                    metrics=['accuracy'])\n",
    "    \n",
    "    def __add_bidirectional_layer(self, n_units, return_sequences):\n",
    "        # forward layer\n",
    "        forward = LSTM(units=n_units,\n",
    "                    activation=\"tanh\",\n",
    "                    recurrent_activation=\"sigmoid\",\n",
    "                    dropout=self.dropuot_fraction,\n",
    "                    recurrent_dropout=self.dropuot_fraction,\n",
    "                    return_sequences=return_sequences)\n",
    "        \n",
    "        # backward layer\n",
    "        backward = LSTM(units=n_units,\n",
    "                    activation=\"tanh\",\n",
    "                    recurrent_activation=\"sigmoid\",\n",
    "                    dropout=self.dropuot_fraction,\n",
    "                    recurrent_dropout=self.dropuot_fraction,\n",
    "                    return_sequences=return_sequences,\n",
    "                    go_backwards=True)\n",
    "\n",
    "        self.model.add(Bidirectional(forward, backward_layer=backward))\n",
    "\n",
    "    def train(self):\n",
    "        # early_stopping\n",
    "        callbacks = EarlyStopping(monitor=\"val_loss\",\n",
    "                                min_delta=1e-12,\n",
    "                                patience=2,\n",
    "                                verbose=0,\n",
    "                                mode=\"auto\",\n",
    "                                baseline=None,\n",
    "                                restore_best_weights=False,\n",
    "                                start_from_epoch=0)\n",
    "\n",
    "        steps_per_epoch = int(self.X_train.shape[0] * 0.3 / self.batch_size)\n",
    "\n",
    "        self.history = self.model.fit(x=np.array(self.X_train),\n",
    "                                y=np.array(self.y_train),\n",
    "                                batch_size=self.batch_size,\n",
    "                                epochs=self.epochs,\n",
    "                                verbose=\"auto\",\n",
    "                                callbacks=[callbacks],\n",
    "                                validation_split=self.validation_fraction, # validation size\n",
    "                                shuffle=True,\n",
    "                                steps_per_epoch=steps_per_epoch).__dict__\n",
    "        self.accuracy = self.model.evaluate(x=self.X_train,\n",
    "                                y=self.y_train,\n",
    "                                batch_size=self.batch_size,\n",
    "                                verbose=\"auto\",\n",
    "                                return_dict=False) # metric return as a list\n",
    "    \n",
    "    def show_learning_curve(self):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "        # error\n",
    "        ax1.plot(self.history['history']['loss'], label=\"Training error\", color=\"red\")\n",
    "        if self.validation_fraction > 0:\n",
    "            ax1.plot(self.history['history']['val_loss'], label=\"Validation error\", color=\"blue\")\n",
    "\n",
    "        ax1.legend()\n",
    "        ax1.grid()\n",
    "        ax1.set(xlabel=\"Epochs\", ylabel=\"Reconstruction Errors (binary crossentropy)\")\n",
    "        ax1.set_title(\"Reconstruction Errors\")\n",
    "        \n",
    "        # accuracy\n",
    "        ax2.plot(self.history['history']['accuracy'], label=\"Training accuracy\", color=\"red\")\n",
    "        if self.validation_fraction > 0:\n",
    "            ax2.plot(self.history['history']['val_accuracy'], label=\"Validation accuracy\", color=\"blue\")\n",
    "\n",
    "        ax2.legend()\n",
    "        ax2.grid()\n",
    "        ax2.set(xlabel=\"Epochs\", ylabel=\"Accuracy\")\n",
    "        ax2.set_title(\"Accuracy\")\n",
    "    \n",
    "    def predict(self, X_test, batch_size=None):\n",
    "        return np.where(self.model.predict(X_test, batch_size=batch_size) < 0.5, 0, 1)\n",
    "    \n",
    "    def compute_prediction_score(self, X_test, y_test, evaluation_metric='accuracy', batch_size=None):\n",
    "        return self.model.evaluate(X_test, y_test, batch_size=batch_size, return_dict=True)[evaluation_metric]\n",
    "\n",
    "    def save_model_history(self, filename):\n",
    "        with open(filename, 'wb') as file:\n",
    "            pickle.dump([self.history['history'], self.accuracy], file)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_history(filename):    \n",
    "    with open(filename, 'rb') as file:\n",
    "        [history, accuracy] = pickle.load(file)\n",
    "            \n",
    "    return history, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bidirectional_LSTM(X_train, y_train, embeddings_matrix=embeddings_matrix, n_hidden_units=[2, 2], \n",
    "                                   dropuot_fraction=0.3, epochs=30, batch_size=4)\n",
    "        \n",
    "model.train()\n",
    "\n",
    "filename = ('models/bi_lstm.pkl' %i)\n",
    "model.save_model_history(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_learning_curve()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
