{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/irenetesta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/irenetesta/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANGER_LEXICO = \"./lexico/anger-NRC-Emotion-Lexicon.txt\"\n",
    "# ANTICIPATION_LEXICO = \"./lexico/anticipation-NRC-Emotion-Lexicon.txt\"\n",
    "# DISGUST_LEXICO = \"./lexico/disgust-NRC-Emotion-Lexicon.txt\"\n",
    "# FEAR_LEXICO = \"./lexico/fear-NRC-Emotion-Lexicon.txt\"\n",
    "# JOY_LEXICO = \"./lexico/joy-NRC-Emotion-Lexicon.txt\"\n",
    "# NEGATIVE_LEXICO = \"./lexico/negative-NRC-Emotion-Lexicon.txt\" # TODO; remove?\n",
    "# POSITIVE_LEXICO = \"./lexico/positive-NRC-Emotion-Lexicon.txt\"\n",
    "# SADNESS_LEXICO = \"./lexico/sadness-NRC-Emotion-Lexicon.txt\"\n",
    "# SUPRISE_LEXICO = \"./lexico/surprise-NRC-Emotion-Lexicon.txt\"\n",
    "# TRUST_LEXICO = \"./lexico/trust-NRC-Emotion-Lexicon.txt\" # TODO; remove?\n",
    "\n",
    "lexico_categories = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'hope']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ham' in category_lexico_dfs['disgust']['word'].tolist()[0]:\n",
    "    print('yes')\n",
    "    category_lexico_dfs['disgust'][category_lexico_dfs['disgust']['word'] == 'baptist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word  anger\n",
      "0         idiotic      1\n",
      "1          offend      1\n",
      "2        strained      1\n",
      "3      punishment      1\n",
      "4         kicking      1\n",
      "...           ...    ...\n",
      "14149     brigade      0\n",
      "14150   recurring      0\n",
      "14151  pharmacist      0\n",
      "14152      caress      0\n",
      "14153    humorous      0\n",
      "\n",
      "[14154 rows x 2 columns]\n",
      "                word  disgust\n",
      "0       disobedience        1\n",
      "1             maggot        1\n",
      "2           betrayal        1\n",
      "3         ungrateful        1\n",
      "4           quagmire        1\n",
      "...              ...      ...\n",
      "14149       approval        0\n",
      "14150   acquisitions        0\n",
      "14151        repress        0\n",
      "14152  sequestration        0\n",
      "14153        lyrical        0\n",
      "\n",
      "[14154 rows x 2 columns]\n",
      "              word  fear\n",
      "0        parachute     1\n",
      "1        horrified     1\n",
      "2         hopeless     1\n",
      "3         validity     1\n",
      "4             pare     1\n",
      "...            ...   ...\n",
      "14149        gloss     0\n",
      "14150  altercation     0\n",
      "14151    occupying     0\n",
      "14152    stimulant     0\n",
      "14153     shilling     0\n",
      "\n",
      "[14154 rows x 2 columns]\n",
      "              word  joy\n",
      "0          angelic    1\n",
      "1          jackpot    1\n",
      "2         pleasant    1\n",
      "3          amnesty    1\n",
      "4           aspire    1\n",
      "...            ...  ...\n",
      "14149    bothering    0\n",
      "14150        broom    0\n",
      "14151  ineffectual    0\n",
      "14152    revolting    0\n",
      "14153          ham    0\n",
      "\n",
      "[14154 rows x 2 columns]\n",
      "                word  sadness\n",
      "0      hydrocephalus        1\n",
      "1         infliction        1\n",
      "2               dull        1\n",
      "3              cross        1\n",
      "4         disqualify        1\n",
      "...              ...      ...\n",
      "14149      absorbent        0\n",
      "14150           swim        0\n",
      "14151      specially        0\n",
      "14152            bis        0\n",
      "14153           rope        0\n",
      "\n",
      "[14154 rows x 2 columns]\n",
      "                word  surprise\n",
      "0               gawk         1\n",
      "1      improvisation         1\n",
      "2         excitation         1\n",
      "3            volcano         1\n",
      "4            cherish         1\n",
      "...              ...       ...\n",
      "14149    coexistence         0\n",
      "14150    justifiable         0\n",
      "14151        beading         0\n",
      "14152      wherefore         0\n",
      "14153          bulky         0\n",
      "\n",
      "[14154 rows x 2 columns]\n",
      "                word  hope\n",
      "0          angelic 0   NaN\n",
      "1          jackpot 1   NaN\n",
      "2         pleasant 1   NaN\n",
      "3          amnesty 0   NaN\n",
      "4           aspire 1   NaN\n",
      "...              ...   ...\n",
      "14149    bothering 0   NaN\n",
      "14150        broom 0   NaN\n",
      "14151  ineffectual 0   NaN\n",
      "14152    revolting 0   NaN\n",
      "14153          ham 0   NaN\n",
      "\n",
      "[14154 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "category_lexico_dfs = {}\n",
    "for category in lexico_categories:\n",
    "\tcategory_lexico_dfs[category] = pd.read_csv(f\"./lexico/{category}-NRC-Emotion-Lexicon.txt\", header=None, names=['word', category], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>hope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>idiotic</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>offend</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>strained</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>punishment</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kicking</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28303</th>\n",
       "      <td>bothering 0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28304</th>\n",
       "      <td>broom 0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28305</th>\n",
       "      <td>ineffectual 0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28306</th>\n",
       "      <td>revolting 0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28307</th>\n",
       "      <td>ham 0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28308 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                word  anger  disgust  fear  joy  sadness  surprise  hope\n",
       "0            idiotic    1.0      1.0   0.0  0.0      0.0       0.0   NaN\n",
       "1             offend    1.0      1.0   0.0  0.0      0.0       0.0   NaN\n",
       "2           strained    1.0      0.0   0.0  0.0      0.0       0.0   NaN\n",
       "3         punishment    1.0      1.0   1.0  0.0      0.0       0.0   NaN\n",
       "4            kicking    1.0      0.0   0.0  0.0      0.0       0.0   NaN\n",
       "...              ...    ...      ...   ...  ...      ...       ...   ...\n",
       "28303    bothering 0    NaN      NaN   NaN  NaN      NaN       NaN   NaN\n",
       "28304        broom 0    NaN      NaN   NaN  NaN      NaN       NaN   NaN\n",
       "28305  ineffectual 0    NaN      NaN   NaN  NaN      NaN       NaN   NaN\n",
       "28306    revolting 0    NaN      NaN   NaN  NaN      NaN       NaN   NaN\n",
       "28307          ham 0    NaN      NaN   NaN  NaN      NaN       NaN   NaN\n",
       "\n",
       "[28308 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexico = pd.DataFrame(columns=['word'])\n",
    "for category in lexico_categories:\n",
    "    lexico = pd.merge(lexico, category_lexico_dfs[category], on='word', how='outer')\n",
    "lexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexico.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"./datasets/WASSA23_essay_level_with_labels_train.tsv\"\n",
    "train_df = pd.read_csv(TRAIN_DATA_PATH, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>hope</th>\n",
       "      <th>preproc_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [word, anger, disgust, fear, joy, sadness, surprise, hope, preproc_word]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    return ' '.join(lemmatizer.lemmatize(word) for word in text.split())\n",
    "\n",
    "def stemming(text):\n",
    "    return ' '.join(stemmer.stem(word) for word in text.split())\n",
    "\n",
    "lexico['preproc_word'] = ''\n",
    "for index, row in lexico.iterrows():\n",
    "    lexico.loc[index, 'preproc_word'] = stemming(lemmatize(row['word']))\n",
    "lexico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexico.to_csv('./lexico/lexico.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it break my heart to see peopl live in those conditions. i hope that all the aid that wa sent to the island make it to the peopl who need it the most. i do not know what i would do it that wa my famili and i. i would hope that i would do my best, but i can see how depress and hopeless you could feel have your whole life chang becaus of a storm and not know where your next meal is come from.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>essay</th>\n",
       "      <th>empathy</th>\n",
       "      <th>distress</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>race</th>\n",
       "      <th>age</th>\n",
       "      <th>...</th>\n",
       "      <th>anger_count</th>\n",
       "      <th>anticipation_count</th>\n",
       "      <th>disgust_count</th>\n",
       "      <th>fear_count</th>\n",
       "      <th>joy_count</th>\n",
       "      <th>negative_count</th>\n",
       "      <th>positive_count</th>\n",
       "      <th>sadness_count</th>\n",
       "      <th>surprise_count</th>\n",
       "      <th>trust_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>It breaks my heart to see people living in tho...</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>6.625</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>I wonder why there aren't more people trying t...</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>6.000</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>After reading the article, you can't help but ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.375</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>213</td>\n",
       "      <td>It is so sad that someone who had such an amaz...</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>6.625</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>213</td>\n",
       "      <td>From reading the article, it looks like the wo...</td>\n",
       "      <td>6.833333</td>\n",
       "      <td>1.000</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>495</td>\n",
       "      <td>218</td>\n",
       "      <td>I feel that this will become a national proble...</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>6.750</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>496</td>\n",
       "      <td>103</td>\n",
       "      <td>The whole situation is sketchy. The wavering r...</td>\n",
       "      <td>3.166667</td>\n",
       "      <td>6.375</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>498</td>\n",
       "      <td>103</td>\n",
       "      <td>The death of a former aide to Russian Presiden...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>499</td>\n",
       "      <td>103</td>\n",
       "      <td>Everything about Russia really freaks me out. ...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>500</td>\n",
       "      <td>103</td>\n",
       "      <td>Whenever Russia and Putin are involved  I do n...</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>4.875</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>792 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     conversation_id  article_id  \\\n",
       "0                  2          35   \n",
       "1                  3          35   \n",
       "2                  5          35   \n",
       "3                  6         213   \n",
       "4                  8         213   \n",
       "..               ...         ...   \n",
       "787              495         218   \n",
       "788              496         103   \n",
       "789              498         103   \n",
       "790              499         103   \n",
       "791              500         103   \n",
       "\n",
       "                                                 essay   empathy  distress  \\\n",
       "0    It breaks my heart to see people living in tho...  6.833333     6.625   \n",
       "1    I wonder why there aren't more people trying t...  5.833333     6.000   \n",
       "2    After reading the article, you can't help but ...  1.000000     1.375   \n",
       "3    It is so sad that someone who had such an amaz...  6.166667     6.625   \n",
       "4    From reading the article, it looks like the wo...  6.833333     1.000   \n",
       "..                                                 ...       ...       ...   \n",
       "787  I feel that this will become a national proble...  6.500000     6.750   \n",
       "788  The whole situation is sketchy. The wavering r...  3.166667     6.375   \n",
       "789  The death of a former aide to Russian Presiden...  6.000000     2.000   \n",
       "790  Everything about Russia really freaks me out. ...  6.000000     6.000   \n",
       "791  Whenever Russia and Putin are involved  I do n...  1.500000     4.875   \n",
       "\n",
       "     speaker_id gender education race age  ... anger_count anticipation_count  \\\n",
       "0            30      1         6    3  37  ...           0                  0   \n",
       "1            19      1         6    2  32  ...           0                  0   \n",
       "2            17      1         6    1  29  ...           0                  0   \n",
       "3            16      2         5    1  28  ...           0                  0   \n",
       "4            30      1         6    3  37  ...           0                  0   \n",
       "..          ...    ...       ...  ...  ..  ...         ...                ...   \n",
       "787          30      1         6    3  37  ...           0                  0   \n",
       "788          16      2         5    1  28  ...           0                  0   \n",
       "789          43      2         6    1  33  ...           0                  0   \n",
       "790          53      2         3    1  27  ...           0                  0   \n",
       "791          30      1         6    3  37  ...           0                  0   \n",
       "\n",
       "    disgust_count fear_count joy_count negative_count positive_count  \\\n",
       "0               0          0         0              0              0   \n",
       "1               0          0         0              0              0   \n",
       "2               0          0         0              0              0   \n",
       "3               0          0         0              0              0   \n",
       "4               0          0         0              0              0   \n",
       "..            ...        ...       ...            ...            ...   \n",
       "787             0          0         0              0              0   \n",
       "788             0          0         0              0              0   \n",
       "789             0          0         0              0              0   \n",
       "790             0          0         0              0              0   \n",
       "791             0          0         0              0              0   \n",
       "\n",
       "    sadness_count surprise_count trust_count  \n",
       "0               0              0           0  \n",
       "1               0              0           0  \n",
       "2               0              0           0  \n",
       "3               0              0           0  \n",
       "4               0              0           0  \n",
       "..            ...            ...         ...  \n",
       "787             0              0           0  \n",
       "788             0              0           0  \n",
       "789             0              0           0  \n",
       "790             0              0           0  \n",
       "791             0              0           0  \n",
       "\n",
       "[792 rows x 34 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for category in lexico_categories:\n",
    "\ttrain_df[f'{category}_count'] = [0 for _ in range(len(train_df))]\n",
    "\n",
    "for index, row in train_df.iterrows():\n",
    "\tpreproc_essay_words = stemming(lemmatize(row['essay']))\n",
    "\tprint(preproc_essay_words)\n",
    "\tbreak\n",
    "\tfor word in preproc_essay_words.split():\n",
    "\t\tif word in lexico['preproc_word'].tolist():\n",
    "\t\t\tfor category in lexico_categories:\n",
    "\t\t\t\ttrain_df.loc[index, f'{category}_count'] += lexico[lexico['preproc_word'] == word][category].tolist()[0]\n",
    "\t\t\t\t#if lexico[lexico['preproc_word'] == word][category].tolist()[0]==1:\n",
    "\t\t\t\t# \tprint(word)\n",
    "\t\t\t\t# \tprint(category)\n",
    "\t\t\t\t# \tprint(\"associate\")\n",
    "\t\t\t\t#train_df[index][f'{category}_count'] += lexico[lexico['preproc_word'] == word][category].tolist()[0]\n",
    "\tbreak\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It breaks my heart to see people living in those conditions. I hope that all the aid that was sent to the island makes it to the people who need it the most. I do not know what I would do it that was my family and I. I would hope that I would do my best, but I can see how depressing and hopeless you could feel having your whole life changed because of a storm and not knowing where your next meal is coming from.'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['essay'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trovato\n"
     ]
    }
   ],
   "source": [
    "# check if the stirng 'hope' is in lexico['preproc_word']\n",
    "if 'hope' in lexico['preproc_word'].to_list():\n",
    "    print(\"trovato\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             idiot\n",
       "1            offend\n",
       "2            strain\n",
       "3            punish\n",
       "4              kick\n",
       "            ...    \n",
       "14149        brigad\n",
       "14150         recur\n",
       "14151    pharmacist\n",
       "14152        caress\n",
       "14153         humor\n",
       "Name: preproc_word, Length: 14153, dtype: object"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexico['preproc_word']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'person'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming(lemmatize('person'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge category_lexico_dfs['anger'] and category_lexico_dfs['disgust'] on 'word' and rename 'associated' to 'anger' and 'disgust'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
