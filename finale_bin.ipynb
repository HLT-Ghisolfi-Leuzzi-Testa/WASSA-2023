{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_B-_QwdhWLe"
      },
      "source": [
        "# WASSA2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:04:39.939508Z",
          "iopub.status.busy": "2023-07-11T19:04:39.938937Z",
          "iopub.status.idle": "2023-07-11T19:04:39.955669Z",
          "shell.execute_reply": "2023-07-11T19:04:39.954582Z",
          "shell.execute_reply.started": "2023-07-11T19:04:39.939467Z"
        },
        "id": "9sDeMksltGSk",
        "outputId": "3c590062-ccac-4b2d-a851-27510f49eb8a",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15KoBLrVhbI8"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FTWi38mQKxUK"
      },
      "outputs": [],
      "source": [
        "repo_path = \"https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/\"\n",
        "branch = \"main\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:04:39.958361Z",
          "iopub.status.busy": "2023-07-11T19:04:39.958022Z",
          "iopub.status.idle": "2023-07-11T19:06:28.298280Z",
          "shell.execute_reply": "2023-07-11T19:06:28.297057Z",
          "shell.execute_reply.started": "2023-07-11T19:04:39.958330Z"
        },
        "id": "1gnWTVNYDMx2",
        "outputId": "a0327264-7d21-4995-a671-b8aac196145e",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.6/157.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.7/135.7 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for NRCLex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n",
            "Finished.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers -q\n",
        "!pip install accelerate -U -q\n",
        "!pip install datasets -q\n",
        "!pip install torch-summary -q\n",
        "!pip install graphviz -q\n",
        "!pip install torchview -q\n",
        "!pip install bertviz -q\n",
        "!pip install NRCLex -q\n",
        "!pip install textblob -q\n",
        "!python -m textblob.download_corpora -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:06:28.300542Z",
          "iopub.status.busy": "2023-07-11T19:06:28.299931Z",
          "iopub.status.idle": "2023-07-11T19:06:30.619028Z",
          "shell.execute_reply": "2023-07-11T19:06:30.617896Z",
          "shell.execute_reply.started": "2023-07-11T19:06:28.300504Z"
        },
        "id": "9Xbo-Ei3IAYB",
        "outputId": "bf819bc6-0d3c-48a2-d836-a601c51d050c",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-18 09:52:44--  https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38053 (37K) [text/plain]\n",
            "Saving to: ‘utils.py’\n",
            "\n",
            "\rutils.py              0%[                    ]       0  --.-KB/s               \rutils.py            100%[===================>]  37.16K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-07-18 09:52:44 (26.3 MB/s) - ‘utils.py’ saved [38053/38053]\n",
            "\n",
            "--2023-07-18 09:52:44--  https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/evaluation.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10675 (10K) [text/plain]\n",
            "Saving to: ‘evaluation.py’\n",
            "\n",
            "evaluation.py       100%[===================>]  10.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-18 09:52:45 (84.0 MB/s) - ‘evaluation.py’ saved [10675/10675]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "utils_url = f\"{repo_path}{branch}/utils.py\"\n",
        "evaluation_url = f\"{repo_path}{branch}/evaluation.py\"\n",
        "\n",
        "import os\n",
        "if os.path.exists(\"utils.py\"):\n",
        "  !rm \"utils.py\"\n",
        "if os.path.exists(\"evaluation.py\"):\n",
        "  !rm \"evaluation.py\"\n",
        "\n",
        "!wget {utils_url}\n",
        "!wget {evaluation_url}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:06:30.622919Z",
          "iopub.status.busy": "2023-07-11T19:06:30.622582Z",
          "iopub.status.idle": "2023-07-11T19:06:38.755450Z",
          "shell.execute_reply": "2023-07-11T19:06:38.754266Z",
          "shell.execute_reply.started": "2023-07-11T19:06:30.622890Z"
        },
        "id": "kEVk5afqZjf1",
        "outputId": "38e87838-7f42-4ffa-8c77-f0428fb41e21",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-18 09:53:06--  https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/datasets/EMO23_lexicon_per_word_.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14536741 (14M) [text/plain]\n",
            "Saving to: ‘EMO23_lexicon_per_word_.json.1’\n",
            "\n",
            "EMO23_lexicon_per_w 100%[===================>]  13.86M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-07-18 09:53:06 (289 MB/s) - ‘EMO23_lexicon_per_word_.json.1’ saved [14536741/14536741]\n",
            "\n",
            "--2023-07-18 09:53:07--  https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/datasets/EMP23_lexicon_per_word_.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8203800 (7.8M) [text/plain]\n",
            "Saving to: ‘EMP23_lexicon_per_word_.json.1’\n",
            "\n",
            "EMP23_lexicon_per_w 100%[===================>]   7.82M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-07-18 09:53:07 (207 MB/s) - ‘EMP23_lexicon_per_word_.json.1’ saved [8203800/8203800]\n",
            "\n",
            "--2023-07-18 09:53:07--  https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/datasets/EMO23_lexicon_per_word_test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 651198 (636K) [text/plain]\n",
            "Saving to: ‘EMO23_lexicon_per_word_test.json.1’\n",
            "\n",
            "EMO23_lexicon_per_w 100%[===================>] 635.94K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-07-18 09:53:07 (29.9 MB/s) - ‘EMO23_lexicon_per_word_test.json.1’ saved [651198/651198]\n",
            "\n",
            "--2023-07-18 09:53:07--  https://raw.githubusercontent.com/HLT-Ghisolfi-Leuzzi-Testa/WASSA-2023/main/datasets/EMP23_lexicon_per_word_test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 362281 (354K) [text/plain]\n",
            "Saving to: ‘EMP23_lexicon_per_word_test.json.1’\n",
            "\n",
            "EMP23_lexicon_per_w 100%[===================>] 353.79K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-07-18 09:53:07 (28.3 MB/s) - ‘EMP23_lexicon_per_word_test.json.1’ saved [362281/362281]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EMO_json_path_train = f\"{repo_path}{branch}/datasets/EMO23_lexicon_per_word_.json\"\n",
        "EMP_json_path_train = f\"{repo_path}{branch}/datasets/EMP23_lexicon_per_word_.json\"\n",
        "EMO_json_path_test = f\"{repo_path}{branch}/datasets/EMO23_lexicon_per_word_test.json\"\n",
        "EMP_json_path_test = f\"{repo_path}{branch}/datasets/EMP23_lexicon_per_word_test.json\"\n",
        "\n",
        "!wget {EMO_json_path_train}\n",
        "!wget {EMP_json_path_train}\n",
        "!wget {EMO_json_path_test}\n",
        "!wget {EMP_json_path_test}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:06:38.757963Z",
          "iopub.status.busy": "2023-07-11T19:06:38.757555Z",
          "iopub.status.idle": "2023-07-11T19:06:52.358142Z",
          "shell.execute_reply": "2023-07-11T19:06:52.357177Z",
          "shell.execute_reply.started": "2023-07-11T19:06:38.757924Z"
        },
        "id": "XDmQslVxDhy1",
        "outputId": "45e63cec-3a89-495f-e865-5c2cc12190ac",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'utils' from '/content/utils.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "import string\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
        "from transformers import TrainingArguments, Trainer, TrainerCallback, EarlyStoppingCallback\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "import torch\n",
        "from torch import nn\n",
        "from transformers import AutoConfig, BertForSequenceClassification, RobertaForSequenceClassification, RobertaModel\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from typing import Optional, Union, Tuple\n",
        "from nrclex import NRCLex\n",
        "import importlib\n",
        "import sys\n",
        "from torch.utils.data import Dataset\n",
        "from utils import *\n",
        "importlib.reload(sys.modules['utils'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:06:52.360520Z",
          "iopub.status.busy": "2023-07-11T19:06:52.359527Z",
          "iopub.status.idle": "2023-07-11T19:06:52.392731Z",
          "shell.execute_reply": "2023-07-11T19:06:52.391807Z",
          "shell.execute_reply.started": "2023-07-11T19:06:52.360482Z"
        },
        "id": "quom7lWCDiiI",
        "outputId": "9612c0fe-bca0-4b59-8599-9902f7fbe126",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======= CUDA Available =======\n"
          ]
        }
      ],
      "source": [
        "# set CUDA if available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print(\"======= CUDA Available =======\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"======= CUDA NOT Available, run on CPU =======\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLrhz2zGdhJT"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:06:52.394814Z",
          "iopub.status.busy": "2023-07-11T19:06:52.394204Z",
          "iopub.status.idle": "2023-07-11T19:06:52.406593Z",
          "shell.execute_reply": "2023-07-11T19:06:52.405612Z",
          "shell.execute_reply.started": "2023-07-11T19:06:52.394779Z"
        },
        "id": "S2o5IfHdgXXf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "EMOTIONS_NAMES = [\n",
        "    'fear',\n",
        "    'anger',\n",
        "    'anticipation',\n",
        "    'trust',\n",
        "    'surprise',\n",
        "    'positive',\n",
        "    'negative',\n",
        "    'sadness',\n",
        "    'disgust',\n",
        "    'joy',\n",
        "    'hope'\n",
        "]\n",
        "\n",
        "config = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "metadata": {
        "id": "GMoRVEFkQIPx"
      },
      "outputs": [],
      "source": [
        "#@title Global Features\n",
        "TASK = \"EMO\" #@param [\"EMO\", \"EMP\"]\n",
        "binary_EMO = True #@param {type:\"boolean\"}\n",
        "EMO_binary_emotion = \"fear\" #@param [\"fear\", \"anger\", \"surprise\", \"sadness\", \"disgust\", \"joy\", \"hope\", \"neutral\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:10:06.052235Z",
          "iopub.status.busy": "2023-07-11T19:10:06.051958Z",
          "iopub.status.idle": "2023-07-11T19:10:06.059593Z",
          "shell.execute_reply": "2023-07-11T19:10:06.058689Z",
          "shell.execute_reply.started": "2023-07-11T19:10:06.052213Z"
        },
        "id": "7JyTdDiw9TWP",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Number of labels\n",
        "num_labels = 8 #@param {type:\"integer\"}\n",
        "config['num_labels'] = num_labels\n",
        "if binary_EMO:\n",
        "  config['num_labels'] = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "metadata": {
        "cellView": "form",
        "id": "g78XkRZL6xHY"
      },
      "outputs": [],
      "source": [
        "#@title Model Class\n",
        "model_id = 'distil-1cls' #@param {type:\"string\"}\n",
        "model_class_string = \"RobertaPreTrainedModel\" #@param [\"BertPreTrainedModel\", \"RobertaPreTrainedModel\"]\n",
        "model_name = 'distilroberta-emotion' #@param [\"bert-base\", \"roberta-base\", \"roberta-emotion\", \"distilroberta-emotion\", \"roberta-empathy\"]\n",
        "\n",
        "if model_class_string == \"BertPreTrainedModel\":\n",
        "  model_class = BertForSequenceClassification\n",
        "else:\n",
        "  model_class = RobertaForSequenceClassification\n",
        "\n",
        "config['model_id'] = model_id\n",
        "config['model_class_string'] = model_class_string\n",
        "config['model_name'] = model_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:10:06.026995Z",
          "iopub.status.busy": "2023-07-11T19:10:06.026313Z",
          "iopub.status.idle": "2023-07-11T19:10:06.034340Z",
          "shell.execute_reply": "2023-07-11T19:10:06.033205Z",
          "shell.execute_reply.started": "2023-07-11T19:10:06.026970Z"
        },
        "id": "j6y4HnEP6gcI",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Global Features\n",
        "emo_count_global = False #@param {type:\"boolean\"}\n",
        "empathy_count_global = False #@param {type:\"boolean\"}\n",
        "distress_count_global = False #@param {type:\"boolean\"}\n",
        "bio_global = True #@param {type:\"boolean\"}\n",
        "gold_emotions = False #@param {type:\"boolean\"}\n",
        "gold_empathy = False #@param {type:\"boolean\"}\n",
        "gold_distress = False #@param {type:\"boolean\"}\n",
        "\n",
        "if TASK == \"EMO\":\n",
        "  gold_emotions = False\n",
        "if TASK ==\"EMP\":\n",
        "  gold_distress = False\n",
        "  gold_empathy = False\n",
        "\n",
        "global_features_names = []\n",
        "emotions_count = ['fear_count',\t'anger_count','anticipation_count',\t'trust_count',\n",
        "                 'positive_count',\t'negative_count', 'surprise_count',\n",
        "                 'sadness_count',\t'disgust_count',\t'joy_count',\t'hope_count']\n",
        "bio = ['gender', 'age', 'income', 'race', 'education']\n",
        "if emo_count_global:\n",
        "  for emo in emotions_count: global_features_names.append(emo)\n",
        "if empathy_count_global: global_features_names.append('empathy_count')\n",
        "if distress_count_global: global_features_names.append('distress_count')\n",
        "if bio_global:\n",
        "  for b in bio: global_features_names.append(b)\n",
        "\n",
        "if gold_empathy: global_features_names.append('empathy')\n",
        "if gold_distress: global_features_names.append('distress')\n",
        "\n",
        "config['emo_count_global'] = emo_count_global\n",
        "config['empathy_count_global'] = empathy_count_global\n",
        "config['distress_count_global'] = distress_count_global\n",
        "config['bio_global'] = bio_global\n",
        "config['gold_emotions'] = gold_emotions\n",
        "config['gold_empathy'] = gold_empathy\n",
        "config['gold_distress'] = gold_distress\n",
        "config['global_features_names'] = global_features_names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "metadata": {
        "cellView": "form",
        "execution": {
          "iopub.execute_input": "2023-07-11T19:10:06.036262Z",
          "iopub.status.busy": "2023-07-11T19:10:06.035925Z",
          "iopub.status.idle": "2023-07-11T19:10:06.048932Z",
          "shell.execute_reply": "2023-07-11T19:10:06.048042Z",
          "shell.execute_reply.started": "2023-07-11T19:10:06.036232Z"
        },
        "id": "n3snAqfK6728",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#@title Local Features\n",
        "emo_count_local = False #@param {type:\"boolean\"}\n",
        "empathy_count_local = False #@param {type:\"boolean\"}\n",
        "distress_count_local = False #@param {type:\"boolean\"}\n",
        "\n",
        "local_features_names = []\n",
        "num_local_features = 0\n",
        "if emo_count_local:\n",
        "  local_features_names.append('emotions')\n",
        "  num_local_features += len(EMOTIONS_NAMES)\n",
        "if empathy_count_local:\n",
        "  local_features_names.append('empathy')\n",
        "  num_local_features += 1\n",
        "if distress_count_local:\n",
        "  local_features_names.append('distress')\n",
        "  num_local_features += 1\n",
        "\n",
        "if len(local_features_names) == 0: local_features_names = None\n",
        "\n",
        "config['emo_count_local'] = emo_count_local\n",
        "config['empathy_count_local'] = empathy_count_local\n",
        "config['distress_count_local'] = distress_count_local\n",
        "config['local_features_names'] = local_features_names\n",
        "\n",
        "dim_extra_features = 0\n",
        "if gold_emotions:\n",
        "  dim_extra_features = len(global_features_names) + 8\n",
        "else:\n",
        "  dim_extra_features = len(global_features_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "metadata": {
        "cellView": "form",
        "id": "EaTQUHDY6xHZ"
      },
      "outputs": [],
      "source": [
        "#@title Prompt\n",
        "bio_prompt = True #@param {type:\"boolean\"}\n",
        "bio_sep = 'after SEP' #@param [\"after SEP\", \"before SEP\"]\n",
        "emo_prompt = False #@param {type:\"boolean\"}\n",
        "emo_sep = 'before SEP' #@param [\"after SEP\", \"before SEP\"]\n",
        "empathy_prompt = False #@param {type:\"boolean\"}\n",
        "emp_sep = 'before SEP' #@param [\"after SEP\", \"before SEP\"]\n",
        "\n",
        "prompt_names_before_SEP = []\n",
        "prompt_names_after_SEP = []\n",
        "if bio_prompt:\n",
        "    if bio_sep=='before SEP':\n",
        "        prompt_names_before_SEP.append('prompt_bio')\n",
        "    else:\n",
        "        prompt_names_after_SEP.append('prompt_bio')\n",
        "if empathy_prompt:\n",
        "    if emp_sep=='before SEP':\n",
        "        prompt_names_before_SEP.append('prompt_emp')\n",
        "    else:\n",
        "        prompt_names_after_SEP.append('prompt_emp')\n",
        "if emo_prompt:\n",
        "    if emo_sep=='before SEP':\n",
        "        prompt_names_before_SEP.append('prompt_emo')\n",
        "    else:\n",
        "        prompt_names_after_SEP.append('prompt_emo')\n",
        "\n",
        "config['bio_prompt'] = bio_prompt\n",
        "config['emo_prompt'] = emo_prompt\n",
        "config['empathy_prompt'] = empathy_prompt\n",
        "config['prompt_names_before_SEP'] = prompt_names_before_SEP\n",
        "config['prompt_names_after_SEP'] = prompt_names_after_SEP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "metadata": {
        "cellView": "form",
        "id": "V361RpgifLr-"
      },
      "outputs": [],
      "source": [
        "#@title Last CLSs\n",
        "n_last_cls = 4 #@param {type:\"integer\"}\n",
        "mean_last_cls = False #@param {type: \"boolean\"}\n",
        "concat_local_features = True #@param {type: \"boolean\"}\n",
        "config['n_last_cls'] = n_last_cls\n",
        "config['mean_last_cls'] = mean_last_cls\n",
        "config['concat_local_features'] = concat_local_features\n",
        "\n",
        "if concat_local_features:\n",
        "  dim_extra_features += num_local_features\n",
        "\n",
        "config['dim_extra_features'] = dim_extra_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgMIYk-HyEbS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "metadata": {
        "cellView": "form",
        "id": "SwMOxxRRg0Vh"
      },
      "outputs": [],
      "source": [
        "#@title Model configuration\n",
        "train_batch_size = 8 #@param {type:\"integer\"}\n",
        "val_batch_size = 8 #@param {type:\"integer\"}\n",
        "learning_rate = 5e-5 #@param {type:\"number\"}\n",
        "weight_decay = 0.08 #@param {type:\"number\"}\n",
        "epochs = 30 #@param {type:\"integer\"}\n",
        "patience = 5 #@param {type:\"integer\"}\n",
        "dropout = 0.3 #@param {type:\"number\"}\n",
        "weighted_loss = \"None\" #@param [\"None\", \"normalized_inverse\", \"balanced\"]\n",
        "\n",
        "model_type ={\n",
        "  'distilroberta-emotion':'j-hartmann/emotion-english-distilroberta-base',\n",
        "  'roberta-emotion':'j-hartmann/emotion-english-roberta-large',\n",
        "  'roberta-empathy': 'bdotloh/roberta-base-empathy',\n",
        "  'bert-base': 'bert-base-cased',\n",
        "  'roberta-base' : 'roberta-base'\n",
        "}\n",
        "\n",
        "model_config = {\n",
        "    'model_id': model_id,\n",
        "    'tokenizer_name': model_type.get(model_name),\n",
        "    'model_name': model_type.get(model_name),\n",
        "    'train_batch_size': train_batch_size,\n",
        "    'val_batch_size': val_batch_size,\n",
        "    'learning_rate': learning_rate,\n",
        "    'weight_decay': weight_decay,\n",
        "    'epochs': epochs,\n",
        "    'seed': 42,\n",
        "    'patience': patience,\n",
        "    'early_stopping_threshold': 0,\n",
        "    'weighted_loss': weighted_loss,\n",
        "    'dropout' : dropout\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:10:06.061536Z",
          "iopub.status.busy": "2023-07-11T19:10:06.061089Z",
          "iopub.status.idle": "2023-07-11T19:10:06.069916Z",
          "shell.execute_reply": "2023-07-11T19:10:06.069051Z",
          "shell.execute_reply.started": "2023-07-11T19:10:06.061505Z"
        },
        "id": "dA224FxADpqd",
        "outputId": "caa5556b-cbfa-477d-a453-b404e7a051a3",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘EMO_distil-1cls’: File exists\n",
            "\n",
            "CONFIGURATION\n",
            "num_labels: 2\n",
            "model_id: distil-1cls\n",
            "model_class_string: RobertaPreTrainedModel\n",
            "model_name: j-hartmann/emotion-english-distilroberta-base\n",
            "emo_count_global: False\n",
            "empathy_count_global: False\n",
            "distress_count_global: False\n",
            "bio_global: True\n",
            "gold_emotions: False\n",
            "gold_empathy: False\n",
            "gold_distress: False\n",
            "global_features_names: ['gender', 'age', 'income', 'race', 'education']\n",
            "emo_count_local: False\n",
            "empathy_count_local: False\n",
            "distress_count_local: False\n",
            "local_features_names: None\n",
            "bio_prompt: True\n",
            "emo_prompt: False\n",
            "empathy_prompt: False\n",
            "prompt_names_before_SEP: []\n",
            "prompt_names_after_SEP: ['prompt_bio']\n",
            "n_last_cls: 4\n",
            "mean_last_cls: False\n",
            "concat_local_features: True\n",
            "dim_extra_features: 5\n",
            "tokenizer_name: j-hartmann/emotion-english-distilroberta-base\n",
            "train_batch_size: 8\n",
            "val_batch_size: 8\n",
            "learning_rate: 5e-05\n",
            "weight_decay: 0.08\n",
            "epochs: 30\n",
            "seed: 42\n",
            "patience: 5\n",
            "early_stopping_threshold: 0\n",
            "weighted_loss: None\n",
            "dropout: 0.3\n"
          ]
        }
      ],
      "source": [
        "path_tosave = f\"{TASK}_{model_config['model_id']}\"\n",
        "!mkdir $path_tosave\n",
        "\n",
        "config.update(model_config)\n",
        "write_dict_to_json(config, f\"{path_tosave}/config.json\")\n",
        "\n",
        "print(\"\\nCONFIGURATION\")\n",
        "for k,v in config.items():\n",
        "  print(f\"{k}: {v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gsXzUtCBv-j"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIUOnGp-_6lV"
      },
      "source": [
        "### WASSA dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:06:52.410007Z",
          "iopub.status.busy": "2023-07-11T19:06:52.409372Z",
          "iopub.status.idle": "2023-07-11T19:06:52.429476Z",
          "shell.execute_reply": "2023-07-11T19:06:52.428716Z",
          "shell.execute_reply.started": "2023-07-11T19:06:52.409972Z"
        },
        "id": "WOuBgl-TmlM-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class WASSADataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        essay,\n",
        "        essay_id,\n",
        "        targets,\n",
        "        prompt_before_SEP=None,\n",
        "        prompt_after_SEP=None,\n",
        "        EMP_lexicon = None,\n",
        "        EMO_lexicon = None,\n",
        "        global_features = None,\n",
        "        local_emotions = False,\n",
        "        local_empathy = False,\n",
        "        local_distress = False,\n",
        "        max_len=None\n",
        "        ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.essay = essay\n",
        "        self.essay_id = essay_id\n",
        "        self.targets = targets\n",
        "        self.EMP_lexicon = EMP_lexicon\n",
        "        self.EMO_lexicon = EMO_lexicon\n",
        "\n",
        "        self.prompt_before_SEP = prompt_before_SEP\n",
        "        self.prompt_after_SEP = prompt_after_SEP\n",
        "\n",
        "        self.global_features = global_features\n",
        "        self.local_emotions = local_emotions\n",
        "        self.local_empathy = local_empathy\n",
        "        self.local_distress = local_distress\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        essay = str(self.essay[index])\n",
        "\n",
        "        essay_id = self.essay_id[index]\n",
        "\n",
        "        prompt_before_SEP = \"\"\n",
        "        if self.prompt_before_SEP is not None:\n",
        "          for p in self.prompt_before_SEP[index]:\n",
        "            prompt_before_SEP += \" \" + str(p)\n",
        "\n",
        "        prompt_after_SEP = \"\"\n",
        "        if self.prompt_after_SEP is not None:\n",
        "          for p in self.prompt_after_SEP[index]:\n",
        "            prompt_after_SEP += \" \" + str(p)\n",
        "\n",
        "        text = essay\n",
        "        text_pair = None\n",
        "        if prompt_before_SEP != \"\":\n",
        "          text += str(prompt_before_SEP)\n",
        "        if prompt_after_SEP != \"\":\n",
        "          text_pair = str(prompt_after_SEP)\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text=essay,\n",
        "            text_pair=text_pair,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "          'input_ids': inputs['input_ids'].flatten(),\n",
        "          'attention_mask': inputs['attention_mask'].flatten(),\n",
        "          'token_type_ids': inputs[\"token_type_ids\"].flatten()\n",
        "        }\n",
        "\n",
        "        if self.targets is not None:\n",
        "          item['labels'] = torch.FloatTensor(self.targets[index])\n",
        "\n",
        "        if self.global_features is not None:\n",
        "          item['global_features'] = self.global_features[index]\n",
        "\n",
        "        n_local_features = 0\n",
        "        features_tokens_row = []\n",
        "        if self.local_emotions:\n",
        "            n_local_features += len(EMOTIONS_NAMES)\n",
        "            for i in range(len(EMOTIONS_NAMES)):\n",
        "                features_tokens_row.append(0)\n",
        "        if self.local_empathy:\n",
        "            n_local_features += 1\n",
        "            features_tokens_row.append(4)\n",
        "        if self.local_distress:\n",
        "            n_local_features += 1\n",
        "            features_tokens_row.append(0)\n",
        "\n",
        "        if n_local_features > 0:\n",
        "            features_tokens = np.full((self.tokenizer.model_max_length, n_local_features), features_tokens_row)\n",
        "        else:\n",
        "          features_tokens = None\n",
        "\n",
        "        word_count=0\n",
        "        first_char=True\n",
        "        last_char_is_space=False\n",
        "        for char_idx, char in enumerate(essay):\n",
        "          token_idx = inputs.char_to_token(char_idx)\n",
        "          if token_idx is None:\n",
        "            if first_char: last_char_is_space=True\n",
        "            if not last_char_is_space and not first_char:\n",
        "              word_count+=1\n",
        "              last_char_is_space=True\n",
        "            continue\n",
        "          elif last_char_is_space:\n",
        "            last_char_is_space=False\n",
        "          first_char=False\n",
        "\n",
        "          j = 0\n",
        "          if char not in string.punctuation:\n",
        "            if self.local_emotions:\n",
        "              for i, emo in enumerate(EMOTIONS_NAMES):\n",
        "                features_tokens[token_idx][i] = self.EMO_lexicon[str(essay_id)][emo][word_count]\n",
        "              j += len(EMOTIONS_NAMES)\n",
        "\n",
        "            if self.local_empathy:\n",
        "              features_tokens[token_idx][j] = self.EMP_lexicon[str(essay_id)]['empathy'][word_count]\n",
        "              j += 1\n",
        "\n",
        "\n",
        "            if self.local_distress:\n",
        "              features_tokens[token_idx][j] = self.EMP_lexicon[str(essay_id)]['distress'][word_count]\n",
        "\n",
        "        if features_tokens is not None:\n",
        "            item['local_features'] = torch.FloatTensor(features_tokens)\n",
        "\n",
        "        #item['return_dict']=True\n",
        "\n",
        "        return item"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFI6AulQsYci"
      },
      "source": [
        "Read dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:09:57.899510Z",
          "iopub.status.busy": "2023-07-11T19:09:57.899102Z",
          "iopub.status.idle": "2023-07-11T19:09:57.904814Z",
          "shell.execute_reply": "2023-07-11T19:09:57.903813Z",
          "shell.execute_reply.started": "2023-07-11T19:09:57.899476Z"
        },
        "id": "BjM03TscDwcz",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\"\"\"TRAIN_DATA = f\"{repo_path}{branch}/datasets/WASSA_essay_level_internal_train_preproc_upsampled.tsv\"\n",
        "VAL_DATA = f\"{repo_path}{branch}/datasets/WASSA_essay_level_internal_val_preproc.tsv\"\n",
        "DEV_DATA = f\"{repo_path}{branch}/datasets/WASSA_essay_level_dev_preproc.tsv\"\n",
        "\"\"\"\n",
        "if binary_EMO:\n",
        "    TRAIN_DATA = f\"{repo_path}{branch}/dataset_split/WASSA23_essay_level_train_preproc_downsampled_{EMO_binary_emotion}.tsv\"\n",
        "    all_train_df = pd.read_csv(TRAIN_DATA, sep='\\t')\n",
        "    train_df, val_df = split_train_val(all_train_df, val_size=0.1)\n",
        "else:\n",
        "    TRAIN_DATA = f\"{repo_path}{branch}/datasets/WASSA23_essay_level_internal_train_preproc.tsv\"\n",
        "    VAL_DATA = f\"{repo_path}{branch}/datasets/WASSA23_essay_level_internal_val_preproc.tsv\"\n",
        "    train_df = pd.read_csv(TRAIN_DATA, sep='\\t')\n",
        "    val_df = pd.read_csv(VAL_DATA, sep='\\t')\n",
        "\n",
        "DEV_DATA = f\"{repo_path}{branch}/datasets/WASSA23_essay_level_dev_preproc.tsv\"\n",
        "dev_df = pd.read_csv(DEV_DATA, sep='\\t')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDJWlHhwsfwL"
      },
      "source": [
        "Encode targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:10:06.006808Z",
          "iopub.status.busy": "2023-07-11T19:10:06.006358Z",
          "iopub.status.idle": "2023-07-11T19:10:06.021824Z",
          "shell.execute_reply": "2023-07-11T19:10:06.020868Z",
          "shell.execute_reply.started": "2023-07-11T19:10:06.006776Z"
        },
        "id": "6LKqRnKdsJdu",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if TASK ==\"EMO\":\n",
        "  label_encoder = EmotionsLabelEncoder()\n",
        "  label_encoder.fit(train_df.emotion)\n",
        "  y_train = label_encoder.encode(train_df.emotion)\n",
        "  y_val = label_encoder.encode(val_df.emotion)\n",
        "  y_dev = label_encoder.encode(dev_df.emotion)\n",
        "\n",
        "if TASK == \"EMP\":\n",
        "  y_train = np.array(train_df[['empathy', 'distress']])\n",
        "  y_val = np.array(val_df[['empathy', 'distress']])\n",
        "  y_dev = np.array(dev_df[['empathy', 'distress']])\n",
        "\n",
        "if binary_EMO:\n",
        "  y_train = np.array(train_df.emotion_dataset)#.reshape(-1,1)\n",
        "  y_val = np.array(val_df.emotion_dataset)#.reshape(-1,1)\n",
        "  y_dev = []\n",
        "  for emo in dev_df['emotion']:\n",
        "    if EMO_binary_emotion in emo:\n",
        "      y_dev.append(1)\n",
        "    else:\n",
        "      y_dev.append(0)\n",
        "  y_dev = np.array(y_dev)#.reshape(-1,1)\n",
        "  \"\"\"y_train = np.array(train_df.emotion_dataset).reshape(-1, 1)\n",
        "  y_train_encoded = np.zeros((y_train.shape[0], 2))\n",
        "  for i in range(y_train.shape[0]):\n",
        "    if y_train[i]==0:\n",
        "      y_train_encoded[i][0] = 1\n",
        "      y_train_encoded[i][1] = 0\n",
        "    else:\n",
        "      y_train_encoded[i][0] = 0\n",
        "      y_train_encoded[i][1] = 1\n",
        "  y_train = y_train_encoded\n",
        "\n",
        "  y_val = np.array(val_df.emotion_dataset).reshape(-1, 1)\n",
        "  y_val_encoded = np.zeros((y_val.shape[0], 2))\n",
        "  for i in range(y_val.shape[0]):\n",
        "    if y_val[i]==0:\n",
        "      y_val_encoded[i][0] = 1\n",
        "      y_val_encoded[i][1] = 0\n",
        "    else:\n",
        "      y_val_encoded[i][0] = 0\n",
        "      y_val_encoded[i][1] = 1\n",
        "  y_val = y_val_encoded\n",
        "\n",
        "  y_dev_encoded = []\n",
        "  for emo in dev_df['emotion']:\n",
        "    if EMO_binary_emotion in emo:\n",
        "      y_dev_encoded.append([0.0,1.0]) # 1->0,1\n",
        "    else:\n",
        "      y_dev_encoded.append([1.0,0.0]) # 0->1,0\n",
        "  y_dev_encoded = np.array(y_dev_encoded)\n",
        "  y_dev = y_dev_encoded\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbCKVWh7-gpg"
      },
      "source": [
        "Extra global features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "metadata": {
        "id": "7VpOAsXL6xHa"
      },
      "outputs": [],
      "source": [
        "global_features_train = None\n",
        "global_features_val = None\n",
        "global_features_dev = None\n",
        "\n",
        "if len(config.get('global_features_names')) > 0:\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(np.array(train_df[config.get('global_features_names')]))\n",
        "\n",
        "  def standard_scalar_features(features):\n",
        "    return scaler.transform(features)\n",
        "\n",
        "\n",
        "  global_features_train =  standard_scalar_features(np.array(train_df[config.get('global_features_names')]))\n",
        "  global_features_val =  standard_scalar_features(np.array(val_df[config.get('global_features_names')]))\n",
        "  global_features_dev =  standard_scalar_features(np.array(dev_df[config.get('global_features_names')]))\n",
        "\n",
        "  if TASK == \"EMP\":\n",
        "    label_encoder = EmotionsLabelEncoder()\n",
        "    label_encoder.fit(train_df.emotion)\n",
        "    gold_emotions_train = label_encoder.encode(train_df.emotion)\n",
        "    gold_emotions_val = label_encoder.encode(val_df.emotion)\n",
        "    gold_emotions_dev = label_encoder.encode(dev_df.emotion)\n",
        "    global_features_train = np.concatenate((global_features_train, gold_emotions_train), axis = 1)\n",
        "    global_features_val = np.concatenate((global_features_val, gold_emotions_val), axis = 1)\n",
        "    global_features_dev = np.concatenate((global_features_dev, gold_emotions_dev), axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfm0n2Mk6xHb"
      },
      "source": [
        "Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "metadata": {
        "id": "5WnGSQwv6xHb"
      },
      "outputs": [],
      "source": [
        "prompt_before_SEP_train = None\n",
        "prompt_after_SEP_train = None\n",
        "prompt_before_SEP_val = None\n",
        "prompt_after_SEP_val = None\n",
        "prompt_before_SEP_dev = None\n",
        "prompt_after_SEP_dev = None\n",
        "\n",
        "if len(config.get('prompt_names_before_SEP')) > 0:\n",
        "  prompt_before_SEP_train = np.array(train_df[config.get('prompt_names_before_SEP')])\n",
        "  prompt_before_SEP_val = np.array(val_df[config.get('prompt_names_before_SEP')])\n",
        "  prompt_before_SEP_dev = np.array(dev_df[config.get('prompt_names_before_SEP')])\n",
        "\n",
        "if len(config.get('prompt_names_after_SEP')) > 0:\n",
        "  prompt_after_SEP_train = np.array(train_df[config.get('prompt_names_after_SEP')])\n",
        "  prompt_after_SEP_val = np.array(val_df[config.get('prompt_names_after_SEP')])\n",
        "  prompt_after_SEP_dev = np.array(dev_df[config.get('prompt_names_after_SEP')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwYXJId_-Sb5"
      },
      "source": [
        "Lexicons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:09.185627Z",
          "iopub.status.busy": "2023-07-11T19:11:09.185261Z",
          "iopub.status.idle": "2023-07-11T19:11:09.667265Z",
          "shell.execute_reply": "2023-07-11T19:11:09.666254Z",
          "shell.execute_reply.started": "2023-07-11T19:11:09.185595Z"
        },
        "id": "Qg0D38kk-Rrv",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "EMO_lexicon_train_dict = None\n",
        "EMP_lexicon_train_dict = None\n",
        "EMO_lexicon_dev_dict = None\n",
        "EMP_lexicon_dev_dict = None\n",
        "EMO_lexicon_test_dict = None\n",
        "EMP_lexicon_test_dict = None\n",
        "if config.get('local_features_names') is not None:\n",
        "  if 'emotions' in config.get('local_features_names'):\n",
        "    with open(\"/content/EMO23_lexicon_per_word_.json\") as json_file:\n",
        "      EMO_lexicon_train_dict = json.load(json_file)\n",
        "    with open(\"/content/EMO23_lexicon_per_word_test.json\") as json_file:\n",
        "      EMO_lexicon_test_dict = json.load(json_file)\n",
        "\n",
        "  if 'empathy' in config.get('local_features_names') or 'distress' in config.get('local_features_names'):\n",
        "    with open(\"/content/EMP23_lexicon_per_word_.json\") as json_file:\n",
        "      EMP_lexicon_train_dict = json.load(json_file)\n",
        "    with open(\"/content/EMP23_lexicon_per_word_test.json\") as json_file:\n",
        "      EMP_lexicon_test_dict = json.load(json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXIimY5ABmlc"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:17.243014Z",
          "iopub.status.busy": "2023-07-11T19:11:17.241592Z",
          "iopub.status.idle": "2023-07-11T19:11:17.253391Z",
          "shell.execute_reply": "2023-07-11T19:11:17.252383Z",
          "shell.execute_reply.started": "2023-07-11T19:11:17.242979Z"
        },
        "id": "hfwVaaj-0qDq",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def get_loss_weights(y, method):\n",
        "  if method == 'balanced':\n",
        "    weights_train = y.shape[0] / (y.shape[1] * np.sum(y, axis=0))\n",
        "  else:\n",
        "    inverse_n_samples = 1 / np.sum(y, axis=0)\n",
        "    sum_inverses = sum(inverse_n_samples)\n",
        "    weights_train = inverse_n_samples / sum_inverses\n",
        "  return torch.cuda.FloatTensor(weights_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:17.257384Z",
          "iopub.status.busy": "2023-07-11T19:11:17.255861Z",
          "iopub.status.idle": "2023-07-11T19:11:22.360433Z",
          "shell.execute_reply": "2023-07-11T19:11:22.359379Z",
          "shell.execute_reply.started": "2023-07-11T19:11:17.257351Z"
        },
        "id": "wVh3_jCFzmK7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "loss_weights_train = None\n",
        "if model_config.get('weighted_loss')!='None':\n",
        "  loss_weights_train = get_loss_weights(y_train, model_config.get('weighted_loss'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6aAhPift1UL"
      },
      "source": [
        "### Custom model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:23.702374Z",
          "iopub.status.busy": "2023-07-11T19:11:23.702059Z",
          "iopub.status.idle": "2023-07-11T19:11:23.732141Z",
          "shell.execute_reply": "2023-07-11T19:11:23.730692Z",
          "shell.execute_reply.started": "2023-07-11T19:11:23.702348Z"
        },
        "id": "2Vj3W-oUhBMZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "    #Head for sentence-level classification tasks.\n",
        "\n",
        "    def __init__(self, config, dim_extra_features, hidden_layers_to_concat, classifier_dropout, local_features, mean_last_cls):\n",
        "        super().__init__()\n",
        "        self.local_features = local_features\n",
        "\n",
        "        if mean_last_cls:\n",
        "          total_dims = config.hidden_size + dim_extra_features\n",
        "        else:\n",
        "          total_dims = config.hidden_size*hidden_layers_to_concat + dim_extra_features\n",
        "        if self.local_features:\n",
        "          total_dims += config.hidden_size\n",
        "        self.dense = nn.Linear(total_dims, total_dims)\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.out_proj = nn.Linear(total_dims, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        features = features.to(torch.float32) # by default float32 is used as the dtype\n",
        "        x = self.dense(features)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "class CustomSequenceClassification(model_class):\n",
        "\n",
        "    def __init__(self, config, dim_extra_features=0, model_class = None, local_features_names=None, loss_weights=None, n_last_cls = 1, mean_last_cls = False, concat_local_features = False):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "        self.local_features_names = local_features_names\n",
        "        self.loss_weights = loss_weights\n",
        "        self.n_last_cls = n_last_cls\n",
        "        self.mean_last_cls = mean_last_cls\n",
        "        self.concat_local_features = concat_local_features\n",
        "        self.model_class = model_class\n",
        "        if self.model_class == \"BertPreTrainedModel\":\n",
        "          self.bert = BertModel(config)\n",
        "\n",
        "        if self.model_class == \"RobertaPreTrainedModel\":\n",
        "          self.roberta = RobertaModel(config)\n",
        "\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = ClassificationHead(config,\n",
        "                                            dim_extra_features,\n",
        "                                            n_last_cls,\n",
        "                                            classifier_dropout,\n",
        "                                            local_features_names is not None,\n",
        "                                            mean_last_cls)\n",
        "        self.post_init()\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        local_features = None,\n",
        "        global_features = None,\n",
        "        token_type_ids: Optional[torch.LongTensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        labels: Optional[torch.LongTensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = True,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple, SequenceClassifierOutput]:\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if self.model_class == \"BertPreTrainedModel\":\n",
        "          outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        if self.model_class == \"RobertaPreTrainedModel\":\n",
        "          outputs = self.roberta(\n",
        "              input_ids,\n",
        "              attention_mask=attention_mask,\n",
        "              token_type_ids=token_type_ids,\n",
        "              position_ids=position_ids,\n",
        "              head_mask=head_mask,\n",
        "              inputs_embeds=inputs_embeds,\n",
        "              output_attentions=output_attentions,\n",
        "              output_hidden_states=output_hidden_states,\n",
        "              return_dict=return_dict,\n",
        "          )\n",
        "\n",
        "        cls_tokens = []\n",
        "        for i in range(1, self.n_last_cls + 1):\n",
        "            cls_tokens.append(outputs.hidden_states[-1 * i][:, 0, :])\n",
        "        if self.mean_last_cls:\n",
        "          # average cls tokens\n",
        "          output = torch.mean(torch.stack(cls_tokens), dim=0)\n",
        "        else:\n",
        "          # concat cls tokens\n",
        "          output = torch.cat(cls_tokens, dim=1)\n",
        "\n",
        "        output = self.dropout(output)\n",
        "\n",
        "        if local_features is not None:\n",
        "          tokens_output = outputs.last_hidden_state\n",
        "          tokens_output = self.dropout(tokens_output)\n",
        "\n",
        "          if self.concat_local_features:\n",
        "            tokens_output = torch.cat((\n",
        "                    tokens_output,\n",
        "                    local_features.reshape(outputs.last_hidden_state.shape[0], outputs.last_hidden_state.shape[1], -1)),\n",
        "                              dim=2)\n",
        "\n",
        "          mask = torch.zeros_like(attention_mask)\n",
        "          # unmask tokens with high or low empathy, high distress levels, or expressing at least one emotion\n",
        "          j = 0\n",
        "          if 'emotions' in self.local_features_names:\n",
        "            emotion_values = local_features[:,:,:11]\n",
        "            mask[emotion_values.sum(dim=-1)>=1] = 1.0\n",
        "            j += 11\n",
        "\n",
        "          if 'empathy' in self.local_features_names:\n",
        "            empathy_values = local_features[:,:,j]\n",
        "            mask[(empathy_values>5) | ((empathy_values<3) & (empathy_values>=1))] = 1.0\n",
        "            j += 1\n",
        "\n",
        "          if 'distress' in self.local_features_names:\n",
        "            distress_values = local_features[:,:,j]\n",
        "            mask[distress_values>4] = 1.0\n",
        "\n",
        "          # mean pooling of unmasked tokens\n",
        "          input_mask_expanded = mask.unsqueeze(-1).expand(tokens_output.size()).float()\n",
        "          sum_embeddings = torch.sum(tokens_output * input_mask_expanded, 1)\n",
        "          sum_mask = input_mask_expanded.sum(1)\n",
        "          sum_mask = torch.clamp(sum_mask, min = 1e-9)\n",
        "          tokens_output = sum_embeddings/sum_mask\n",
        "\n",
        "          # concat pooled tokens lexically relevant with cls token\n",
        "          output = torch.cat((output, tokens_output), dim=-1)\n",
        "\n",
        "\n",
        "        if global_features is not None: # global\n",
        "          output = torch.cat((output, global_features), dim=-1)\n",
        "\n",
        "        logits = self.classifier(output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = nn.MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = nn.CrossEntropyLoss(weight = self.loss_weights)\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = nn.BCEWithLogitsLoss(weight = self.loss_weights)\n",
        "                loss = loss_fct(logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BinaryDataset(Dataset):\n",
        "\n",
        "\n",
        "  def __init__(\n",
        "  self,\n",
        "  tokenizer,\n",
        "  essay,\n",
        "  targets,\n",
        "  max_len=None\n",
        "  ):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "    self.essay = essay\n",
        "    self.targets = targets\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.targets)\n",
        "\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    essay = str(self.essay[index])\n",
        "\n",
        "\n",
        "    inputs = self.tokenizer.encode_plus(\n",
        "    text=essay,\n",
        "    add_special_tokens=True,\n",
        "    max_length=self.max_len,\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        "    return_token_type_ids=True\n",
        "    )\n",
        "\n",
        "\n",
        "    item = {\n",
        "    'input_ids': inputs['input_ids'].flatten(),\n",
        "    'attention_mask': inputs['attention_mask'].flatten(),\n",
        "    'token_type_ids': inputs[\"token_type_ids\"].flatten()\n",
        "    }\n",
        "\n",
        "    print(self.targets[index])\n",
        "    if self.targets is not None:\n",
        "      item['labels'] = torch.FloatTensor(self.targets[index])\n",
        "\n",
        "\n",
        "    return item"
      ],
      "metadata": {
        "id": "RLvG99jbk3iB"
      },
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 353,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:22.362620Z",
          "iopub.status.busy": "2023-07-11T19:11:22.361776Z",
          "iopub.status.idle": "2023-07-11T19:11:23.697235Z",
          "shell.execute_reply": "2023-07-11T19:11:23.696230Z",
          "shell.execute_reply.started": "2023-07-11T19:11:22.362585Z"
        },
        "id": "wd7W2HhYsbai",
        "outputId": "587c8e6a-d907-4920-e44c-c1859545b409",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaTokenizerFast(name_or_path='j-hartmann/emotion-english-distilroberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)}, clean_up_tokenization_spaces=True)"
            ]
          },
          "metadata": {},
          "execution_count": 353
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(config.get('tokenizer_name'), truncation=True)\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 354,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:23.734900Z",
          "iopub.status.busy": "2023-07-11T19:11:23.734558Z",
          "iopub.status.idle": "2023-07-11T19:11:27.562951Z",
          "shell.execute_reply": "2023-07-11T19:11:27.562030Z",
          "shell.execute_reply.started": "2023-07-11T19:11:23.734871Z"
        },
        "id": "GvJbF80XhSDE",
        "outputId": "0e565d24-46cb-4659-f970-f0d39f518535",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at j-hartmann/emotion-english-distilroberta-base and are newly initialized because the shapes did not match:\n",
            "- classifier.out_proj.weight: found shape torch.Size([7, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.out_proj.bias: found shape torch.Size([7]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 354
        }
      ],
      "source": [
        "if TASK == \"EMO\":\n",
        "  problem_type = \"multi_label_classification\"\n",
        "if TASK == \"EMP\":\n",
        "  problem_type = \"regression\"\n",
        "\n",
        "\"\"\"model = CustomSequenceClassification.from_pretrained(config.get('model_name'),\n",
        "                                                     problem_type = problem_type,\n",
        "                                                     classifier_dropout = config.get('dropout'),\n",
        "                                                     model_class = config.get('model_class_string'),\n",
        "                                                     num_labels=config.get('num_labels'),\n",
        "                                                     dim_extra_features = config.get('dim_extra_features'),\n",
        "                                                     local_features_names = config.get('local_features_names'),\n",
        "                                                     n_last_cls = config.get('n_last_cls'),\n",
        "                                                     mean_last_cls = config.get('mean_last_cls'),\n",
        "                                                     concat_local_features = config.get('concat_local_features'),\n",
        "                                                     loss_weights = loss_weights_train,\n",
        "                                                     ignore_mismatched_sizes=True\n",
        "                                                    )\"\"\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "  config.get('model_name'),\n",
        "  problem_type = \"single_label_classification\",\n",
        "  num_labels = config.get('num_labels'),\n",
        "  classifier_dropout = config.get('dropout'),\n",
        "  ignore_mismatched_sizes=True)\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xvHbhOhhttG"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY0SLN6usjYs"
      },
      "source": [
        "Prepare datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 355,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:27.565429Z",
          "iopub.status.busy": "2023-07-11T19:11:27.564800Z",
          "iopub.status.idle": "2023-07-11T19:11:27.572683Z",
          "shell.execute_reply": "2023-07-11T19:11:27.571493Z",
          "shell.execute_reply.started": "2023-07-11T19:11:27.565394Z"
        },
        "id": "DUIqO6AiZ8al",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "e7a262e5-4a0d-4fe9-a4d1-787029dce665"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"train_set = WASSADataset(tokenizer=tokenizer,\\n                    essay=train_df.essay,\\n                    prompt_before_SEP=prompt_before_SEP_train,\\n                    prompt_after_SEP=prompt_after_SEP_train,\\n                    essay_id = train_df.essay_id,\\n                    targets = y_train,\\n                    global_features = global_features_train,\\n                    EMO_lexicon = EMO_lexicon_train_dict,\\n                    EMP_lexicon = EMP_lexicon_train_dict,\\n                    local_emotions = config.get('emo_count_local'),\\n                    local_empathy = config.get('empathy_count_local'),\\n                    local_distress = config.get('distress_count_local')\\n                )\\n\\nval_set = WASSADataset(tokenizer=tokenizer,\\n                    essay = val_df.essay,\\n                    prompt_before_SEP = prompt_before_SEP_val,\\n                    prompt_after_SEP = prompt_after_SEP_val,\\n                    essay_id = val_df.essay_id,\\n                    targets = y_val,\\n                    global_features = global_features_val,\\n                    EMO_lexicon = EMO_lexicon_train_dict,\\n                    EMP_lexicon = EMP_lexicon_train_dict,\\n                    local_emotions = config.get('emo_count_local'),\\n                    local_empathy = config.get('empathy_count_local'),\\n                    local_distress = config.get('distress_count_local')\\n                )\\ndev_set = WASSADataset(tokenizer=tokenizer,\\n                    essay = dev_df.essay,\\n                    prompt_before_SEP = prompt_before_SEP_dev,\\n                    prompt_after_SEP = prompt_after_SEP_dev,\\n                    essay_id = dev_df.essay_id,\\n                    targets = y_dev,\\n                    global_features = global_features_dev,\\n                    EMO_lexicon = EMO_lexicon_train_dict,\\n                    EMP_lexicon = EMP_lexicon_train_dict,\\n                    local_emotions = config.get('emo_count_local'),\\n                    local_empathy = config.get('empathy_count_local'),\\n                    local_distress = config.get('distress_count_local')\\n                )\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 355
        }
      ],
      "source": [
        "train_set = BinaryDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    essay=train_df.essay,\n",
        "    targets = y_train)\n",
        "\n",
        "val_set = BinaryDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    essay=val_df.essay,\n",
        "    targets = y_val)\n",
        "\n",
        "dev_set = BinaryDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    essay=dev_df.essay,\n",
        "    targets = y_dev)\n",
        "\"\"\"train_set = WASSADataset(tokenizer=tokenizer,\n",
        "                    essay=train_df.essay,\n",
        "                    prompt_before_SEP=prompt_before_SEP_train,\n",
        "                    prompt_after_SEP=prompt_after_SEP_train,\n",
        "                    essay_id = train_df.essay_id,\n",
        "                    targets = y_train,\n",
        "                    global_features = global_features_train,\n",
        "                    EMO_lexicon = EMO_lexicon_train_dict,\n",
        "                    EMP_lexicon = EMP_lexicon_train_dict,\n",
        "                    local_emotions = config.get('emo_count_local'),\n",
        "                    local_empathy = config.get('empathy_count_local'),\n",
        "                    local_distress = config.get('distress_count_local')\n",
        "                )\n",
        "\n",
        "val_set = WASSADataset(tokenizer=tokenizer,\n",
        "                    essay = val_df.essay,\n",
        "                    prompt_before_SEP = prompt_before_SEP_val,\n",
        "                    prompt_after_SEP = prompt_after_SEP_val,\n",
        "                    essay_id = val_df.essay_id,\n",
        "                    targets = y_val,\n",
        "                    global_features = global_features_val,\n",
        "                    EMO_lexicon = EMO_lexicon_train_dict,\n",
        "                    EMP_lexicon = EMP_lexicon_train_dict,\n",
        "                    local_emotions = config.get('emo_count_local'),\n",
        "                    local_empathy = config.get('empathy_count_local'),\n",
        "                    local_distress = config.get('distress_count_local')\n",
        "                )\n",
        "dev_set = WASSADataset(tokenizer=tokenizer,\n",
        "                    essay = dev_df.essay,\n",
        "                    prompt_before_SEP = prompt_before_SEP_dev,\n",
        "                    prompt_after_SEP = prompt_after_SEP_dev,\n",
        "                    essay_id = dev_df.essay_id,\n",
        "                    targets = y_dev,\n",
        "                    global_features = global_features_dev,\n",
        "                    EMO_lexicon = EMO_lexicon_train_dict,\n",
        "                    EMP_lexicon = EMP_lexicon_train_dict,\n",
        "                    local_emotions = config.get('emo_count_local'),\n",
        "                    local_empathy = config.get('empathy_count_local'),\n",
        "                    local_distress = config.get('distress_count_local')\n",
        "                )\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvduRx8HsteS"
      },
      "source": [
        "Set up training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 356,
      "metadata": {
        "id": "ba9vfO44N58k"
      },
      "outputs": [],
      "source": [
        "if TASK == \"EMO\":\n",
        "  metric_for_val = \"eval_macro_f1\"\n",
        "  compute_metrics_trainer = compute_EMO_metrics_trainer\n",
        "if TASK == \"EMP\":\n",
        "  metric_for_val = \"eval_avg_pearson\"\n",
        "  compute_metrics_trainer = compute_EMP_metrics_trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 357,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:27.574736Z",
          "iopub.status.busy": "2023-07-11T19:11:27.574061Z",
          "iopub.status.idle": "2023-07-11T19:11:27.586314Z",
          "shell.execute_reply": "2023-07-11T19:11:27.585303Z",
          "shell.execute_reply.started": "2023-07-11T19:11:27.574702Z"
        },
        "id": "Z1DbZxhkD1R7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "train_arguments = TrainingArguments(\n",
        "    output_dir=f\"./{config.get('model_name')}\",\n",
        "    per_device_train_batch_size = config.get('train_batch_size'),\n",
        "    per_device_eval_batch_size = config.get('val_batch_size'),\n",
        "    num_train_epochs = config.get('epochs'),\n",
        "    evaluation_strategy = \"steps\",\n",
        "    save_strategy = \"steps\",\n",
        "    logging_strategy = \"steps\",\n",
        "    logging_steps = 150,\n",
        "    eval_steps = 150,\n",
        "    save_steps = 150,\n",
        "    learning_rate=config.get('learning_rate'),\n",
        "    weight_decay=config.get('weight_decay'),\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model = \"eval_loss\",\n",
        "    seed=config.get('seed'),\n",
        ") # TODO: custom other params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 358,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:27.588557Z",
          "iopub.status.busy": "2023-07-11T19:11:27.587598Z",
          "iopub.status.idle": "2023-07-11T19:11:27.726922Z",
          "shell.execute_reply": "2023-07-11T19:11:27.725911Z",
          "shell.execute_reply.started": "2023-07-11T19:11:27.588523Z"
        },
        "id": "hrI2rj4U3K2Q",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=train_arguments,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=val_set,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics_trainer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-sruHQdsraI"
      },
      "source": [
        "Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 359,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:27.731325Z",
          "iopub.status.busy": "2023-07-11T19:11:27.731059Z",
          "iopub.status.idle": "2023-07-11T19:11:27.738285Z",
          "shell.execute_reply": "2023-07-11T19:11:27.737157Z",
          "shell.execute_reply.started": "2023-07-11T19:11:27.731302Z"
        },
        "id": "4kUY7tU3sqr3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class TrainerLoggingCallback(TrainerCallback):\n",
        "    def __init__(self, log_path):\n",
        "        self.log_path = log_path\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        _ = logs.pop(\"total_flos\", None)\n",
        "        if state.is_local_process_zero: # whether this process is the main one in a distributed setting\n",
        "            with open(self.log_path, \"a\") as f:\n",
        "                f.write(json.dumps(logs) + \"\\n\")\n",
        "\n",
        "trainer.add_callback(EarlyStoppingCallback(\n",
        "    early_stopping_patience = config.get('patience'),\n",
        "    early_stopping_threshold = config.get('early_stopping_threshold')))\n",
        "\n",
        "trainer.add_callback(TrainerLoggingCallback(config.get('model_id')+\"_log.json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcCqSooFsw0X"
      },
      "source": [
        "Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 360,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "execution": {
          "iopub.execute_input": "2023-07-11T19:11:27.740439Z",
          "iopub.status.busy": "2023-07-11T19:11:27.740100Z",
          "iopub.status.idle": "2023-07-11T19:20:18.229718Z",
          "shell.execute_reply": "2023-07-11T19:20:18.228678Z",
          "shell.execute_reply.started": "2023-07-11T19:11:27.740408Z"
        },
        "id": "L8xAPa81D5tu",
        "outputId": "375ba432-452b-4f44-8cc6-43c7ad9a6482",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-360-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m         )\n\u001b[0;32m-> 1645\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1646\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2758\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2759\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2782\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2783\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2784\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2785\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2786\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1250\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1251\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3029\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (8) to match target batch_size (0)."
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:20:18.263809Z",
          "iopub.status.busy": "2023-07-11T19:20:18.263461Z",
          "iopub.status.idle": "2023-07-11T19:20:18.274394Z",
          "shell.execute_reply": "2023-07-11T19:20:18.272774Z",
          "shell.execute_reply.started": "2023-07-11T19:20:18.263776Z"
        },
        "id": "Dd-3octehTlH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Access the training logs\n",
        "train_logs = trainer.state.log_history\n",
        "\n",
        "metrics = [list(log.keys())[:-5] for log in train_logs if log.get('eval_loss') is not None][0]\n",
        "\n",
        "train_loss_values = []\n",
        "eval_loss_values = []\n",
        "eval_metrics_values = []\n",
        "for _ in metrics:\n",
        "  eval_metrics_values.append([])\n",
        "train_epochs = []\n",
        "eval_epochs = []\n",
        "\n",
        "best_metric = 0\n",
        "best_train_loss = float('inf')\n",
        "best_eval_loss = float('inf')\n",
        "\n",
        "for log in train_logs:\n",
        "\n",
        "  if log.get(metric_for_val) is not None:\n",
        "\n",
        "    if log.get(metric_for_val) > best_metric:\n",
        "      best_metric = log.get(metric_for_val)\n",
        "      best_steps = log.get('step')\n",
        "      best_epoch_metric = log.get('epoch')\n",
        "    if log.get('eval_loss') < best_eval_loss:\n",
        "      best_eval_loss = log.get('eval_loss')\n",
        "      best_epoch_eval_loss = log.get('epoch')\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "      eval_metrics_values[i].append(log.get(metric))\n",
        "\n",
        "    eval_loss_values.append(log.get('eval_loss'))\n",
        "    eval_epochs.append(log.get('epoch'))\n",
        "\n",
        "  if log.get('loss') is not None:\n",
        "    if log.get('loss') < best_train_loss:\n",
        "      best_train_loss = log.get('loss')\n",
        "\n",
        "    train_loss_values.append(log.get('loss'))\n",
        "    train_epochs.append(log.get('epoch'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V5eavAamxKr"
      },
      "source": [
        "Plot losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:20:18.303708Z",
          "iopub.status.busy": "2023-07-11T19:20:18.303144Z",
          "iopub.status.idle": "2023-07-11T19:20:18.378460Z",
          "shell.execute_reply": "2023-07-11T19:20:18.376257Z",
          "shell.execute_reply.started": "2023-07-11T19:20:18.303655Z"
        },
        "id": "ECEu3ajRl6k3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "fig_name = \"losses\"\n",
        "plot_metric_curve(\n",
        "    values = [train_loss_values, eval_loss_values],\n",
        "    epochs = [train_epochs, eval_epochs],\n",
        "    metrics = [\"train loss\", \"eval loss\"],\n",
        "    title = fig_name,\n",
        "    path = f\"{path_tosave}/{fig_name}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms0inN8amz33"
      },
      "source": [
        "Plot metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgGrIimdm3si"
      },
      "outputs": [],
      "source": [
        "fig_name = \"metrics\"\n",
        "plot_metric_curve(\n",
        "    values = eval_metrics_values[1:],\n",
        "    epochs = [eval_epochs for _ in eval_metrics_values[1:]],\n",
        "    metrics = metrics[1:],\n",
        "    title = fig_name,\n",
        "    path = f\"{path_tosave}/{fig_name}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc5stfSah0Kj"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-07-11T19:32:52.626228Z",
          "iopub.status.busy": "2023-07-11T19:32:52.625699Z",
          "iopub.status.idle": "2023-07-11T19:32:52.641902Z",
          "shell.execute_reply": "2023-07-11T19:32:52.638400Z",
          "shell.execute_reply.started": "2023-07-11T19:32:52.626186Z"
        },
        "id": "19G80dVoH3HZ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def predict_emotions(results, gold_emotions):\n",
        "\n",
        "  binarized_predictions = np.where(results >= 0.5, 1, 0)\n",
        "\n",
        "  for i, bin_pred in enumerate(binarized_predictions):\n",
        "    if np.all(bin_pred==0):\n",
        "      binarized_predictions[i][np.argmax(results[i])] = 1\n",
        "\n",
        "  predicted_emotions = label_encoder.decode(binarized_predictions)\n",
        "  return predicted_emotions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WIAU6a7Mox4"
      },
      "outputs": [],
      "source": [
        "print(trainer.state.best_model_checkpoint)\n",
        "\n",
        "outs = trainer.predict(dev_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXgFzu32_RBo"
      },
      "outputs": [],
      "source": [
        "if binary_EMO:\n",
        "  predictions = np.where(outs.predictions[0]>=0.5 ,1.0, 0.0)\n",
        "  golds = outs.label_ids\n",
        "  golds_str=[]\n",
        "  predictions_str=[]\n",
        "  for i in range(golds.shape[0]):\n",
        "    if golds[i][0]==0 and golds[i][1]==1:\n",
        "      golds_str.append('fear')\n",
        "    else:\n",
        "      golds_str.append('no-fear')\n",
        "    if predictions[i][0]==0 and predictions[i][1]==1:\n",
        "      predictions_str.append('fear')\n",
        "    elif predictions[i][0]==1 and predictions[i][1]==0:\n",
        "      predictions_str.append('no-fear')\n",
        "    else:\n",
        "      print(golds[i])\n",
        "      print(predictions[i])\n",
        "      print(outs.predictions[0][i])\n",
        "      if outs.predictions[0][i][0] > outs.predictions[0][i][1]:\n",
        "        predictions_str.append('no-fear')\n",
        "      else:\n",
        "        predictions_str.append('fear')\n",
        "  golds=golds_str\n",
        "  predictions=predictions_str\n",
        "elif TASK == \"EMO\":\n",
        "  golds = label_encoder.decode(outs.label_ids)\n",
        "  predictions = predict_emotions(outs.predictions[0], golds)\n",
        "if TASK == \"EMP\":\n",
        "  golds = outs.label_ids\n",
        "  predictions = outs.predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T19:20:18.388286Z",
          "iopub.status.idle": "2023-07-11T19:20:18.389135Z",
          "shell.execute_reply": "2023-07-11T19:20:18.388900Z",
          "shell.execute_reply.started": "2023-07-11T19:20:18.388873Z"
        },
        "id": "Im2VGnmQs7bY",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "path_predictions = f\"{path_tosave}/predictions_{TASK}.tsv\"\n",
        "path_metrics = f\"{path_tosave}/dev_metrics_{TASK}.json\"\n",
        "\n",
        "scores = {\n",
        "    'train_loss': float(best_train_loss),\n",
        "    'eval_loss': float(best_eval_loss)\n",
        "}\n",
        "\n",
        "write_predictions(predictions, path_predictions)\n",
        "challenge_metrics = compute_metrics(golds=golds, predictions=predictions, task=TASK)\n",
        "scores.update(challenge_metrics)\n",
        "scores['best_metric'] = float(best_metric)\n",
        "scores['best_epoch_metric'] = float(best_epoch_metric)\n",
        "scores['best_epoch_eval_loss'] =  float(best_epoch_eval_loss)\n",
        "\n",
        "write_dict_to_json(scores, path_metrics)\n",
        "\n",
        "scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T19:20:18.390758Z",
          "iopub.status.idle": "2023-07-11T19:20:18.391292Z",
          "shell.execute_reply": "2023-07-11T19:20:18.391058Z",
          "shell.execute_reply.started": "2023-07-11T19:20:18.391035Z"
        },
        "id": "ITunC36XXLoN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "if TASK == \"EMO\":\n",
        "  fig_name = \"confusion_matrix\"\n",
        "  plot_confusion_matrix(golds=golds,\n",
        "                        predictions=predictions,\n",
        "                        path= f\"{path_tosave}/{fig_name}.png\"\n",
        "                        )\n",
        "  print(\"\\n\")\n",
        "  fig_name = \"confusion_matrix_per_emotions\"\n",
        "  plot_confusion_matrix_per_emotions(gold_emotions = golds,\n",
        "                                     predicted_emotions = predictions,\n",
        "                                     path= f\"{path_tosave}/{fig_name}.png\"\n",
        "                                     )\n",
        "\n",
        "if TASK == \"EMP\":\n",
        "  fig_name = f\"empathy_true_vs_predicted\"\n",
        "  plot_true_vs_predicted(golds = golds[:,0],\n",
        "                        predictions = predictions[:,0],\n",
        "                        path= f\"{path_tosave}/{fig_name}.png\"\n",
        "                        )\n",
        "  print(\"\\n\")\n",
        "  fig_name = f\"distress_true_vs_predicted\"\n",
        "  plot_true_vs_predicted(golds = golds[:,1],\n",
        "                        predictions = predictions[:,1],\n",
        "                        path= f\"{path_tosave}/{fig_name}.png\"\n",
        "                        )\n",
        "  print(\"\\n\")\n",
        "  fig_name = f\"abs_diff_true_vs_predicted\"\n",
        "  plot_abs_diff_emp(golds = golds,\n",
        "                        predictions = predictions,\n",
        "                        path= f\"{path_tosave}/{fig_name}.png\"\n",
        "                        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pkHv9w4tM1F"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1u9D1Z2R_Jc"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint_and_compute_metrics(checkpoint_path, test_set):\n",
        "\n",
        "  model = CustomSequenceClassification.from_pretrained(checkpoint_path,\n",
        "                                                     problem_type = problem_type,\n",
        "                                                     classifier_dropout = config.get('dropout'),\n",
        "                                                     model_class = config.get('model_class_string'),\n",
        "                                                     num_labels=config.get('num_labels'),\n",
        "                                                     dim_extra_features = config.get('dim_extra_features'),\n",
        "                                                     local_features_names = config.get('local_features_names'),\n",
        "                                                     n_last_cls = config.get('n_last_cls'),\n",
        "                                                     mean_last_cls = config.get('mean_last_cls'),\n",
        "                                                     concat_local_features = config.get('concat_local_features'),\n",
        "                                                     loss_weights = loss_weights_train,\n",
        "                                                     ignore_mismatched_sizes=True\n",
        "                                                    )\n",
        "  trainer = Trainer(model=model)\n",
        "  # Perform prediction using the loaded checkpoint\n",
        "  outs = trainer.predict(test_set)\n",
        "\n",
        "  if TASK == \"EMO\":\n",
        "    golds = label_encoder.decode(outs.label_ids)\n",
        "    predictions = predict_emotions(outs.predictions[0], golds)\n",
        "  if TASK == \"EMP\":\n",
        "    golds = outs.label_ids\n",
        "    predictions = outs.predictions[0]\n",
        "\n",
        "  path_predictions = f\"{path_tosave}/predictions_{TASK}_best_metric.tsv\"\n",
        "  path_metrics = f\"{path_tosave}/dev_metrics_{TASK}_best_metric.json\"\n",
        "\n",
        "  write_predictions(predictions, path_predictions)\n",
        "  challenge_metrics = compute_metrics(golds=golds, predictions=predictions, task=TASK)\n",
        "\n",
        "  write_dict_to_json(challenge_metrics, path_metrics)\n",
        "\n",
        "  print(challenge_metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC88bo8dSSsr"
      },
      "outputs": [],
      "source": [
        "best_metric_path = f\"/content/{config.get('model_name')}/checkpoint-{best_steps}\"\n",
        "load_checkpoint_and_compute_metrics(best_metric_path, dev_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DuCaju6tSHB"
      },
      "source": [
        "## Save on Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WxgVCLCUSN8"
      },
      "outputs": [],
      "source": [
        "# move the best checkpoint in the folder with model id\n",
        "best_model_path = trainer.state.best_model_checkpoint\n",
        "\n",
        "!mv $best_model_path /content/$path_tosave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Q_AjZEVUSN8"
      },
      "outputs": [],
      "source": [
        "# move the checkpoint with best metric in the folder with model id\n",
        "!mv $best_metric_path /content/$path_tosave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-07-11T19:20:18.405534Z",
          "iopub.status.idle": "2023-07-11T19:20:18.406365Z",
          "shell.execute_reply": "2023-07-11T19:20:18.406135Z",
          "shell.execute_reply.started": "2023-07-11T19:20:18.406111Z"
        },
        "id": "b1KcxQQ0uUl6",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# move the results to personal drive\n",
        "!mv /content/$path_tosave /content/drive/MyDrive/hlt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}